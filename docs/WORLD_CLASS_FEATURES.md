# ä¸–ç•Œçº§åè®­ç»ƒå¹³å°ï¼šæœ€åä¸‰åˆ€å®ç°

æœ¬æ–‡æ¡£å±•ç¤ºäº†è®­ç»ƒå¹³å°çš„ä¸‰å¤§ä¸–ç•Œçº§ç‰¹æ€§ï¼š

1. **çœŸæ­£çš„å¯æ¢å¤ DAG Pipeline ç¼–æ’**
2. **å®Œæ•´çš„ Metrics é—­ç¯ç³»ç»Ÿ**
3. **æ™ºèƒ½è¯Šæ–­ä¸è‡ªåŠ¨åŒ–**

---

## ğŸ¯ ç‰¹æ€§ 1: çœŸæ­£çš„å¯æ¢å¤ DAG Pipeline ç¼–æ’

### æ ¸å¿ƒèƒ½åŠ›

- âœ… **æ¯ä¸ª stage éƒ½æ˜¯ç‹¬ç«‹çš„ Celery task** - å¯ç‹¬ç«‹é‡è¯•ã€ç›‘æ§ã€æ¢å¤
- âœ… **çœŸæ­£çš„ä¾èµ–å…³ç³»è§£æ** - æ”¯æŒçº¿æ€§ä¾èµ– (Aâ†’Bâ†’C) å’Œå¹¶è¡Œä¾èµ– (Aâ†’[B,C]â†’D)
- âœ… **è‡ªåŠ¨æ‹“æ‰‘æ’åº** - DAG è‡ªåŠ¨åˆ†å±‚æ‰§è¡Œï¼Œæœ€å¤§åŒ–å¹¶è¡Œåº¦
- âœ… **å¾ªç¯æ£€æµ‹** - è‡ªåŠ¨æ£€æµ‹å¹¶æ‹’ç»å¾ªç¯ä¾èµ–
- âœ… **å¤±è´¥æ¢å¤** - Pipeline ä¸­æ–­åå¯ä»å¤±è´¥ç‚¹æ¢å¤
- âœ… **æ¯ä¸ª stage çš„ task_id è®°å½•åˆ° DB** - å®Œæ•´çš„çŠ¶æ€è¿½è¸ª

### ä½¿ç”¨ç¤ºä¾‹

#### 1. åˆ›å»º Pipeline with Dependencies

```python
from training_platform.core.pipeline_executor import PipelineExecutor

# å®šä¹‰ Pipeline stagesï¼ˆå¸¦ä¾èµ–å…³ç³»ï¼‰
stages = [
    {
        "name": "preprocess",
        "task": "preprocess_dataset",
        "params": {"dataset_uuid": "xxx"},
        "depends_on": []  # æ— ä¾èµ–ï¼Œç¬¬ä¸€å±‚æ‰§è¡Œ
    },
    {
        "name": "train_sft",
        "task": "train_model",
        "params": {"job_uuid": "yyy", "config": {...}},
        "depends_on": ["preprocess"]  # ä¾èµ– preprocess
    },
    {
        "name": "train_rl_1",
        "task": "train_model",
        "params": {"job_uuid": "zzz1"},
        "depends_on": ["train_sft"]
    },
    {
        "name": "train_rl_2",
        "task": "train_model",
        "params": {"job_uuid": "zzz2"},
        "depends_on": ["train_sft"]  # å¹¶è¡Œï¼štrain_rl_1 å’Œ train_rl_2 åŒæ—¶æ‰§è¡Œ
    },
    {
        "name": "evaluate",
        "task": "run_evaluation",
        "params": {"job_uuid": "zzz"},
        "depends_on": ["train_rl_1", "train_rl_2"]  # ç­‰å¾…ä¸¤ä¸ªè®­ç»ƒä»»åŠ¡å®Œæˆ
    }
]

# æ‰§è¡Œ Pipeline
executor = PipelineExecutor(pipeline_uuid="pipeline-001")
result = executor.execute(stages)

print(f"Pipeline submitted: {result['root_task_id']}")
print(f"Execution plan: {result['layers']} layers")
```

**æ‰§è¡Œè®¡åˆ’ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰ï¼š**
```
Layer 0: [preprocess]              # ç¬¬ä¸€å±‚
Layer 1: [train_sft]               # ç¬¬äºŒå±‚ï¼ˆç­‰å¾… Layer 0ï¼‰
Layer 2: [train_rl_1, train_rl_2]  # ç¬¬ä¸‰å±‚ï¼ˆå¹¶è¡Œï¼‰
Layer 3: [evaluate]                # ç¬¬å››å±‚ï¼ˆç­‰å¾… Layer 2ï¼‰
```

#### 2. æ¢å¤ä¸­æ–­çš„ Pipeline

```python
# Pipeline å¤±è´¥æˆ–ä¸­æ–­å
executor = PipelineExecutor(pipeline_uuid="pipeline-001")
result = executor.resume()

# ç³»ç»Ÿä¼šï¼š
# 1. è¯»å– DB ä¸­å·²å®Œæˆçš„ stages
# 2. è·³è¿‡å·²å®Œæˆçš„ stages
# 3. é‡æ–°æ‰§è¡Œå¤±è´¥æˆ–æœªæ‰§è¡Œçš„ stages
```

#### 3. å®æ—¶ç›‘æ§ Pipeline çŠ¶æ€

```python
from training_platform.core.pipeline_executor import get_pipeline_status

status = get_pipeline_status("pipeline-001")

print(f"Pipeline Status: {status['status']}")
print(f"Celery Task ID: {status['celery_task_id']}")

for stage in status['stages']:
    print(f"  {stage['name']}: {stage['status']}")
    print(f"    Task ID: {stage['celery_task_id']}")
    print(f"    Started: {stage['started_at']}")
```

### æŠ€æœ¯å®ç°

**DAG è§£æå™¨ (DagResolver)**
```python
class DagResolver:
    def validate(self) -> bool:
        """éªŒè¯ DAG æœ‰æ•ˆæ€§ï¼ˆæ£€æŸ¥ä¾èµ–å­˜åœ¨ã€æ— å¾ªç¯ï¼‰"""

    def get_execution_layers(self) -> List[List[str]]:
        """æ‹“æ‰‘æ’åºï¼Œè¿”å›æ‰§è¡Œå±‚çº§"""
```

**æ‰§è¡Œå¼•æ“ (PipelineExecutor)**
```python
class PipelineExecutor:
    def execute(self, stages) -> Dict:
        """
        1. è§£æä¾èµ–å…³ç³»
        2. æ„å»º Celery Canvas (chain/group)
        3. æäº¤å¼‚æ­¥ä»»åŠ¡
        4. è®°å½• task_id åˆ° DB
        """

    def resume(self) -> Dict:
        """ä»å¤±è´¥ç‚¹æ¢å¤ Pipeline"""
```

---

## ğŸ¯ ç‰¹æ€§ 2: å®Œæ•´çš„ Metrics é—­ç¯ç³»ç»Ÿ

### æ•°æ®æµ

```
è®­ç»ƒä¾§ â†’ ç»“æ„åŒ– callback â†’ MetricsBuffer â†’ æ‰¹é‡å…¥åº“ â†’ Diagnostics â†’ å‘Šè­¦/åŠ¨ä½œ
```

### æ ¸å¿ƒç»„ä»¶

#### 1. è®­ç»ƒä¾§ Callback é›†æˆ

**æ–¹å¼ A: ç›´æ¥è°ƒç”¨ï¼ˆæœ¬åœ°è®­ç»ƒï¼‰**

```python
from training_platform.core.metrics_persister import create_training_callback

# åˆ›å»º callback
callback = create_training_callback(job_uuid="job-123")

# è®­ç»ƒå¾ªç¯ä¸­è°ƒç”¨
for step in range(num_steps):
    # è®­ç»ƒä¸€æ­¥
    outputs = train_step(batch)

    # å®æ—¶æ¨é€ metrics
    callback(
        step=step,
        epoch=epoch,
        policy_loss=outputs.policy_loss,
        value_loss=outputs.value_loss,
        reward_mean=outputs.reward.mean(),
        reward_std=outputs.reward.std(),
        kl_divergence=outputs.kl,
        grad_norm_actor=outputs.grad_norms.actor,
        tokens_per_second=throughput,
        gpu_memory_allocated_gib=gpu_mem / 1024**3,
    )
```

**æ–¹å¼ B: æ–‡ä»¶è½ç›˜ï¼ˆè¿œç¨‹è®­ç»ƒ / SSHï¼‰**

```python
from training_platform.core.metrics_persister import create_metrics_file_writer

# åˆ›å»ºæ–‡ä»¶ writer
writer = create_metrics_file_writer(job_uuid="job-123", metrics_dir="./metrics")

# è®­ç»ƒå¾ªç¯
for step in range(num_steps):
    outputs = train_step(batch)

    # å†™å…¥æ–‡ä»¶ï¼ˆJSONL æ ¼å¼ï¼‰
    writer(
        step=step,
        epoch=epoch,
        policy_loss=outputs.policy_loss,
        ...
    )

# åå° watcher ä¼šè‡ªåŠ¨è¯»å–å¹¶å…¥åº“
```

#### 2. MetricsBufferï¼ˆè‡ªåŠ¨æ‰¹å¤„ç†ï¼‰

```python
class MetricsBuffer:
    """
    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨æ‰¹é‡å…¥åº“ï¼ˆmax_size=100 æˆ– max_age_seconds=30ï¼‰
    - ä½å»¶è¿Ÿï¼ˆå¼‚æ­¥ç´¯ç§¯ï¼Œæ‰¹é‡åˆ·æ–°ï¼‰
    - å®¹é”™ï¼ˆå•æ¡å¤±è´¥ä¸å½±å“å…¶ä»–ï¼‰
    """
    def add(self, metric):
        """æ·»åŠ  metricï¼Œè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦åˆ·æ–°"""

    def flush(self) -> int:
        """æ‰¹é‡å†™å…¥æ•°æ®åº“"""
```

#### 3. Celery å®šæ—¶ä»»åŠ¡

**Celery Beat é…ç½®ï¼š**

```python
# celery_config.py
from celery.schedules import crontab

app.conf.beat_schedule = {
    # æ¯ 30 ç§’åˆ·æ–° metrics buffer
    'flush-metrics-buffer': {
        'task': 'training_platform.core.metrics_persister.periodic_flush',
        'schedule': 30.0,
    },

    # æ¯åˆ†é’Ÿæ‰«æå¤±è´¥ä»»åŠ¡å¹¶è¯Šæ–­
    'scan-failed-jobs': {
        'task': 'training_platform.core.celery_tasks.scan_failed_jobs',
        'schedule': crontab(minute='*/1'),
    },
}
```

### æ•°æ®æ¨¡å‹

```python
@dataclass
class StructuredMetric:
    """æ ‡å‡†åŒ–çš„è®­ç»ƒæŒ‡æ ‡"""
    job_uuid: str
    step: int
    epoch: int
    timestamp: datetime

    # Core training metrics
    loss: float
    learning_rate: float
    grad_norm: float

    # RL-specific metrics
    reward_mean: float
    reward_std: float
    kl_divergence: float
    entropy: float

    # Performance metrics
    throughput_samples_per_sec: float
    gpu_memory_allocated_gb: float
    gpu_utilization_percent: float

    # Validation metrics
    eval_loss: float
    eval_accuracy: float

    # Custom metrics (flexible)
    custom: Dict[str, Any]
```

---

## ğŸ¯ ç‰¹æ€§ 3: æ™ºèƒ½è¯Šæ–­ä¸è‡ªåŠ¨åŒ–

### Diagnostics å®æ—¶åˆ¤å®š

**è‡ªåŠ¨æ£€æµ‹ï¼š**
- âœ… Loss NaN / Inf
- âœ… KL Divergence çˆ†ç‚¸
- âœ… Reward å¼‚å¸¸
- âœ… æ¢¯åº¦æ¶ˆå¤± / æ¢¯åº¦çˆ†ç‚¸
- âœ… Loss plateauï¼ˆé•¿æ—¶é—´ä¸æ”¹å–„ï¼‰
- âœ… GPU OOM

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
from training_platform.core.diagnostics import DiagnosticService

with Session(engine) as session:
    diagnostics = DiagnosticService(session)

    # è¯Šæ–­å•ä¸ª step
    result = diagnostics.diagnose_step(job_uuid="job-123", step=1000)

    if result['has_anomaly']:
        print(f"Anomaly detected: {result['anomaly_type']}")
        print(f"Message: {result['anomaly_message']}")
        print(f"Suggestions: {result['suggestions']}")

    # è¯Šæ–­æ•´ä¸ª job
    full_result = diagnostics.diagnose_job(job_uuid="job-123")
```

### å‘Šè­¦ç³»ç»Ÿï¼ˆå¾…å®Œå–„ï¼‰

**Webhook é›†æˆï¼š**

```python
# é…ç½®å‘Šè­¦
ALERT_WEBHOOKS = {
    "slack": "https://hooks.slack.com/xxx",
    "feishu": "https://open.feishu.cn/xxx",
}

# å½“æ£€æµ‹åˆ°å¼‚å¸¸æ—¶è§¦å‘å‘Šè­¦
def send_alert(job_uuid: str, anomaly_type: str, message: str):
    """å‘é€å‘Šè­¦åˆ° Slack/é£ä¹¦"""
    payload = {
        "job_uuid": job_uuid,
        "anomaly": anomaly_type,
        "message": message,
        "timestamp": datetime.utcnow().isoformat(),
    }

    requests.post(ALERT_WEBHOOKS["slack"], json=payload)
```

### è‡ªåŠ¨åŠ¨ä½œï¼ˆå¾…å®Œå–„ï¼‰

**å¯èƒ½çš„è‡ªåŠ¨åŠ¨ä½œï¼š**
- æš‚åœè®­ç»ƒï¼ˆæ£€æµ‹åˆ° NaNï¼‰
- è°ƒæ•´å­¦ä¹ ç‡ï¼ˆæ£€æµ‹åˆ°æ¢¯åº¦çˆ†ç‚¸ï¼‰
- è§¦å‘ checkpointï¼ˆreward è¾¾åˆ°æ–°é«˜ï¼‰
- å‘é€é€šçŸ¥

```python
class AutoAction:
    """è‡ªåŠ¨åŒ–åŠ¨ä½œç³»ç»Ÿ"""

    def on_nan_detected(self, job_uuid: str):
        """æ£€æµ‹åˆ° NaN æ—¶æš‚åœè®­ç»ƒ"""
        pause_job(job_uuid)
        send_alert(job_uuid, "NaN detected", "Training paused")

    def on_gradient_explosion(self, job_uuid: str, grad_norm: float):
        """æ¢¯åº¦çˆ†ç‚¸æ—¶é™ä½å­¦ä¹ ç‡"""
        current_lr = get_learning_rate(job_uuid)
        new_lr = current_lr * 0.1
        update_learning_rate(job_uuid, new_lr)
        send_alert(job_uuid, "Gradient explosion", f"LR reduced to {new_lr}")
```

---

## ğŸ“Š å®Œæ•´æµç¨‹ç¤ºä¾‹

### åœºæ™¯ï¼šå¤šé˜¶æ®µ PPO è®­ç»ƒ Pipeline

```python
# 1. åˆ›å»º Pipeline
pipeline_config = {
    "pipeline_uuid": "ppo-training-001",
    "stages": [
        {
            "name": "sft_stage",
            "task": "train_model",
            "params": {
                "job_uuid": "sft-job-001",
                "config": {
                    "algorithm": "sft",
                    "model_path": "meta-llama/Llama-2-7b",
                    "train_data_path": "./data/sft_data.parquet",
                    "num_epochs": 3,
                },
                "run_mode": "ssh",
                "ssh_config": {...},
            },
            "depends_on": []
        },
        {
            "name": "ppo_stage_1",
            "task": "train_model",
            "params": {
                "job_uuid": "ppo-job-001",
                "config": {
                    "algorithm": "ppo",
                    "model_path": "${sft_stage.checkpoint}",  # ä¾èµ–ä¸Šä¸€é˜¶æ®µ
                    "train_data_path": "./data/ppo_data.parquet",
                    "num_epochs": 1,
                    "kl_coef": 0.1,
                },
            },
            "depends_on": ["sft_stage"]
        },
        {
            "name": "evaluation",
            "task": "run_evaluation",
            "params": {
                "job_uuid": "ppo-job-001",
                "checkpoint_path": "${ppo_stage_1.checkpoint}",
                "eval_dataset_uuid": "eval-001",
            },
            "depends_on": ["ppo_stage_1"]
        }
    ]
}

# 2. æäº¤ Pipeline
from training_platform.api.routers.pipelines import create_pipeline

response = await create_pipeline(pipeline_config)
print(f"Pipeline created: {response['uuid']}")

# 3. è®­ç»ƒä¾§é›†æˆ Metrics Callback
def execute_training(job_uuid, config, ...):
    """åœ¨ execute_training ä¸­é›†æˆ callback"""
    from training_platform.core.metrics_persister import create_training_callback

    callback = create_training_callback(job_uuid)

    # Verl è®­ç»ƒå¾ªç¯
    for step in range(num_steps):
        outputs = trainer.train_step(batch)

        # å®æ—¶æ¨é€ metrics
        callback(
            step=step,
            epoch=epoch,
            policy_loss=outputs.policy_loss.item(),
            value_loss=outputs.value_loss.item(),
            reward_mean=outputs.rewards.mean().item(),
            kl_divergence=outputs.approx_kl.item(),
            grad_norm_actor=outputs.grad_norm_actor,
        )

# 4. å®æ—¶ç›‘æ§
while pipeline_status != "COMPLETED":
    status = get_pipeline_status("ppo-training-001")

    for stage in status['stages']:
        print(f"{stage['name']}: {stage['status']}")

        if stage['status'] == 'RUNNING':
            # è·å–æœ€æ–° metrics
            metrics = get_latest_metrics(stage['job_uuid'], limit=10)
            print(f"  Latest loss: {metrics[0].loss}")

    time.sleep(10)

# 5. å¼‚å¸¸æ£€æµ‹ä¸å‘Šè­¦
diagnostics = DiagnosticService(session)
result = diagnostics.diagnose_job("ppo-job-001")

if result['has_anomaly']:
    send_alert("ppo-job-001", result['anomaly_type'], result['anomaly_message'])
```

---

## ğŸš€ ç”Ÿäº§ç¯å¢ƒé…ç½®

### Celery Workers

```bash
# Worker for training tasks (long-running)
celery -A training_platform.core.celery_config worker \
    --queues=training \
    --concurrency=2 \
    --max-tasks-per-child=1 \
    --loglevel=info

# Worker for fast tasks (metrics, diagnostics)
celery -A training_platform.core.celery_config worker \
    --queues=metrics,diagnostics \
    --concurrency=10 \
    --loglevel=info

# Celery Beat (scheduled tasks)
celery -A training_platform.core.celery_config beat \
    --loglevel=info
```

### Redis é…ç½®

```python
# celery_config.py
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

app.conf.update(
    broker_url=REDIS_URL,
    result_backend=REDIS_URL,
    task_serializer='json',
    result_serializer='json',
    accept_content=['json'],
    timezone='UTC',
    enable_utc=True,
)
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### Metrics Buffer

- **æ‰¹é‡å¤§å°**: 100 æ¡ï¼ˆå¯é…ç½®ï¼‰
- **åˆ·æ–°é—´éš”**: 30 ç§’ï¼ˆå¯é…ç½®ï¼‰
- **é¢„æœŸåå**: > 1000 metrics/sec

### Pipeline å¹¶è¡Œåº¦

- **è‡ªåŠ¨å¹¶è¡Œ**: DAG è‡ªåŠ¨åˆ†å±‚ï¼ŒåŒå±‚ stages å¹¶è¡Œæ‰§è¡Œ
- **èµ„æºéš”ç¦»**: æ¯ä¸ª stage ç‹¬ç«‹ Celery task
- **å¤±è´¥éš”ç¦»**: å•ä¸ª stage å¤±è´¥ä¸å½±å“å…¶ä»– stages

---

## ğŸ“ è®¾è®¡åŸåˆ™

1. **å¯æ¢å¤æ€§ä¼˜å…ˆ** - æ‰€æœ‰å¼‚æ­¥æ“ä½œéƒ½å¯æ¢å¤
2. **æ•°æ®é©±åŠ¨** - çŠ¶æ€å…¨éƒ¨è®°å½•åˆ° DB
3. **æ¾è€¦åˆ** - è®­ç»ƒä¾§ä¸å¹³å°ä¾§è§£è€¦
4. **æ‰¹é‡ä¼˜åŒ–** - æ‰€æœ‰ I/O æ“ä½œæ‰¹é‡åŒ–
5. **å®¹é”™è®¾è®¡** - å•ç‚¹å¤±è´¥ä¸å½±å“æ•´ä½“

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### Pipeline å¡ä½ä¸æ‰§è¡Œ

```bash
# æ£€æŸ¥ Celery workers
celery -A training_platform.core.celery_config inspect active

# æ£€æŸ¥ Pipeline çŠ¶æ€
python -c "from training_platform.core.pipeline_executor import get_pipeline_status; \
           print(get_pipeline_status('pipeline-uuid'))"

# æ¢å¤ Pipeline
python -c "from training_platform.core.pipeline_executor import PipelineExecutor; \
           executor = PipelineExecutor('pipeline-uuid'); \
           executor.resume()"
```

### Metrics æœªå…¥åº“

```bash
# æ‰‹åŠ¨åˆ·æ–° buffer
python -c "from training_platform.core.metrics_persister import flush_metrics; \
           flush_metrics()"

# æ£€æŸ¥ metrics æ–‡ä»¶
ls -lh ./metrics/*_metrics.jsonl

# æ‰‹åŠ¨åŒæ­¥æ–‡ä»¶
python -c "from training_platform.core.metrics_persister import sync_metrics_from_file; \
           from pathlib import Path; \
           sync_metrics_from_file('job-uuid', Path('./metrics/job-uuid_metrics.jsonl'), session)"
```

---

## ğŸ† æ€»ç»“

é€šè¿‡è¿™ä¸‰å¤§ç‰¹æ€§ï¼Œæˆ‘ä»¬å®ç°äº†ï¼š

âœ… **ä¸–ç•Œçº§çš„ç¼–æ’èƒ½åŠ›** - çœŸæ­£çš„ DAG ä¾èµ–ã€è‡ªåŠ¨æ¢å¤ã€çŠ¶æ€è¿½è¸ª
âœ… **å®Œæ•´çš„æŒ‡æ ‡é—­ç¯** - è®­ç»ƒä¾§â†’æŒä¹…åŒ–â†’è¯Šæ–­â†’å‘Šè­¦çš„å®Œæ•´é“¾è·¯
âœ… **ç”Ÿäº§çº§çš„å¯é æ€§** - æ‰¹é‡ä¼˜åŒ–ã€å®¹é”™è®¾è®¡ã€çŠ¶æ€å¯æ¢å¤

è¿™æ˜¯ä¸€ä¸ªå¯ä»¥ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒçš„åè®­ç»ƒå¹³å°ï¼ğŸš€
