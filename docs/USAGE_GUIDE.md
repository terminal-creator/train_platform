# Training Platform 完整使用指南

基于 verl 框架的 LLM 训练平台，支持 SFT、PPO、GRPO、DPO 等多种训练算法。

---

## 目录

1. [环境安装](#1-环境安装)
2. [快速开始](#2-快速开始)
3. [运行模式配置](#3-运行模式配置)
4. [数据集管理与审核](#4-数据集管理与审核)
5. [计算配置器](#5-计算配置器)
6. [训练任务配置](#6-训练任务配置)
7. [实时监控](#7-实时监控)
8. [模型手术](#8-模型手术)
9. [自定义评估](#9-自定义评估)
10. [安全性说明](#10-安全性说明)
11. [常见问题](#11-常见问题)

---

## 1. 环境安装

平台采用**固定版本依赖**管理，确保环境可复现和稳定性。

### 1.1 系统要求

- **Python**: 3.9, 3.10, 或 3.11（推荐 3.10）
- **管理节点**：无需 GPU，16GB+ RAM
- **训练节点**：NVIDIA GPU + CUDA 12.1+，64GB+ RAM（推荐）

### 1.2 本地开发环境（管理节点）

```bash
cd train_platform

# 自动安装（推荐）
bash scripts/setup_local_env.sh manager

# 手动安装
python3 -m venv venv
source venv/bin/activate
pip install -r environments/requirements-manager.txt
pip install -e environments/verl
```

### 1.3 训练服务器环境

**方法一：远程自动安装（推荐）**

```bash
# 从本地推送并安装
bash scripts/setup_remote_env.sh user@gpu-server.example.com ~/train_platform
```

**方法二：手动安装**

```bash
# 在训练服务器上执行
cd ~/train_platform
bash scripts/setup_local_env.sh training
```

### 1.4 环境验证

```bash
# 管理节点
python scripts/verify_env.py --mode manager

# 训练节点
python scripts/verify_env.py --mode training
```

验证通过后显示：
```
✓ Python 版本: 3.10.x
✓ PyTorch: 2.1.2
✓ Transformers: 4.37.2
✓ CUDA 可用: True
✓ verl: 已安装
✓ 环境验证通过！
```

### 1.5 版本说明

当前平台版本：**v1.0.0**
verl 版本：**b12eb3b** (v0.7.0-23)
详见：`environments/version.json`

---

## 2. 快速开始

### 2.1 启动平台

```bash
# 启动后端
cd train_platform
source venv/bin/activate  # 激活虚拟环境
python -m uvicorn training_platform.api.main:app --reload --port 8000

# 启动前端
cd frontend
npm run dev
```

访问 `http://localhost:5173` 进入平台。

### 1.2 平台功能概览

| 功能模块 | 说明 | 入口 |
|----------|------|------|
| **训练任务** | 创建和管理训练任务 | 侧边栏 → 训练任务 |
| **数据集** | 上传、审核、分析数据集 | 侧边栏 → 数据集 |
| **计算配置** | 自动计算最优训练参数 | 侧边栏 → 计算配置 |
| **模型手术** | 模型合并、检查点选择 | 侧边栏 → 模型手术 |
| **评估中心** | 自定义评估任务 | 侧边栏 → 评估 |
| **设置** | 配置运行模式 | 侧边栏 → 设置 |

---

## 2. 运行模式配置

### 2.1 选择运行模式

平台支持两种运行模式：

| 模式 | 适用场景 | 配置位置 |
|------|----------|----------|
| **本地 (Local)** | 本机有 GPU | 设置页面 |
| **SSH 远程** | 使用远程 GPU 服务器 | 设置页面 |

### 2.2 Local 模式配置

**本机环境要求：**
```bash
# 安装 verl 环境
conda create -n verl python=3.10
conda activate verl
pip install torch verl vllm ray[default]
```

**平台配置：**
1. 进入 **设置** 页面
2. 选择 **本地** 模式
3. 点击 **保存配置**

### 2.3 SSH 远程模式配置

**远程服务器准备：**
```bash
# 1. 创建工作目录
mkdir -p ~/verl_jobs/{datasets,models,outputs,logs}

# 2. 安装 verl 环境
conda create -n verl python=3.10
conda activate verl
pip install torch verl vllm ray[default] pyzmq

# 3. 下载模型
pip install modelscope
python -c "from modelscope import snapshot_download; snapshot_download('Qwen/Qwen2.5-0.5B-Instruct', local_dir='~/autodl-tmp/qwen_0.5b')"
```

**平台配置：**

| 字段 | 说明 | 示例 |
|------|------|------|
| 主机地址 | 服务器 IP 或域名 | `****` |
| 端口 | SSH 端口 | `****` |
| 用户名 | 登录用户 | `root` |
| 密码 | 登录密码 | `********` |
| 工作目录 | 远程工作目录 | `~/verl_jobs` |
| Conda环境 | 远程环境名 | `verl` |

配置完成后点击 **测试连接** 验证。

### 2.4 快速切换模式

在 **训练任务** 页面顶部可以快速切换运行模式：
- 点击 **本地** 或 **SSH远程** 按钮
- 模型/数据集列表会自动切换数据源

---

## 3. 数据集管理与审核

### 3.1 上传数据集

**入口：** 侧边栏 → 数据集 → 上传

**支持格式：**
- JSON (`.json`)
- JSON Lines (`.jsonl`)
- Parquet (`.parquet`) - 推荐

**上传步骤：**
1. 点击 **上传数据集**
2. 选择文件并填写名称
3. 选择数据集类型（SFT/GRPO/PPO/DPO）
4. 点击上传

> SSH 模式下，上传的数据集会自动同步到远程服务器。

### 3.2 数据集格式要求

#### SFT 数据集（监督微调）
```json
[
  {
    "prompt": "什么是机器学习？",
    "response": "机器学习是人工智能的一个分支，通过数据训练模型来做出预测或决策。"
  }
]
```

#### GRPO 数据集（强化学习 - 规则奖励）
```json
[
  {
    "prompt": "计算: 25 + 37 = ?",
    "data_source": "math",
    "solution": "62"
  }
]
```
- `data_source`: 数据来源标签，用于奖励函数选择
- `solution`: 正确答案（用于奖励函数验证）

#### PPO 数据集（强化学习 - 模型奖励）
```json
[
  {
    "prompt": "请写一首关于春天的诗",
    "data_source": "creative"
  }
]
```

#### DPO 数据集（偏好优化）
```json
[
  {
    "prompt": "如何学习编程？",
    "chosen": "学习编程建议从 Python 开始，它语法简洁，适合入门...",
    "rejected": "随便学学就行"
  }
]
```

### 3.3 数据集审核功能

上传后进入数据集详情页，可以进行以下审核：

#### 3.3.1 数据分布分析

- **字段分布图**：查看各字段的值分布
- **长度统计**：prompt/response 的 token 长度分布
- **标签分布**：data_source 等标签的分布情况

#### 3.3.2 样本预览

- 浏览数据集中的具体样本
- 支持分页查看
- 可以查看单条数据的详细内容

#### 3.3.3 质量检查

| 检查项 | 说明 |
|--------|------|
| 空值检测 | 检查必填字段是否有空值 |
| 重复检测 | 检测重复的 prompt |
| 长度异常 | 检测过长或过短的样本 |
| 格式验证 | 验证数据格式是否正确 |

#### 3.3.4 字段配置

- **配置标签字段**：指定哪些字段作为分类标签
- **配置损失字段**：指定 prompt 和 response 字段（用于 SFT）

### 3.4 数据集同步

SSH 模式下，可以手动同步数据集到远程服务器：

1. 在数据集列表点击 **同步** 按钮
2. 或点击 **全部同步** 同步所有数据集
3. 查看同步状态（已同步/同步中/未同步）

---

## 4. 计算配置器

### 4.1 功能介绍

计算配置器帮助你自动计算最优的训练参数，避免 OOM（显存不足）问题。

**入口：** 侧边栏 → 计算配置

### 4.2 输入参数

| 参数 | 说明 | 示例 |
|------|------|------|
| 模型大小 | 模型参数量 | 0.5B / 7B / 72B |
| GPU 类型 | 显卡型号 | RTX 5090 / A100-80G |
| GPU 数量 | 使用的 GPU 数量 | 1 / 4 / 8 |
| 序列长度 | 最大上下文长度 | 2048 / 4096 / 8192 |
| 训练算法 | 使用的算法 | SFT / GRPO / PPO |

### 4.3 输出建议

计算器会给出以下建议：

| 输出项 | 说明 |
|--------|------|
| **推荐 Batch Size** | 最优批次大小 |
| **推荐 Micro Batch** | 每个 GPU 的微批次 |
| **ZeRO Stage** | 分布式训练策略 |
| **显存估算** | 预估显存使用量 |
| **是否启用 LoRA** | 显存不足时建议启用 |
| **梯度检查点** | 是否需要开启 |

### 4.4 使用示例

```
输入：
- 模型: Qwen2.5-7B (7B 参数)
- GPU: RTX 5090 (32GB) x 1
- 序列长度: 4096
- 算法: GRPO

输出建议：
- Batch Size: 64
- Micro Batch Size: 4
- ZeRO Stage: 2
- 预估显存: 28.5 GB
- 建议启用 LoRA: 否
- 梯度检查点: 是
```

### 4.5 一键应用

计算完成后，点击 **应用到新任务** 可以直接创建训练任务，自动填充推荐参数。

---

## 5. 训练任务配置

### 5.1 创建任务

**入口：** 训练任务 → 新建任务

### 5.2 基础配置

| 配置项 | 说明 | 必填 |
|--------|------|------|
| 任务名称 | 自定义名称 | 是 |
| 训练算法 | SFT/GRPO/PPO/DPO | 是 |
| 基础模型 | 模型路径 | 是 |
| 训练数据集 | 数据集路径 | 是 |
| GPU 数量 | 使用的 GPU 数 | 是 |

### 5.3 各算法详细配置

#### 5.3.1 SFT（监督微调）

**适用场景：** 让模型学习特定任务或风格

**关键参数：**

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| Learning Rate | 1e-5 ~ 5e-5 | 学习率 |
| Batch Size | 4-32 | 总批次大小 |
| Epochs | 2-5 | 训练轮数 |
| 序列长度 | 2048-4096 | 最大长度 |

**数据集要求：**
- 字段：`prompt`, `response`
- 格式：Parquet 推荐

#### 5.3.2 GRPO（组相对策略优化）

**适用场景：** 使用规则函数作为奖励信号的强化学习

**关键参数：**

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| Learning Rate | 1e-6 ~ 1e-5 | 学习率（比 SFT 小） |
| Batch Size | 128-512 | 总批次大小 |
| Rollout N | 4-16 | 每个 prompt 生成的样本数 |
| KL 系数 | 0.01-0.1 | KL 散度惩罚系数 |

**奖励函数配置：**

| 配置项 | 选项 | 说明 |
|--------|------|------|
| 函数类型 | math_verify / format_check / custom | 奖励函数类型 |
| 答案提取 | boxed / last_number / json | 从回答中提取答案的方式 |
| 比较方法 | exact / numeric / fuzzy | 答案比较方式 |
| 答案字段 | solution | 数据集中正确答案的字段名 |

**示例配置：**
```
奖励函数类型: math_verify
答案提取方式: boxed (从 \boxed{} 中提取)
比较方法: numeric (数值比较)
答案字段: solution
```

#### 5.3.3 PPO（近端策略优化）

**适用场景：** 使用奖励模型的强化学习

**关键参数：**

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| Learning Rate | 1e-6 ~ 5e-6 | Actor 学习率 |
| Batch Size | 128-512 | 总批次大小 |
| Rollout N | 4-8 | 每个 prompt 生成的样本数 |
| Clip Ratio | 0.2 | PPO 裁剪比例 |

**奖励模型配置：**

| 配置项 | 说明 |
|--------|------|
| 奖励模型路径 | 预训练奖励模型的路径 |
| 梯度检查点 | 是否开启（节省显存） |
| 参数卸载 | 是否卸载到 CPU |
| Micro Batch | 奖励模型的微批次大小 |

> 注意：PPO 需要预先训练好的奖励模型

#### 5.3.4 DPO（直接偏好优化）

**适用场景：** 直接从偏好数据学习，无需奖励模型

**关键参数：**

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| Learning Rate | 1e-6 ~ 5e-6 | 学习率 |
| Beta | 0.1-0.5 | DPO 温度参数 |
| Batch Size | 32-128 | 总批次大小 |

**数据集要求：**
- 字段：`prompt`, `chosen`, `rejected`

### 5.4 高级配置

#### LoRA 配置

| 参数 | 说明 | 推荐值 |
|------|------|--------|
| 启用 LoRA | 低秩适配，节省显存 | 显存不足时启用 |
| LoRA Rank | 秩大小 | 8 / 16 / 32 |
| LoRA Alpha | 缩放因子 | 通常 = 2 × Rank |

#### 检查点配置

| 参数 | 说明 |
|------|------|
| 检查点间隔 | 每 N 步保存一次 |
| 评估间隔 | 每 N 步评估一次 |
| 保留检查点数 | 最多保留几个检查点 |

### 5.5 启动训练

1. 填写完配置后点击 **创建任务**
2. 任务创建后状态为 `pending`
3. 点击 **启动** 按钮开始训练
4. 或勾选 **创建后立即启动**

---

## 6. 实时监控

### 6.1 进入监控页面

- 在训练任务列表点击任务名称
- 或点击 **查看详情** 按钮

### 6.2 监控面板

#### 6.2.1 基本信息

| 信息项 | 说明 |
|--------|------|
| 任务状态 | running / paused / completed / failed |
| 当前步数 | 已训练的步数 |
| 当前轮次 | 已完成的 epoch |
| 运行时间 | 已运行的时间 |
| 预计剩余 | 预估剩余时间 |

#### 6.2.2 训练指标图表

**Loss 曲线：**
- Policy Loss（策略损失）
- Value Loss（价值损失，PPO）
- Total Loss（总损失）

**Reward 曲线（强化学习）：**
- Reward Mean（平均奖励）
- Reward Std（奖励标准差）

**其他指标：**
- KL Divergence（KL 散度）
- Entropy（熵）
- Learning Rate（学习率变化）
- Gradient Norm（梯度范数）

#### 6.2.3 GPU 资源监控

| 指标 | 说明 |
|------|------|
| GPU 利用率 | 各 GPU 的计算利用率 |
| 显存使用 | 已用/总显存 |
| 温度 | GPU 温度 |
| 功耗 | 当前功耗 |

#### 6.2.4 实时日志

- 实时显示训练日志
- 支持日志级别筛选
- 支持关键词搜索
- 可下载完整日志

### 6.3 梯度热力图

**功能：** 可视化各层梯度的变化，帮助诊断训练问题

**使用方法：**
1. 点击 **梯度分析** 标签
2. 选择要查看的步数范围
3. 查看热力图：
   - 横轴：训练步数
   - 纵轴：模型层
   - 颜色：梯度大小

**诊断指南：**
- 梯度过小（接近 0）：可能梯度消失
- 梯度过大：可能梯度爆炸
- 梯度不均匀：可能需要调整学习率

### 6.4 训练控制

| 操作 | 说明 |
|------|------|
| **暂停** | 暂停训练，保留状态 |
| **恢复** | 从暂停状态恢复 |
| **停止** | 终止训练 |

---

## 7. 模型手术

### 7.1 功能介绍

模型手术提供训练后的模型优化功能：

| 功能 | 说明 |
|------|------|
| **模型合并** | 合并多个模型的权重 |
| **检查点选择** | 选择最优检查点 |
| **权重平均** | SWA/EMA 权重平均 |

**入口：** 侧边栏 → 模型手术

### 7.2 模型合并

#### 7.2.1 合并方法

| 方法 | 说明 | 适用场景 |
|------|------|----------|
| **SLERP** | 球面线性插值 | 两个模型合并 |
| **TIES** | 任务向量合并 | 多个微调模型 |
| **DARE** | 稀疏权重合并 | 保留重要权重 |
| **Linear** | 线性插值 | 简单平均 |

#### 7.2.2 使用步骤

1. 选择 **合并方法**
2. 添加要合并的模型（至少 2 个）
3. 设置各模型的权重比例
4. 配置输出路径
5. 点击 **开始合并**

#### 7.2.3 参数说明

| 参数 | 说明 |
|------|------|
| 模型路径 | 要合并的模型路径 |
| 权重比例 | 各模型的权重（总和为 1） |
| 输出路径 | 合并后模型的保存路径 |

### 7.3 检查点选择

#### 7.3.1 功能说明

从训练过程中的多个检查点中选择最优的一个。

#### 7.3.2 选择策略

| 策略 | 说明 |
|------|------|
| **最低 Loss** | 选择验证 Loss 最低的检查点 |
| **最高 Reward** | 选择平均 Reward 最高的检查点 |
| **最后检查点** | 选择最新的检查点 |
| **自定义** | 手动选择特定步数的检查点 |

#### 7.3.3 使用步骤

1. 选择训练任务
2. 查看所有检查点列表
3. 查看各检查点的指标
4. 选择目标检查点
5. 点击 **导出模型**

### 7.4 权重平均 (SWA)

#### 7.4.1 功能说明

Stochastic Weight Averaging：对多个检查点进行权重平均，提升泛化能力。

#### 7.4.2 使用步骤

1. 选择训练任务
2. 选择要平均的检查点范围
3. 设置平均方式：
   - **均匀平均**：所有检查点权重相同
   - **指数加权**：越新的检查点权重越大
4. 点击 **执行 SWA**

---

## 8. 自定义评估

### 8.1 功能介绍

评估中心支持对训练的模型进行多维度评估。

**入口：** 侧边栏 → 评估

### 8.2 评估数据集管理

#### 8.2.1 上传评估数据集

1. 点击 **上传评估集**
2. 选择文件（JSON/JSONL/Parquet）
3. 填写数据集名称和描述
4. 选择评估类型

#### 8.2.2 评估数据集格式

```json
[
  {
    "prompt": "问题内容",
    "reference": "参考答案",
    "category": "math"
  }
]
```

| 字段 | 说明 | 必填 |
|------|------|------|
| prompt | 评估问题 | 是 |
| reference | 参考答案 | 是 |
| category | 问题类别 | 否 |

### 8.3 创建评估任务

#### 8.3.1 配置评估

| 配置项 | 说明 |
|--------|------|
| 评估名称 | 自定义名称 |
| 模型/检查点 | 要评估的模型或检查点 |
| 评估数据集 | 选择评估数据集 |
| 评估指标 | 选择评估指标 |

#### 8.3.2 评估指标

| 指标 | 说明 | 适用场景 |
|------|------|----------|
| **Accuracy** | 准确率 | 分类/选择题 |
| **BLEU** | 翻译质量 | 生成任务 |
| **ROUGE** | 摘要质量 | 摘要任务 |
| **Exact Match** | 精确匹配 | QA 任务 |
| **Pass@k** | 代码通过率 | 代码生成 |

#### 8.3.3 生成参数

| 参数 | 说明 | 默认值 |
|------|------|--------|
| Max Tokens | 最大生成长度 | 512 |
| Temperature | 温度 | 0.7 |
| Top-p | 核采样 | 0.9 |
| 采样数 | 每个问题生成几个回答 | 1 |

### 8.4 查看评估结果

#### 8.4.1 总体结果

- 各指标的得分
- 各类别的得分分布
- 与基线模型的对比

#### 8.4.2 详细分析

- 查看每条评估样本的结果
- 模型回答 vs 参考答案
- 错误样本分析

### 8.5 模型对比

#### 8.5.1 创建对比

1. 点击 **新建对比**
2. 选择要对比的模型/检查点（2-5 个）
3. 选择评估数据集
4. 开始对比评估

#### 8.5.2 对比结果

- 各模型在各指标上的对比图表
- 各模型在不同类别上的表现
- 具体样本的差异对比

---

## 9. 常见问题

### 9.1 连接问题

**Q: SSH 连接失败**
```
A: 检查以下项目：
1. 服务器地址和端口是否正确
2. 用户名密码是否正确
3. 服务器是否开机
4. 是否安装了 paramiko：pip install paramiko
```

**Q: 测试连接成功但无法读取模型/数据集**
```
A: 检查工作目录配置是否正确，确保远程路径存在
```

### 9.2 训练问题

**Q: OOM（显存不足）**
```
A: 尝试以下方法：
1. 减小 batch_size
2. 减小序列长度
3. 启用 LoRA
4. 启用梯度检查点
5. 使用计算配置器获取推荐参数
```

**Q: Flash Attention 错误**
```
A: 修改 verl 使用 SDPA：
find /path/to/verl -name "*.py" -exec sed -i 's/flash_attention_2/sdpa/g' {} \;
```

**Q: 训练 Loss 不下降**
```
A: 检查以下项目：
1. 学习率是否合适
2. 数据集格式是否正确
3. 数据集是否有问题（使用数据集审核功能检查）
```

### 9.3 数据问题

**Q: 数据集上传失败**
```
A: 检查：
1. 文件格式是否正确（JSON/JSONL/Parquet）
2. 文件编码是否为 UTF-8
3. JSON 格式是否正确
```

**Q: 数据集同步失败**
```
A: 检查：
1. SSH 连接是否正常
2. 远程目录是否有写权限
3. 磁盘空间是否充足
```

---

## 附录

### A. 目录结构

**Local 模式：**
```
train_platform/
├── models/              # 模型目录
├── datasets/            # 数据集目录
└── outputs/             # 输出目录
```

**SSH 模式（远程服务器）：**
```
~/verl_jobs/
├── datasets/            # 数据集
├── models/              # 模型（可选）
├── outputs/             # 训练输出
└── logs/                # 日志

~/autodl-tmp/            # 大文件存储（AutoDL）
└── qwen_0.5b/           # 模型
```

### B. 支持的模型

| 模型系列 | 支持的大小 |
|----------|-----------|
| Qwen2.5 | 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B |
| LLaMA 3 | 8B, 70B |
| Mistral | 7B |

### C. GPU 显存参考

| 模型大小 | SFT (Full) | SFT (LoRA) | GRPO/PPO |
|----------|------------|------------|----------|
| 0.5B | 8GB | 4GB | 16GB |
| 7B | 40GB | 16GB | 80GB |
| 14B | 80GB | 32GB | 160GB |

---

## 10. 安全性说明

平台已通过安全加固，实现以下安全特性：

### 10.1 SSH 密码加密存储

所有 SSH 密码使用 **Fernet 对称加密**存储，配置文件中不保存明文密码。

**设置加密密钥（生产环境必须）**：

```bash
# 生成密钥
python training_platform/core/crypto_utils.py

# 设置环境变量
export PLATFORM_SECRET_KEY='your-generated-key-here'

# 添加到 shell 配置文件
echo "export PLATFORM_SECRET_KEY='your-key'" >> ~/.bashrc
```

### 10.2 命令注入防护

所有命令执行都经过安全处理：

- **本地执行**：使用命令列表而非字符串，避免 `shell=True`
- **SSH 执行**：所有参数使用 `shlex.quote()` 转义
- **路径验证**：检测危险模式（管道、命令替换、路径遍历）

**安全工具**：
- `command_utils.py` - 提供安全命令构造工具
- `validate_path()` - 路径安全验证
- `SafeCommands` - 预定义安全命令模板

### 10.3 环境隔离

- **虚拟环境**：所有依赖在虚拟环境中隔离
- **固定版本**：依赖版本固定，防止供应链攻击
- **权限最小化**：SSH 用户仅需要训练相关权限

### 10.4 安全最佳实践

1. **不要暴露平台到公网**（仅内网使用）
2. **定期更新加密密钥**
3. **使用 SSH 密钥认证**优于密码认证
4. **定期备份配置文件**（已加密）
5. **监控异常登录和命令执行**

### 10.5 已知限制

- Session 生命周期问题将在 Phase 1 修复
- API 端点尚未全部添加速率限制
- 建议仅在受信任的网络环境中使用

---

*最后更新: 2026-01-08 (Phase 0 安全加固完成)*
