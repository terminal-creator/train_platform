# å…³é”®é—®é¢˜ä¿®å¤æ€»ç»“

åŸºäºä»£ç å¤æŸ¥ï¼Œä¿®å¤äº†ä¸‰ä¸ªä¼šåœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒç¿»è½¦çš„å…³é”®é—®é¢˜ï¼š

---

## âœ… ä¿®å¤ 1: å®Œå–„ Stage task_id å…¥åº“å’Œæ¢å¤æœºåˆ¶

### é—®é¢˜æè¿°
- Pipeline root task_id å†™å…¥äº† DBï¼Œä½†æ¯ä¸ª stage çš„ `celery_task_id` æ²¡æœ‰è®°å½•
- Stage çš„å¼€å§‹/ç»“æŸæ—¶é—´æ²¡æœ‰æ›´æ–°
- Resume æœºåˆ¶ä¸å®Œæ•´ï¼Œå¯èƒ½é‡æ–°æ‰§è¡Œå·²å®Œæˆçš„ stages

### ä¿®å¤æ–¹æ¡ˆ

**1. åˆ›å»ºåŒ…è£… Task (`execute_stage_with_tracking`)**

```python
@app.task(bind=True, name="training_platform.core.pipeline_executor.execute_stage_with_tracking")
def execute_stage_with_tracking(
    self,
    pipeline_uuid: str,
    stage_name: str,
    celery_task_name: str,
    task_params: Dict[str, Any],
):
    """
    åŒ…è£… taskï¼Œå®Œæ•´è®°å½• stage çŠ¶æ€ï¼š
    1. è®°å½• celery_task_id (self.request.id)
    2. æ›´æ–°çŠ¶æ€ä¸º RUNNING å¹¶è®°å½• started_at
    3. æ‰§è¡Œå®é™…çš„è®­ç»ƒ task
    4. æ›´æ–°çŠ¶æ€ä¸º COMPLETED/FAILED å¹¶è®°å½• completed_at
    """
```

**2. ä¿®æ”¹ _create_stage_task()**

```python
# ä¹‹å‰ï¼šç›´æ¥åˆ›å»º task signature
task_sig = signature(celery_task_name, kwargs=node.params)

# ä¹‹åï¼šä½¿ç”¨åŒ…è£… task
task_sig = signature(
    "training_platform.core.pipeline_executor.execute_stage_with_tracking",
    args=(self.pipeline_uuid, stage_name, celery_task_name, node.params),
)
```

**3. å®Œå–„ resume() å‡½æ•°**

```python
def resume(self) -> Dict[str, Any]:
    """
    çœŸæ­£çš„æ¢å¤é€»è¾‘ï¼š
    1. ä» DB è¯»å–æ‰€æœ‰ stagesï¼ˆåŒ…å« task_name, task_params, depends_onï¼‰
    2. æ‰¾å‡ºå·²å®Œæˆçš„ stagesï¼ˆstatus == COMPLETEDï¼‰
    3. è¿‡æ»¤æ‰å·²å®Œæˆçš„ stages
    4. è°ƒæ•´ä¾èµ–å…³ç³»ï¼ˆç§»é™¤å·²å®Œæˆçš„ä¾èµ–ï¼‰
    5. é‡æ–°æ‰§è¡Œå‰©ä½™ stages
    """
    # ä» DB é‡å»º stage é…ç½®
    all_stages = []
    for db_stage in db_stages:
        stage_config = {
            "name": db_stage.stage_name,
            "task": db_stage.task_name,  # ä» DB è¯»å–
            "params": db_stage.task_params,  # ä» DB è¯»å–
            "depends_on": db_stage.depends_on,  # ä» DB è¯»å–
        }
        all_stages.append(stage_config)

    # è¿‡æ»¤å·²å®Œæˆçš„ stages
    remaining_stages = [
        stage for stage in all_stages
        if stage["name"] not in completed_stages
    ]

    # è°ƒæ•´ä¾èµ–å…³ç³»
    for stage in remaining_stages:
        stage["depends_on"] = [
            dep for dep in stage.get("depends_on", [])
            if dep not in completed_stages
        ]

    # é‡æ–°æ‰§è¡Œ
    return self.execute(remaining_stages)
```

### éªŒè¯ç‚¹

å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼éªŒè¯ä¿®å¤ï¼š

```python
# 1. åˆ›å»ºä¸€ä¸ª pipeline with å¹¶è¡Œå±‚
stages = [
    {"name": "A", "task": "train_model", "params": {...}, "depends_on": []},
    {"name": "B", "task": "train_model", "params": {...}, "depends_on": ["A"]},
    {"name": "C", "task": "train_model", "params": {...}, "depends_on": ["A"]},  # ä¸ B å¹¶è¡Œ
    {"name": "D", "task": "run_evaluation", "params": {...}, "depends_on": ["B", "C"]},
]

executor = PipelineExecutor("test-pipeline")
executor.execute(stages)

# 2. æ£€æŸ¥ DB
with Session(engine) as session:
    repo = PipelineRepository(session)
    stages = repo.get_stages("test-pipeline")

    for stage in stages:
        print(f"{stage.stage_name}:")
        print(f"  celery_task_id: {stage.celery_task_id}")  # åº”è¯¥æœ‰å€¼
        print(f"  started_at: {stage.started_at}")  # åº”è¯¥æœ‰å€¼
        print(f"  completed_at: {stage.completed_at}")  # COMPLETED åº”è¯¥æœ‰å€¼

# 3. æ‰‹åŠ¨è®© stage B å¤±è´¥ï¼Œç„¶å resume
executor.resume()

# åº”è¯¥è·³è¿‡ Aï¼ˆå·²å®Œæˆï¼‰ï¼Œé‡æ–°æ‰§è¡Œ B, C, D
```

---

## âœ… ä¿®å¤ 2: Metrics Persister çš„ print() å’Œå…¨æ–‡ä»¶è¯»å–

### é—®é¢˜æè¿°

**é—®é¢˜ A: å¤§é‡ print() è€Œä¸æ˜¯ logger**
- 14 å¤„ `print()` è°ƒç”¨
- é«˜é¢‘åœºæ™¯ä¸‹ä¼šæ‹–æ…¢ workerã€æ±¡æŸ“æ—¥å¿—
- ä¸åˆ©äºé›†ä¸­åŒ–æ—¥å¿—ç³»ç»Ÿ

**é—®é¢˜ B: sync_metrics_from_file è¯»å…¨æ–‡ä»¶**
- æ¯æ¬¡éƒ½ä»å¤´è¯»å–æ•´ä¸ªæ–‡ä»¶
- æ–‡ä»¶è¶Šå¤§è¶Šæ…¢ï¼ˆO(n) å¤æ‚åº¦ï¼‰
- ä¸é€‚åˆé«˜é¢‘è½®è¯¢

### ä¿®å¤æ–¹æ¡ˆ

**1. å…¨éƒ¨æ”¹ç”¨ logger**

```python
# ä¹‹å‰
print(f"[MetricsPersister] Syncing {len(new_metrics)} new metrics...")

# ä¹‹å
logger.info(f"Syncing {len(new_metrics)} new metrics...")
```

**2. æ–‡ä»¶å¢é‡è¯»å–ï¼ˆä½¿ç”¨ offsetï¼‰**

```python
def sync_metrics_from_file(
    job_uuid: str,
    metrics_file: Path,
    session: Session,
    batch_size: int = 100,
    last_offset: int = 0,  # âœ¨ æ–°å¢å‚æ•°
) -> Dict[str, Any]:
    """
    ä» last_offset å¼€å§‹è¯»å–ï¼Œè€Œä¸æ˜¯ä»å¤´è¯»

    Returns:
        - new_metrics_count: æ–°å¢æŒ‡æ ‡æ•°é‡
        - new_offset: æ–°çš„æ–‡ä»¶ offset  # âœ¨ è¿”å›æ–° offset
        - file_size: å½“å‰æ–‡ä»¶å¤§å°
    """
    file_size = metrics_file.stat().st_size

    # å¦‚æœæ–‡ä»¶æ²¡æœ‰å¢é•¿ï¼Œç›´æ¥è¿”å›
    if file_size <= last_offset:
        return {"new_metrics_count": 0, "new_offset": last_offset, ...}

    with open(metrics_file, 'r') as f:
        # âœ¨ è·³åˆ°ä¸Šæ¬¡è¯»å–çš„ä½ç½®
        f.seek(last_offset)

        for line in f:
            # è§£ææ–°å¢çš„è¡Œ
            ...

        # âœ¨ è®°å½•æ–°çš„ offset
        new_offset = f.tell()

    return {
        "new_metrics_count": len(new_metrics),
        "new_offset": new_offset,  # âœ¨ è¿”å›æ–° offset ä¾›ä¸‹æ¬¡ä½¿ç”¨
        "file_size": file_size,
    }
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# ç¬¬ä¸€æ¬¡åŒæ­¥
result = sync_metrics_from_file("job-123", Path("metrics.jsonl"), session, last_offset=0)
print(result)
# {"new_metrics_count": 100, "new_offset": 12345, "file_size": 15000}

# ç¬¬äºŒæ¬¡åŒæ­¥ï¼ˆåªè¯»å–æ–°å¢éƒ¨åˆ†ï¼‰
result = sync_metrics_from_file("job-123", Path("metrics.jsonl"), session, last_offset=12345)
print(result)
# {"new_metrics_count": 50, "new_offset": 15000, "file_size": 15000}
```

### æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | ä¹‹å‰ï¼ˆè¯»å…¨æ–‡ä»¶ï¼‰ | ä¹‹åï¼ˆå¢é‡è¯»å–ï¼‰ |
|------|----------------|----------------|
| æ–‡ä»¶ 1MB, é¦–æ¬¡è¯»å– | 1MB | 1MB |
| æ–‡ä»¶ 1MB, æ–°å¢ 10KB | 1MB | 10KB âœ… |
| æ–‡ä»¶ 100MB, æ–°å¢ 10KB | 100MB | 10KB âœ… |

---

## âœ… ä¿®å¤ 3: validate_file_path çš„å‰¯ä½œç”¨

### é—®é¢˜æè¿°

```python
# ä¹‹å‰çš„å®ç°
def validate_file_path(file_path: str) -> str:
    for allowed_dir in ALLOWED_DATASET_DIRS:
        os.makedirs(allowed_dir, exist_ok=True)  # âŒ å‰¯ä½œç”¨ï¼
        allowed_real = os.path.realpath(allowed_dir)
        ...
```

**å‰¯ä½œç”¨ï¼š**
- æ ¡éªŒå‡½æ•°åœ¨è®¿é—®æ—¶åˆ›å»ºç›®å½•
- é”™è¯¯é…ç½®ä¸‹ä¼šåˆ›å»ºä¸€å †ç›®å½•
- æƒé™/æ²™ç®±è¾¹ç•Œæ¨¡ç³Š

### ä¿®å¤æ–¹æ¡ˆ

```python
def validate_file_path(file_path: str) -> str:
    """
    **Important**: This function is pure validation - no side effects.
    It does NOT create directories. Allowed directories should be
    created at deployment time.
    """
    for allowed_dir in ALLOWED_DATASET_DIRS:
        # âœ… åªåš expand å’Œ realpathï¼Œä¸åˆ›å»ºç›®å½•
        allowed_expanded = os.path.expanduser(allowed_dir)
        allowed_real = os.path.realpath(allowed_expanded)

        # âœ… ä½¿ç”¨ commonpath æ£€æŸ¥ï¼ˆæ›´å¥å£®ï¼‰
        try:
            common = os.path.commonpath([allowed_real, real_path])
            if common == allowed_real:
                is_allowed = True
                break
        except ValueError:
            # Paths on different drives or not comparable
            continue
```

### éƒ¨ç½²è¦æ±‚

**åœ¨éƒ¨ç½²æ—¶é¢„åˆ›å»º allowed ç›®å½•ï¼š**

```bash
# Docker entrypoint.sh
mkdir -p ~/train_platform/datasets
mkdir -p ~/datasets
mkdir -p ./datasets
mkdir -p ./data

# æˆ–åœ¨ docker-compose.yml
volumes:
  - ./datasets:/app/datasets  # ç¡®ä¿ç›®å½•å­˜åœ¨
```

---

## ğŸ“Š ä¿®å¤æ€»ç»“

| é—®é¢˜ | å½±å“ | ä¿®å¤çŠ¶æ€ | éªŒè¯æ–¹å¼ |
|------|------|---------|---------|
| Stage task_id æœªå…¥åº“ | âŒ Resume ä¸å¯é ã€çŠ¶æ€ä¸å¯è¿½è¸ª | âœ… å®Œå…¨ä¿®å¤ | æ£€æŸ¥ DB stage.celery_task_id |
| print() ä»£æ›¿ logger | âš ï¸ é«˜é¢‘åœºæ™¯æ€§èƒ½å·®ã€æ—¥å¿—æ±¡æŸ“ | âœ… å®Œå…¨ä¿®å¤ | æœç´¢ä»£ç æ—  print() |
| å…¨æ–‡ä»¶è¯»å– | âš ï¸ å¤§æ–‡ä»¶åœºæ™¯è¶Šæ¥è¶Šæ…¢ | âœ… å®Œå…¨ä¿®å¤ | æµ‹è¯•å¢é‡è¯»å–æ€§èƒ½ |
| validate æœ‰å‰¯ä½œç”¨ | âš ï¸ å¯èƒ½åˆ›å»ºä¸è¯¥æœ‰çš„ç›®å½• | âœ… å®Œå…¨ä¿®å¤ | éªŒè¯æ—  makedirs è°ƒç”¨ |

---

## ğŸ”§ åç»­å»ºè®®

### 1. Pipeline éªŒè¯æµ‹è¯•

å»ºè®®åˆ›å»ºé›†æˆæµ‹è¯•éªŒè¯ Pipeline æ¢å¤æœºåˆ¶ï¼š

```python
def test_pipeline_resume():
    # åˆ›å»º pipeline with 4 stages
    # è®© stage 2 å¤±è´¥
    # è°ƒç”¨ resume()
    # éªŒè¯åªé‡æ–°æ‰§è¡Œ stage 2, 3, 4
    # éªŒè¯ stage 1 ä¸é‡å¤æ‰§è¡Œ
```

### 2. Metrics æ€§èƒ½æµ‹è¯•

å»ºè®®æµ‹è¯•å¤§æ–‡ä»¶åœºæ™¯ï¼š

```bash
# ç”Ÿæˆ 100MB metrics æ–‡ä»¶
python generate_test_metrics.py --size=100MB

# æµ‹è¯•å¢é‡è¯»å–æ€§èƒ½
time python test_sync_metrics.py --last-offset=0
time python test_sync_metrics.py --last-offset=90000000  # 90MB
```

### 3. è·¯å¾„éªŒè¯æµ‹è¯•

å»ºè®®æµ‹è¯•è¾¹ç•Œæƒ…å†µï¼š

```python
def test_path_validation():
    # æµ‹è¯• ../ æ”»å‡»
    validate_file_path("~/datasets/../../../etc/passwd")  # åº”è¯¥æ‹’ç»

    # æµ‹è¯• symlink ç»•è¿‡
    os.symlink("/etc", "~/datasets/etc_link")
    validate_file_path("~/datasets/etc_link/passwd")  # åº”è¯¥æ‹’ç»

    # æµ‹è¯•æ­£å¸¸è·¯å¾„
    validate_file_path("~/datasets/train.parquet")  # åº”è¯¥å…è®¸
```

---

## ğŸ“ å·¥ç¨‹ç»éªŒæ€»ç»“

### å…³äº Pipeline çŠ¶æ€è¿½è¸ª

**æ•™è®­ï¼š**
- Celery task çš„ `task_id` åªæœ‰åœ¨ task æ‰§è¡Œæ—¶æ‰èƒ½è·å¾—ï¼ˆé€šè¿‡ `self.request.id`ï¼‰
- ä¸èƒ½åœ¨æäº¤æ—¶é¢„çŸ¥ task_idï¼Œå¿…é¡»åœ¨ task å†…éƒ¨è®°å½•

**æœ€ä½³å®è·µï¼š**
- ä½¿ç”¨åŒ…è£… task ç»Ÿä¸€å¤„ç†çŠ¶æ€æ›´æ–°
- æ‰€æœ‰çŠ¶æ€å­—æ®µéƒ½è®°å½•åˆ° DBï¼ˆtask_id, started_at, completed_at, result, errorï¼‰
- Resume é€»è¾‘ä» DB é‡å»ºçŠ¶æ€ï¼Œè€Œä¸æ˜¯ä¾èµ–å†…å­˜

### å…³äºæ–‡ä»¶è¯»å–æ€§èƒ½

**æ•™è®­ï¼š**
- è®­ç»ƒ metrics æ–‡ä»¶ä¼šæŒç»­å¢é•¿ï¼ˆå‡ å°æ—¶è®­ç»ƒå¯èƒ½è¾¾åˆ°å‡  GBï¼‰
- æ¯æ¬¡ä»å¤´è¯»å–ä¼šå¯¼è‡´æ€§èƒ½çº¿æ€§ä¸‹é™
- `print()` åœ¨é«˜é¢‘åœºæ™¯ä¸‹ä¼šæˆä¸ºç“¶é¢ˆ

**æœ€ä½³å®è·µï¼š**
- ä½¿ç”¨ `f.seek(offset)` å¢é‡è¯»å–
- ä½¿ç”¨ `logger` è€Œä¸æ˜¯ `print()`
- è¿”å›æ–° offset ä¾›ä¸‹æ¬¡ä½¿ç”¨

### å…³äºå®‰å…¨æ ¡éªŒ

**æ•™è®­ï¼š**
- æ ¡éªŒå‡½æ•°ä¸åº”è¯¥æœ‰å‰¯ä½œç”¨ï¼ˆåˆ›å»ºç›®å½•ã€ä¿®æ”¹æ–‡ä»¶ç­‰ï¼‰
- å‰¯ä½œç”¨ä¼šè®©ç³»ç»Ÿè¡Œä¸ºéš¾ä»¥é¢„æµ‹

**æœ€ä½³å®è·µï¼š**
- æ ¡éªŒåªåšæ£€æŸ¥ï¼Œä¸åšä¿®æ”¹
- ä½¿ç”¨ `os.path.commonpath` è€Œä¸æ˜¯å­—ç¬¦ä¸²å‰ç¼€åŒ¹é…
- åœ¨éƒ¨ç½²æ—¶é¢„åˆ›å»ºå¿…è¦çš„ç›®å½•

---

è¿™äº›ä¿®å¤ä½¿ç³»ç»Ÿä»"èƒ½è·‘"å˜æˆ"èƒ½åœ¨ç”Ÿäº§ç¯å¢ƒç¨³å®šè·‘"ï¼âœ…
