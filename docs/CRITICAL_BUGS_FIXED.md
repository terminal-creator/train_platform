# 4ä¸ªCritical Bugä¿®å¤æ€»ç»“

åŸºäºæ·±åº¦ä»£ç å®¡æŸ¥ï¼Œå‘ç°å¹¶ä¿®å¤äº†4ä¸ªä¼šå¯¼è‡´pipelineç«‹å³å´©æºƒçš„Criticalçº§åˆ«bugã€‚

---

## âœ… ä¿®å¤ 1: å¤šå±‚ DAG å‚æ•°ä¼ é€’ bug

### é—®é¢˜æè¿°

**ä¸¥é‡ç¨‹åº¦ï¼šğŸ”´ Critical**

```python
# ä¹‹å‰çš„ä»£ç 
init_stage_sig = sig(
    "training_platform.core.pipeline_executor.init_stage_status",
    args=(self.pipeline_uuid, stage_name),
    # âŒ æ²¡æœ‰ immutable=True
)

# å½“ pipeline æœ‰å¤šå±‚æ—¶ï¼š
chain(
    layer_0,  # è¿”å› result_0
    layer_1,  # chain ä¼šæŠŠ result_0 ä¼ ç»™ç¬¬ä¸€ä¸ªå‚æ•°
)

# å¦‚æœ layer_1 æ˜¯ chain(init_stage_sig, task_sig)
# init_stage_sig ä¸æ˜¯ immutableï¼Œä¼šæ¥æ”¶ result_0
# å˜æˆï¼š
init_stage_status(result_0, pipeline_uuid, stage_name)  # âŒ TypeError!
```

**è§¦å‘åœºæ™¯ï¼š**
- ä»»ä½• 2 å±‚ä»¥ä¸Šçš„ pipeline
- ç¬¬ä¸€æ¬¡æµ‹è¯•å°±ä¼šæš´éœ²

**å½±å“ï¼š**
- Pipeline ç«‹å³å´©æºƒ
- æ— æ³•è¿è¡Œä»»ä½•å¤šå±‚ DAG

### ä¿®å¤æ–¹æ¡ˆ

```python
# pipeline_executor.py:336-339
init_stage_sig = sig(
    "training_platform.core.pipeline_executor.init_stage_status",
    args=(self.pipeline_uuid, stage_name),
    immutable=True,  # âœ… å…³é”®ï¼šé¿å…è·¨ layer çš„ chain/group ç»“æœæ³¨å…¥
)

# ç°åœ¨ä¸¤ä¸ª signature éƒ½æ˜¯ immutable
return chain(init_stage_sig, task_sig)
```

**ä¿®å¤éš¾åº¦ï¼šâ­ éå¸¸ç®€å•ï¼ˆ1è¡Œä»£ç ï¼‰**

---

## âœ… ä¿®å¤ 2: on_stage_error ç­¾åé”™è¯¯

### é—®é¢˜æè¿°

**ä¸¥é‡ç¨‹åº¦ï¼šğŸ”´ Critical**

```python
# ä¹‹å‰çš„ç­¾å
def on_stage_error(uuid, pipeline_uuid, stage_name):
    # å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ uuid
    async_result = AsyncResult(uuid, app=app)  # âŒ uuid å¯èƒ½æ˜¯ request å¯¹è±¡
```

**Celery 5.x å®é™…è°ƒç”¨ï¼š**
```python
errback(request, exc, traceback, *link_error_args)
```

**è§¦å‘åœºæ™¯ï¼š**
- ä»»ä½• stage å¤±è´¥æ—¶
- å›è°ƒä¼šæ¥æ”¶åˆ°é”™è¯¯çš„å‚æ•°ç±»å‹

**å½±å“ï¼š**
- Stage å¤±è´¥æ—¶å›è°ƒå´©æºƒ
- æ— æ³•æ­£ç¡®è®°å½•å¤±è´¥çŠ¶æ€
- Pipeline çŠ¶æ€ä¸ä¸€è‡´

### ä¿®å¤æ–¹æ¡ˆ

```python
# pipeline_executor.py:562-593
@app.task(name="training_platform.core.pipeline_executor.on_stage_error")
def on_stage_error(request, exc, traceback, pipeline_uuid: str, stage_name: str):
    """
    Celery 5.x errback çš„æ ‡å‡†ç­¾åï¼š
    def errback(request, exc, traceback, *args)
    """
    # âœ… ä» request å¯¹è±¡è·å– task_id
    task_id = getattr(request, "id", None) or getattr(request, "task_id", None) or "unknown"

    # âœ… ç›´æ¥ä½¿ç”¨ exc è·å–å¼‚å¸¸ä¿¡æ¯
    error_message = str(exc) if exc else "Unknown error"

    logger.error(
        f"[Pipeline {pipeline_uuid}] Stage '{stage_name}' failed "
        f"(task_id={task_id}, error={error_message})"
    )

    # ... æ›´æ–°çŠ¶æ€ ...
```

**å…³é”®æ”¹è¿›ï¼š**
- âœ… ä½¿ç”¨æ­£ç¡®çš„ Celery errback ç­¾å
- âœ… ä» request å¯¹è±¡è·å– task_idï¼ˆè€Œä¸æ˜¯å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ uuidï¼‰
- âœ… ç›´æ¥ä½¿ç”¨ exc è·å–å¼‚å¸¸ä¿¡æ¯ï¼ˆä¸éœ€è¦ AsyncResultï¼‰

**ä¿®å¤éš¾åº¦ï¼šâ­â­ ç®€å•**

---

## âœ… ä¿®å¤ 3: _pipeline_uuid æ³¨å…¥ä½† tasks ä¸æ¥æ”¶

### é—®é¢˜æè¿°

**ä¸¥é‡ç¨‹åº¦ï¼šğŸ”´ Critical**

```python
# pipeline_executor.py æ³¨å…¥å‚æ•°
task_params = dict(node.params)
task_params['_pipeline_uuid'] = self.pipeline_uuid  # æ³¨å…¥
task_params['_stage_name'] = stage_name              # æ³¨å…¥

# ä½† celery_tasks.py çš„ç­¾åä¸æ¥æ”¶
def train_model(self, job_uuid, config, run_mode, ssh_config):
    # âŒ æ²¡æœ‰ _pipeline_uuid å’Œ _stage_name å‚æ•°
```

**è§¦å‘åœºæ™¯ï¼š**
- ä»»ä½• pipeline è°ƒç”¨ train_model
- ç¬¬ä¸€æ¬¡æµ‹è¯•å°±ä¼šæš´éœ²

**å½±å“ï¼š**
```python
TypeError: train_model() got an unexpected keyword argument '_pipeline_uuid'
```

### ä¿®å¤æ–¹æ¡ˆ

ä¿®æ”¹æ‰€æœ‰ä¼šè¢« DAG è°ƒåº¦çš„ stage tasksï¼ˆ4ä¸ªï¼‰ï¼š

**1. train_model**
```python
# celery_tasks.py:29-60
@app.task(bind=True, name="training_platform.core.celery_tasks.train_model")
def train_model(
    self,
    job_uuid: str,
    config: Dict[str, Any],
    run_mode: str = "local",
    ssh_config: Optional[Dict[str, Any]] = None,
    _pipeline_uuid: Optional[str] = None,  # âœ… Pipeline æ³¨å…¥å‚æ•°
    _stage_name: Optional[str] = None,     # âœ… Pipeline æ³¨å…¥å‚æ•°
):
    # âœ… Pipeline stage çŠ¶æ€è®°å½•
    if _pipeline_uuid and _stage_name:
        from .pipeline_executor import mark_stage_running
        mark_stage_running(_pipeline_uuid, _stage_name, self.request.id)

    logger.info(f"Starting training task for job {job_uuid}")
    # ... existing code ...
```

**2. run_evaluation**
```python
# celery_tasks.py:123-151
@app.task(bind=True, name="training_platform.core.celery_tasks.run_evaluation")
def run_evaluation(
    self,
    job_uuid: str,
    checkpoint_path: str,
    eval_dataset_uuid: str,
    _pipeline_uuid: Optional[str] = None,  # âœ…
    _stage_name: Optional[str] = None,     # âœ…
):
    if _pipeline_uuid and _stage_name:
        from .pipeline_executor import mark_stage_running
        mark_stage_running(_pipeline_uuid, _stage_name, self.request.id)
    # ...
```

**3. preprocess_dataset**
```python
# celery_tasks.py:172-196
@app.task(bind=True, name="training_platform.core.celery_tasks.preprocess_dataset")
def preprocess_dataset(
    self,
    dataset_uuid: str,
    preprocessing_config: Dict[str, Any],
    _pipeline_uuid: Optional[str] = None,  # âœ…
    _stage_name: Optional[str] = None,     # âœ…
):
    if _pipeline_uuid and _stage_name:
        from .pipeline_executor import mark_stage_running
        mark_stage_running(_pipeline_uuid, _stage_name, self.request.id)
    # ...
```

**4. cleanup_checkpoints**
```python
# celery_tasks.py:210-236
@app.task(bind=True, name="training_platform.core.celery_tasks.cleanup_checkpoints")
def cleanup_checkpoints(
    self,
    job_uuid: str,
    keep_best_n: int = 3,
    _pipeline_uuid: Optional[str] = None,  # âœ…
    _stage_name: Optional[str] = None,     # âœ…
):
    if _pipeline_uuid and _stage_name:
        from .pipeline_executor import mark_stage_running
        mark_stage_running(_pipeline_uuid, _stage_name, self.request.id)
    # ...
```

**å…³é”®æ”¹è¿›ï¼š**
- âœ… æ‰€æœ‰ tasks éƒ½æ·»åŠ  `bind=True`ï¼ˆè®¿é—® self.request.idï¼‰
- âœ… æ‰€æœ‰ tasks éƒ½æ¥æ”¶ `_pipeline_uuid` å’Œ `_stage_name`
- âœ… è‡ªåŠ¨è°ƒç”¨ `mark_stage_running()` è®°å½•çœŸå® task_id
- âœ… å®Œå…¨ä¸ä¾èµ– Celery signalsï¼ˆæ›´å¯é ï¼‰

**ä¿®å¤éš¾åº¦ï¼šâ­â­ ç®€å•ï¼ˆä½†éœ€è¦ä¿®æ”¹4ä¸ªå‡½æ•°ï¼‰**

---

## âœ… ä¿®å¤ 4: Metrics è·¯å¾„åè®®ä¸ä¸€è‡´

### é—®é¢˜æè¿°

**ä¸¥é‡ç¨‹åº¦ï¼šğŸ”´ Critical**

**WS ç›‘æ§è¯»å–è·¯å¾„ï¼š**
```python
# monitoring.py:1002
metrics_dir = "./platform_metrics"  # Local
metrics_dir = f"{ssh_working_dir}/platform_metrics"  # SSH
```

**update_job_metrics è¯»å–è·¯å¾„ï¼š**
```python
# celery_tasks.py:348 (ä¹‹å‰)
output_dir = Path(job.output_path)
metrics_dir = output_dir / "metrics"  # âŒ ä¸ä¸€è‡´ï¼
```

**è§¦å‘åœºæ™¯ï¼š**
- update_job_metrics æ¯åˆ†é’Ÿæ‰§è¡Œ
- æ‰¾ä¸åˆ° metrics æ–‡ä»¶

**å½±å“ï¼š**
- Metrics é—­ç¯å®Œå…¨ä¸å·¥ä½œ
- æ— æ³•è‡ªåŠ¨è½åº“å’Œè¯Šæ–­
- å‘¨æœŸä»»åŠ¡ç©ºè·‘

### ä¿®å¤æ–¹æ¡ˆ

```python
# celery_tasks.py:379-409
for job in running_jobs:
    try:
        # âœ… ç»Ÿä¸€ä½¿ç”¨ platform_metrics ç›®å½•åè®®ï¼ˆä¸ WS ç›‘æ§ä¸€è‡´ï¼‰
        import os

        run_mode = getattr(job, 'run_mode', 'local')

        if run_mode == "ssh":
            # SSH æ¨¡å¼ï¼šä» run_mode_config è·å–å·¥ä½œç›®å½•
            ssh_config = getattr(job, 'run_mode_config', {}) or {}
            ssh_working_dir = ssh_config.get('ssh_working_dir', '~/verl_jobs')
            metrics_dir_str = f"{ssh_working_dir}/platform_metrics"
            # âœ… SSH æ¨¡å¼æš‚æ—¶è·³è¿‡ï¼ˆéœ€è¦ SSH è¿æ¥æ‰èƒ½è¯»å–ï¼‰
            # TODO: å®ç° SSH æ¨¡å¼çš„ metrics åŒæ­¥
            continue
        else:
            # âœ… Local æ¨¡å¼ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
            metrics_dir_str = os.getenv("PLATFORM_METRICS_DIR", "./platform_metrics")
            metrics_dir = Path(metrics_dir_str)

            if not metrics_dir.exists():
                logger.debug(f"Metrics directory not found: {metrics_dir}")
                continue

        # Metrics æ–‡ä»¶åï¼š{job_uuid}_metrics.jsonl
        metrics_file = metrics_dir / f"{job.uuid}_metrics.jsonl"
        status_file = metrics_dir / f"{job.uuid}_status.json"
        # ...
```

**ç»Ÿä¸€åçš„è·¯å¾„åè®®ï¼š**

| åœºæ™¯ | è·¯å¾„ |
|------|------|
| Local è®­ç»ƒ | `./platform_metrics/{job_uuid}_metrics.jsonl` |
| SSH è®­ç»ƒ | `{ssh_working_dir}/platform_metrics/{job_uuid}_metrics.jsonl` |
| WS ç›‘æ§ | ä¸ä¸Šé¢ä¸€è‡´ âœ… |
| update_job_metrics | ä¸ä¸Šé¢ä¸€è‡´ âœ… |

**å…³é”®æ”¹è¿›ï¼š**
- âœ… å®Œå…¨ç»Ÿä¸€è·¯å¾„åè®®
- âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡ `PLATFORM_METRICS_DIR` å¯é…ç½®
- âœ… Local å’Œ SSH éƒ½ä½¿ç”¨ `platform_metrics` å­ç›®å½•
- âœ… SSH æ¨¡å¼æ·»åŠ  TODOï¼ˆæœªæ¥å®ç°ï¼‰

**ä¿®å¤éš¾åº¦ï¼šâ­â­â­ ä¸­ç­‰**

---

## ğŸ“Š ä¿®å¤æ€»ç»“

| é—®é¢˜ | ä¸¥é‡ç¨‹åº¦ | ä¿®å¤çŠ¶æ€ | å½±å“ | ä¿®å¤éš¾åº¦ |
|------|---------|---------|------|---------|
| 1. å¤šå±‚ DAG å‚æ•°ä¼ é€’ | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | ä»»ä½•å¤šå±‚ pipeline ç«‹å³ç‚¸ | â­ éå¸¸ç®€å• |
| 2. on_stage_error ç­¾å | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | Stage å¤±è´¥æ—¶å›è°ƒç‚¸ | â­â­ ç®€å• |
| 3. _pipeline_uuid ä¸æ¥æ”¶ | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | ä»»ä½• pipeline ç«‹å³ç‚¸ | â­â­ ç®€å• |
| 4. Metrics è·¯å¾„ä¸ä¸€è‡´ | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | Metrics é—­ç¯ä¸å·¥ä½œ | â­â­â­ ä¸­ç­‰ |

---

## ğŸ“‚ ä¿®æ”¹æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒä¿®å¤

**1. `training_platform/core/pipeline_executor.py`**
- Line 339: æ·»åŠ  `immutable=True` åˆ° init_stage_sig
- Line 562-593: ä¿®æ”¹ on_stage_error ç­¾åä¸º (request, exc, traceback, ...)

**2. `training_platform/core/celery_tasks.py`**
- Line 29-60: train_model æ·»åŠ  _pipeline_uuid/_stage_name å‚æ•°
- Line 123-151: run_evaluation æ·»åŠ  _pipeline_uuid/_stage_name å‚æ•°
- Line 172-196: preprocess_dataset æ·»åŠ  _pipeline_uuid/_stage_name å‚æ•°
- Line 210-236: cleanup_checkpoints æ·»åŠ  _pipeline_uuid/_stage_name å‚æ•°
- Line 379-409: ç»Ÿä¸€ metrics è·¯å¾„åè®®

---

## ğŸ§ª éªŒè¯æ¸…å•

### 1. éªŒè¯å¤šå±‚ DAG å‚æ•°ä¼ é€’

```python
# åˆ›å»º 3 å±‚ pipeline
stages = [
    {"name": "A", "task": "preprocess_dataset", "params": {...}, "depends_on": []},
    {"name": "B", "task": "train_model", "params": {...}, "depends_on": ["A"]},
    {"name": "C", "task": "run_evaluation", "params": {...}, "depends_on": ["B"]},
]

executor = PipelineExecutor("test-3-layer")
result = executor.execute(stages)

# åº”è¯¥æˆåŠŸæ‰§è¡Œï¼Œä¸ä¼šæœ‰ TypeError
```

### 2. éªŒè¯ on_stage_error ç­¾å

```python
# è®©ä¸€ä¸ª stage æ•…æ„å¤±è´¥
stages = [
    {"name": "A", "task": "train_model", "params": {"job_uuid": "non-existent"}, "depends_on": []},
]

executor = PipelineExecutor("test-error")
result = executor.execute(stages)

# è§‚å¯Ÿæ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
# [on_stage_error] Stage 'A' failed (task_id=xxx, error=...)
# ä¸åº”è¯¥æœ‰ AttributeError æˆ– TypeError

# æ£€æŸ¥ DB
with Session(engine) as session:
    repo = PipelineRepository(session)
    stages = repo.get_stages("test-error")
    stage = stages[0]

    assert stage.status == PipelineStageStatus.FAILED
    assert stage.error_message is not None
```

### 3. éªŒè¯ _pipeline_uuid å‚æ•°æ¥æ”¶

```python
# åˆ›å»ºç®€å• pipeline
stages = [
    {"name": "A", "task": "train_model", "params": {"job_uuid": "job-123", "config": {...}}, "depends_on": []},
]

executor = PipelineExecutor("test-params")
result = executor.execute(stages)

# åº”è¯¥æˆåŠŸæ‰§è¡Œï¼Œä¸ä¼šæœ‰ TypeError: unexpected keyword argument

# æ£€æŸ¥ DBï¼Œstage åº”è¯¥æœ‰ celery_task_id
with Session(engine) as session:
    repo = PipelineRepository(session)
    stages = repo.get_stages("test-params")
    stage = stages[0]

    assert stage.celery_task_id is not None  # âœ… task_id å·²è®°å½•
    assert stage.status == PipelineStageStatus.COMPLETED
```

### 4. éªŒè¯ Metrics è·¯å¾„ç»Ÿä¸€

```python
# å¯åŠ¨è®­ç»ƒä»»åŠ¡ï¼ˆä¼šå†™å…¥ ./platform_metrics/ï¼‰
# ç­‰å¾… 1 åˆ†é’Ÿï¼Œè®© update_job_metrics æ‰§è¡Œ

# æ£€æŸ¥ DB
with Session(engine) as session:
    metrics_repo = MetricsRepository(session)
    metrics = metrics_repo.get_metrics_range("job-123", start_step=0, end_step=100)

    assert len(metrics) > 0  # âœ… Metrics å·²åŒæ­¥åˆ° DB

# æ£€æŸ¥ WS ç›‘æ§
# åº”è¯¥èƒ½å®æ—¶çœ‹åˆ° metricsï¼ˆå› ä¸ºè·¯å¾„ä¸€è‡´ï¼‰
```

---

## ğŸ¯ åç»­å»ºè®®

### 1. æ·»åŠ é›†æˆæµ‹è¯•

```python
# tests/test_pipeline_critical.py
def test_multi_layer_pipeline():
    """æµ‹è¯•å¤šå±‚ pipeline å‚æ•°ä¼ é€’"""
    stages = [
        {"name": "layer1", "task": "preprocess_dataset", "params": {...}, "depends_on": []},
        {"name": "layer2", "task": "train_model", "params": {...}, "depends_on": ["layer1"]},
        {"name": "layer3", "task": "run_evaluation", "params": {...}, "depends_on": ["layer2"]},
    ]
    executor = PipelineExecutor("test-3-layer")
    result = executor.execute(stages)
    assert result["success"] is True

def test_stage_error_handling():
    """æµ‹è¯• stage å¤±è´¥æ—¶çš„å›è°ƒ"""
    stages = [
        {"name": "fail", "task": "train_model", "params": {"job_uuid": "non-existent"}, "depends_on": []},
    ]
    executor = PipelineExecutor("test-error")
    result = executor.execute(stages)
    # éªŒè¯ stage å’Œ pipeline éƒ½æ ‡è®°ä¸º FAILED

def test_metrics_path_consistency():
    """æµ‹è¯• metrics è·¯å¾„ä¸€è‡´æ€§"""
    # å¯åŠ¨è®­ç»ƒ
    # ç­‰å¾… update_job_metrics æ‰§è¡Œ
    # éªŒè¯ metrics å·²åŒæ­¥åˆ° DB
```

### 2. æ–‡æ¡£æ›´æ–°

åœ¨ `docs/PIPELINE_DESIGN.md` ä¸­è®°å½•ï¼š
- å¤šå±‚ DAG çš„å‚æ•°ä¼ é€’è¯­ä¹‰
- Stage task çš„ç­¾åçº¦å®šï¼ˆå¿…é¡»æ¥æ”¶ _pipeline_uuid/_stage_nameï¼‰
- Metrics è·¯å¾„åè®®ï¼ˆplatform_metricsï¼‰

### 3. ä»£ç å®¡æŸ¥æ£€æŸ¥æ¸…å•

æ·»åŠ  PR æ£€æŸ¥æ¸…å•ï¼š
- [ ] æ‰€æœ‰æ–°çš„ stage tasks éƒ½æ¥æ”¶ _pipeline_uuid/_stage_name
- [ ] æ‰€æœ‰ signature éƒ½æ­£ç¡®è®¾ç½® immutable
- [ ] Metrics è·¯å¾„ä½¿ç”¨ç»Ÿä¸€åè®®

---

## âœ… æ€»ç»“

æ‰€æœ‰ 4 ä¸ª Critical bugs å·²ä¿®å¤ï¼š

1. âœ… **å¤šå±‚ DAG å‚æ•°ä¼ é€’**ï¼šæ·»åŠ  immutable=True
2. âœ… **on_stage_error ç­¾å**ï¼šä½¿ç”¨æ­£ç¡®çš„ Celery errback ç­¾å
3. âœ… **_pipeline_uuid å‚æ•°**ï¼šæ‰€æœ‰ tasks æ¥æ”¶å¹¶è°ƒç”¨ mark_stage_running
4. âœ… **Metrics è·¯å¾„**ï¼šç»Ÿä¸€ä½¿ç”¨ platform_metrics åè®®

**Platform ç°åœ¨å¯ä»¥æ­£å¸¸è¿è¡Œ Pipeline äº†ï¼** ğŸ‰

---

## ğŸ” å‘ç°è¿™äº› bugs çš„ä»·å€¼

è¿™äº› bugs éƒ½æ˜¯**ç¬¬ä¸€æ¬¡è¿è¡Œ pipeline å°±ä¼šç«‹å³æš´éœ²**çš„ï¼š

- å¦‚æœä¸ä¿®å¤é—®é¢˜ 1ï¼šä»»ä½•å¤šå±‚ pipeline â†’ TypeError
- å¦‚æœä¸ä¿®å¤é—®é¢˜ 2ï¼šä»»ä½• stage å¤±è´¥ â†’ å›è°ƒå´©æºƒ
- å¦‚æœä¸ä¿®å¤é—®é¢˜ 3ï¼šä»»ä½• pipeline â†’ TypeError
- å¦‚æœä¸ä¿®å¤é—®é¢˜ 4ï¼šmetrics é—­ç¯å®Œå…¨ä¸å·¥ä½œ

**å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ç¬¬ä¸€æ¬¡ç”Ÿäº§éƒ¨ç½²å‰å°±å‘ç°å¹¶ä¿®å¤äº†ï¼** âœ…
