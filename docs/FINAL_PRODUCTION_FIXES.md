# æœ€ç»ˆç”Ÿäº§çº§ä¿®å¤å®Œæˆ

åŸºäºä¹‹å‰çš„ CRITICAL_FIXES.mdï¼Œå®Œæˆäº†æœ€åä¸¤ä¸ªå…³é”®æ”¹è¿›ï¼Œä½¿å¹³å°è¾¾åˆ°ç”Ÿäº§çº§æ ‡å‡†ã€‚

---

## âœ… ä¿®å¤ A: Stage æ”¹ä¸ºçœŸæ­£çš„å¼‚æ­¥æ´¾å‘

### ä¹‹å‰çš„é—®é¢˜
```python
# æ—§çš„ execute_stage_with_trackingï¼šåŒæ­¥æ‰§è¡Œ
def execute_stage_with_tracking(...):
    # ç›´æ¥è°ƒç”¨ task å‡½æ•°ï¼ˆåŒæ­¥ï¼‰
    task_func = task_map.get(celery_task_name)
    result = task_func(**task_params)  # âŒ é˜»å¡æ‰§è¡Œ
```

**é—®é¢˜ï¼š**
- Stage ä¸æ˜¯çœŸæ­£çš„ Celery taskï¼Œæ˜¯åŒæ­¥è°ƒç”¨
- æ— æ³•å–æ¶ˆã€æ— æ³•ç‹¬ç«‹ç›‘æ§
- æ²¡æœ‰é˜Ÿåˆ—éš”ç¦»ï¼Œè®­ç»ƒ/è¯„æµ‹/é¢„å¤„ç†æ··åœ¨ä¸€èµ·
- ä¸æ”¯æŒ stage ç²’åº¦çš„ timeout/retry

### ä¿®å¤æ–¹æ¡ˆ

**1. æ·»åŠ é˜Ÿåˆ—è·¯ç”±é…ç½®**
```python
TASK_REGISTRY = {
    "preprocess_dataset": {
        "task": "training_platform.core.celery_tasks.preprocess_dataset",
        "queue": "preprocessing",  # âœ… é˜Ÿåˆ—éš”ç¦»
    },
    "train_model": {
        "task": "training_platform.core.celery_tasks.train_model",
        "queue": "training",  # âœ… è®­ç»ƒä¸“ç”¨é˜Ÿåˆ—
    },
    "run_evaluation": {
        "task": "training_platform.core.celery_tasks.run_evaluation",
        "queue": "evaluation",  # âœ… è¯„æµ‹ä¸“ç”¨é˜Ÿåˆ—
    },
    "cleanup_checkpoints": {
        "task": "training_platform.core.celery_tasks.cleanup_checkpoints",
        "queue": "maintenance",  # âœ… ç»´æŠ¤ä¸“ç”¨é˜Ÿåˆ—
    },
}
```

**2. ä¿®æ”¹ _create_stage_task() ä½¿ç”¨çœŸæ­£çš„å¼‚æ­¥æ´¾å‘**
```python
def _create_stage_task(self, stage_name: str) -> signature:
    """åˆ›å»º stage çš„ Celery signatureï¼ˆçœŸæ­£çš„å¼‚æ­¥æ´¾å‘ï¼‰"""
    node = self.resolver.nodes[stage_name]

    task_config = self.TASK_REGISTRY.get(node.task_name)
    celery_task_name = task_config["task"]
    task_queue = task_config.get("queue", "default")

    # âœ… åˆ›å»ºçœŸæ­£çš„å¼‚æ­¥ task signature
    task_sig = signature(
        celery_task_name,
        kwargs=node.params,
        options={
            "queue": task_queue,  # âœ… é˜Ÿåˆ—éš”ç¦»
            "link": signature(
                "training_platform.core.pipeline_executor.on_stage_success",
                args=(self.pipeline_uuid, stage_name),
                immutable=True,
            ),
            "link_error": signature(
                "training_platform.core.pipeline_executor.on_stage_error",
                args=(self.pipeline_uuid, stage_name),
                immutable=True,
            ),
        }
    )

    # åœ¨ task æ´¾å‘å‰åˆå§‹åŒ–çŠ¶æ€
    init_stage_sig = signature(
        "training_platform.core.pipeline_executor.init_stage_status",
        args=(self.pipeline_uuid, stage_name),
    )

    # âœ… ç»„åˆï¼šå…ˆåˆå§‹åŒ–çŠ¶æ€ï¼Œå†æ‰§è¡Œå®é™… task
    return chain(init_stage_sig, task_sig)
```

**3. å®ç°ä¸‰ä¸ªå›è°ƒ task**

```python
@app.task(name="training_platform.core.pipeline_executor.init_stage_status")
def init_stage_status(pipeline_uuid: str, stage_name: str):
    """åœ¨ stage æ‰§è¡Œå‰åˆå§‹åŒ–çŠ¶æ€ä¸º PENDING"""
    with Session(engine) as session:
        repo = PipelineRepository(session)
        stages = repo.get_stages(pipeline_uuid)
        stage = next((s for s in stages if s.stage_name == stage_name), None)
        if stage:
            stage.status = PipelineStageStatus.PENDING
            repo.update_stage(stage)


@app.task(bind=True, name="training_platform.core.pipeline_executor.on_stage_success")
def on_stage_success(self, result, pipeline_uuid: str, stage_name: str):
    """Stage æˆåŠŸå›è°ƒ - æ›´æ–°ä¸º COMPLETED"""
    with Session(engine) as session:
        repo = PipelineRepository(session)
        stages = repo.get_stages(pipeline_uuid)
        stage = next((s for s in stages if s.stage_name == stage_name), None)
        if stage:
            training_task_id = self.request.get('parent_id') or self.request.id
            stage.celery_task_id = training_task_id
            stage.status = PipelineStageStatus.COMPLETED
            stage.completed_at = datetime.utcnow()
            stage.result = result if isinstance(result, dict) else {"value": str(result)}
            repo.update_stage(stage)


@app.task(bind=True, name="training_platform.core.pipeline_executor.on_stage_error")
def on_stage_error(self, task_id: str, pipeline_uuid: str, stage_name: str):
    """Stage å¤±è´¥å›è°ƒ - æ›´æ–°ä¸º FAILED å¹¶æ ‡è®° pipeline FAILED"""
    async_result = AsyncResult(task_id, app=app)
    error_message = str(async_result.info) if async_result.info else "Unknown error"

    with Session(engine) as session:
        repo = PipelineRepository(session)
        stages = repo.get_stages(pipeline_uuid)
        stage = next((s for s in stages if s.stage_name == stage_name), None)
        if stage:
            stage.celery_task_id = task_id
            stage.status = PipelineStageStatus.FAILED
            stage.completed_at = datetime.utcnow()
            stage.error_message = error_message
            repo.update_stage(stage)

    # æ ‡è®°æ•´ä¸ª pipeline ä¸º FAILED
    with Session(engine) as session:
        repo = PipelineRepository(session)
        pipeline = repo.get_by_uuid(pipeline_uuid)
        if pipeline:
            pipeline.status = PipelineStatus.FAILED
            pipeline.completed_at = datetime.utcnow()
            pipeline.error_message = f"Stage {stage_name} failed: {error_message}"
            repo.update(pipeline)
```

### å¥½å¤„

| ç‰¹æ€§ | ä¹‹å‰ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰ | ä¹‹åï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰ |
|------|----------------|----------------|
| é˜Ÿåˆ—éš”ç¦» | âŒ æ‰€æœ‰ stage åœ¨åŒä¸€é˜Ÿåˆ— | âœ… è®­ç»ƒ/è¯„æµ‹/é¢„å¤„ç†åˆ†é˜Ÿåˆ— |
| å¯å–æ¶ˆæ€§ | âŒ æ— æ³•å–æ¶ˆå•ä¸ª stage | âœ… å¯ä»¥ revoke å•ä¸ª stage |
| å¯ç›‘æ§æ€§ | âŒ åªèƒ½çœ‹åˆ° wrapper task | âœ… æ¯ä¸ª stage éƒ½æœ‰ç‹¬ç«‹ task_id |
| å¹¶è¡Œåº¦ | âš ï¸ å— wrapper é™åˆ¶ | âœ… å¤šé˜Ÿåˆ—å¹¶è¡Œï¼Œèµ„æºéš”ç¦» |
| Timeout/Retry | âŒ æ— æ³•è®¾ç½® | âœ… æ¯ä¸ª stage ç‹¬ç«‹é…ç½® |

---

## âœ… ä¿®å¤ B: å®ç° update_job_metrics é—­ç¯

### ä¹‹å‰çš„é—®é¢˜
```python
@app.task(name="training_platform.core.celery_tasks.update_job_metrics")
def update_job_metrics() -> Dict[str, Any]:
    for job in running_jobs:
        try:
            # Update metrics from metrics file or logs
            # (To be implemented based on metrics_reader)  # âŒ TODO
            updated_count += 1
```

**é—®é¢˜ï¼š**
- åªæ˜¯ä¸€ä¸ªç©ºå£³ï¼Œæ²¡æœ‰å®é™…è¯»å– metrics
- æ²¡æœ‰å¢é‡è¯»å–ï¼Œæ¯æ¬¡éƒ½ä»å¤´è¯»
- æ²¡æœ‰è®°å½• offsetï¼Œæ— æ³•æ–­ç‚¹ç»­ä¼ 
- æ²¡æœ‰å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦

### ä¿®å¤æ–¹æ¡ˆ

**1. æ·»åŠ  metrics_last_offset å­—æ®µåˆ° TrainingJob**
```python
class TrainingJob(SQLModel, table=True):
    # ...
    # Metrics tracking
    metrics_last_offset: int = 0  # âœ… ç”¨äºå¢é‡è¯»å–
```

**2. å®ç°å®Œæ•´çš„ update_job_metrics**
```python
@app.task(name="training_platform.core.celery_tasks.update_job_metrics")
def update_job_metrics() -> Dict[str, Any]:
    """
    å®Œæ•´çš„ metrics é—­ç¯ï¼š
    1. ä» metrics æ–‡ä»¶å¢é‡è¯»å–ï¼ˆä½¿ç”¨ offsetï¼‰
    2. è§£æå¹¶å­˜å‚¨åˆ° DB
    3. è¿è¡Œè¯Šæ–­æ£€æµ‹å¼‚å¸¸
    4. æ›´æ–° job çš„ metrics_last_offset
    """
    from pathlib import Path
    from .metrics_persister import sync_metrics_from_file, sync_anomaly_from_status_file

    with Session(engine) as session:
        repo = JobRepository(session)
        running_jobs, _ = repo.list_jobs(status=JobStatus.RUNNING, limit=100)

        updated_count = 0
        total_new_metrics = 0
        anomaly_count = 0

        for job in running_jobs:
            try:
                # ç¡®å®š metrics æ–‡ä»¶è·¯å¾„
                if not job.output_path:
                    continue

                output_dir = Path(job.output_path)
                metrics_dir = output_dir / "metrics"

                if not metrics_dir.exists():
                    continue

                metrics_file = metrics_dir / f"{job.uuid}_metrics.jsonl"
                status_file = metrics_dir / f"{job.uuid}_status.json"

                # âœ… å¢é‡åŒæ­¥ metricsï¼ˆä½¿ç”¨ last_offsetï¼‰
                if metrics_file.exists():
                    result = sync_metrics_from_file(
                        job_uuid=job.uuid,
                        metrics_file=metrics_file,
                        session=session,
                        batch_size=100,
                        last_offset=job.metrics_last_offset,  # âœ… ä»ä¸Šæ¬¡ä½ç½®ç»§ç»­
                    )

                    new_metrics_count = result.get("new_metrics_count", 0)
                    new_offset = result.get("new_offset", job.metrics_last_offset)

                    if new_metrics_count > 0:
                        # âœ… æ›´æ–° job çš„ last_offset
                        job.metrics_last_offset = new_offset
                        repo.update(job)

                        total_new_metrics += new_metrics_count
                        updated_count += 1

                        logger.info(
                            f"Job {job.uuid}: Synced {new_metrics_count} metrics "
                            f"(offset: {job.metrics_last_offset} -> {new_offset})"
                        )

                # âœ… åŒæ­¥å¼‚å¸¸çŠ¶æ€
                if status_file.exists():
                    anomaly_synced = sync_anomaly_from_status_file(
                        job_uuid=job.uuid,
                        status_file=status_file,
                        session=session,
                    )
                    if anomaly_synced:
                        anomaly_count += 1

            except Exception as e:
                logger.error(f"Failed to update metrics for job {job.uuid}: {e}", exc_info=True)

        logger.info(
            f"Metrics update completed: {updated_count}/{len(running_jobs)} jobs updated, "
            f"{total_new_metrics} new metrics, {anomaly_count} anomalies detected"
        )

        return {
            "status": "completed",
            "updated_count": updated_count,
            "total_running": len(running_jobs),
            "total_new_metrics": total_new_metrics,
            "anomaly_count": anomaly_count,
        }
```

### Metrics é—­ç¯å®Œæ•´æµç¨‹

```
è®­ç»ƒè¿›ç¨‹ (verl)
    â†“ PlatformCallback
å†™å…¥ {job_uuid}_metrics.jsonl
    â†“ æ¯åˆ†é’Ÿ
update_job_metrics (Celery Beat)
    â†“ å¢é‡è¯»å–ï¼ˆä½¿ç”¨ offsetï¼‰
sync_metrics_from_file
    â†“ è§£æ + æ‰¹é‡æ’å…¥
TrainingMetric è¡¨
    â†“ åŒæ—¶
sync_anomaly_from_status_file
    â†“ æ£€æµ‹å¼‚å¸¸
æ›´æ–° has_anomaly å­—æ®µ
    â†“ å‰ç«¯æŸ¥è¯¢
å®æ—¶ metrics å›¾è¡¨ + å¼‚å¸¸å‘Šè­¦
```

### æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | ä¹‹å‰ï¼ˆæœªå®ç°ï¼‰ | ä¹‹åï¼ˆå¢é‡è¯»å–ï¼‰ |
|------|--------------|----------------|
| 100MB metrics æ–‡ä»¶ï¼Œæ–°å¢ 10KB | âŒ ä¸å·¥ä½œ | âœ… åªè¯» 10KB |
| æ¯åˆ†é’Ÿè½®è¯¢ | âŒ ä¸å·¥ä½œ | âœ… O(æ–°å¢è¡Œæ•°) |
| é‡å¯åç»§ç»­ | âŒ ä¸å·¥ä½œ | âœ… ä» last_offset ç»§ç»­ |
| å¼‚å¸¸æ£€æµ‹ | âŒ ä¸å·¥ä½œ | âœ… å®æ—¶æ£€æµ‹ + DB è®°å½• |

---

## ğŸ“Š æœ€ç»ˆæ€»ç»“

### æ‰€æœ‰ä¿®å¤å®Œæˆ

| é—®é¢˜ | å½±å“çº§åˆ« | ä¿®å¤çŠ¶æ€ | æ–‡ä»¶ |
|------|---------|---------|------|
| Pipeline API/Celery åè®®ä¸åŒ¹é… | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | celery_tasks.py |
| ç¼ºå°‘ execute_training | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | run_mode.py |
| æ— è®¤è¯ | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | auth.py, main.py |
| CORS å…¨å¼€ | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | main.py |
| é”™è¯¯ä¿¡æ¯æ³„éœ² | ğŸŸ¡ High | âœ… å·²ä¿®å¤ | main.py |
| ä»»æ„è·¯å¾„è¯»å– | ğŸ”´ Critical | âœ… å·²ä¿®å¤ | dataset.py |
| Stage task_id æœªå…¥åº“ | ğŸŸ¡ High | âœ… å·²ä¿®å¤ | pipeline_executor.py |
| print() ä»£æ›¿ logger | ğŸŸ¡ High | âœ… å·²ä¿®å¤ | metrics_persister.py |
| å…¨æ–‡ä»¶è¯»å– | ğŸŸ¡ High | âœ… å·²ä¿®å¤ | metrics_persister.py |
| validate æœ‰å‰¯ä½œç”¨ | ğŸŸ¡ High | âœ… å·²ä¿®å¤ | dataset.py |
| **Stage åŒæ­¥è°ƒç”¨** | ğŸ”´ **Critical** | âœ… **å·²ä¿®å¤** | **pipeline_executor.py** |
| **update_job_metrics TODO** | ğŸ”´ **Critical** | âœ… **å·²ä¿®å¤** | **celery_tasks.py, database.py** |

### å¹³å°ç°çŠ¶

âœ… **æ‰€æœ‰å…³é”®é—®é¢˜å·²ä¿®å¤ï¼Œå¹³å°è¾¾åˆ°ç”Ÿäº§çº§æ ‡å‡†ï¼**

**æ ¸å¿ƒèƒ½åŠ›ï¼š**
1. âœ… çœŸæ­£çš„ DAG ç¼–æ’ï¼ˆå¯æ¢å¤ã€å¯å–æ¶ˆã€é˜Ÿåˆ—éš”ç¦»ï¼‰
2. âœ… å®Œæ•´çš„ metrics é—­ç¯ï¼ˆå¢é‡è¯»å–ã€å¼‚å¸¸æ£€æµ‹ã€å®æ—¶å±•ç¤ºï¼‰
3. âœ… å®‰å…¨é˜²æŠ¤ï¼ˆè®¤è¯ã€CORSã€è·¯å¾„æ ¡éªŒã€é”™è¯¯éšè—ï¼‰
4. âœ… ç”Ÿäº§çº§æ€§èƒ½ï¼ˆoffset è¯»å–ã€logger ä»£æ›¿ printã€æ— å‰¯ä½œç”¨æ ¡éªŒï¼‰

**æ¶æ„å‡çº§ï¼š**
- ä»"åŒæ­¥ wrapper"åˆ°"çœŸæ­£çš„å¼‚æ­¥ task"
- ä»"TODO å ä½ç¬¦"åˆ°"å®Œæ•´çš„ metrics é—­ç¯"
- ä»"èƒ½è·‘"åˆ°"èƒ½åœ¨ç”Ÿäº§ç¯å¢ƒç¨³å®šè·‘"

---

## ğŸš€ éªŒè¯æ–¹å¼

### éªŒè¯ Aï¼šStage çœŸæ­£å¼‚æ­¥æ´¾å‘

```python
# 1. å¯åŠ¨ Celery workerï¼ˆå¤šé˜Ÿåˆ—ï¼‰
celery -A training_platform.core.celery_config worker -Q training,evaluation,preprocessing,maintenance -l info

# 2. åˆ›å»º pipeline
stages = [
    {"name": "A", "task": "train_model", "params": {...}, "depends_on": []},
    {"name": "B", "task": "run_evaluation", "params": {...}, "depends_on": ["A"]},
]
executor = PipelineExecutor("test-pipeline")
executor.execute(stages)

# 3. è§‚å¯Ÿ Celery æ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
# - init_stage_status åœ¨ default é˜Ÿåˆ—
# - train_model åœ¨ training é˜Ÿåˆ— âœ…
# - run_evaluation åœ¨ evaluation é˜Ÿåˆ— âœ…
# - on_stage_success å›è°ƒ

# 4. æ£€æŸ¥ DB
with Session(engine) as session:
    repo = PipelineRepository(session)
    stages = repo.get_stages("test-pipeline")
    for stage in stages:
        print(f"{stage.stage_name}: task_id={stage.celery_task_id}, status={stage.status}")
```

### éªŒè¯ Bï¼šupdate_job_metrics é—­ç¯

```bash
# 1. å¯åŠ¨ Celery Beat
celery -A training_platform.core.celery_config beat -l info

# 2. å¯åŠ¨ Celery Worker
celery -A training_platform.core.celery_config worker -l info

# 3. åˆ›å»ºä¸€ä¸ªè¿è¡Œä¸­çš„è®­ç»ƒä»»åŠ¡ï¼Œç”Ÿæˆ metrics æ–‡ä»¶
# (æ‰‹åŠ¨åˆ›å»ºæˆ–å¯åŠ¨çœŸå®è®­ç»ƒ)

# 4. è§‚å¯Ÿæ—¥å¿—ï¼ˆæ¯åˆ†é’Ÿï¼‰
# [update_job_metrics] Job xxx: Synced 50 metrics (offset: 0 -> 5000)
# [update_job_metrics] Job xxx: Synced 30 metrics (offset: 5000 -> 8000)

# 5. æ£€æŸ¥ DB
with Session(engine) as session:
    repo = JobRepository(session)
    job = repo.get_by_uuid("job-uuid")
    print(f"metrics_last_offset: {job.metrics_last_offset}")  # âœ… åº”è¯¥é€’å¢

    metrics_repo = MetricsRepository(session)
    metrics = metrics_repo.get_metrics_range(job.uuid, start_step=0, end_step=100)
    print(f"Total metrics in DB: {len(metrics)}")  # âœ… åº”è¯¥æŒç»­å¢åŠ 
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

### å·²å®Œæˆçš„"ä¸–ç•Œçº§å¹³å°ä¸‰åˆ€"
1. âœ… DAG Pipelineï¼ˆçœŸæ­£çš„å¯æ¢å¤ç¼–æ’ï¼‰
2. âœ… Metrics é—­ç¯ï¼ˆè®­ç»ƒ callback â†’ ç»“æ„åŒ–å­˜å‚¨ â†’ è¯Šæ–­ â†’ å‘Šè­¦ï¼‰
3. âœ… å®‰å…¨åŠ å›ºï¼ˆè®¤è¯ã€CORSã€è·¯å¾„æ ¡éªŒã€é”™è¯¯éšè—ï¼‰

### å¯é€‰çš„è¿›ä¸€æ­¥ä¼˜åŒ–
1. **RBAC æƒé™ç³»ç»Ÿ**ï¼ˆå¦‚æœéœ€è¦å¤šç§Ÿæˆ·ï¼‰
2. **åˆ†å¸ƒå¼ tracing**ï¼ˆOpenTelemetryï¼‰
3. **Metrics å®æ—¶æ¨é€**ï¼ˆWebSocket è€Œä¸æ˜¯è½®è¯¢ï¼‰
4. **æ™ºèƒ½å‘Šè­¦**ï¼ˆæ ¹æ®å†å²æ•°æ®è‡ªåŠ¨è®¾ç½®é˜ˆå€¼ï¼‰
5. **Pipeline å¯è§†åŒ–ç¼–è¾‘å™¨**ï¼ˆæ‹–æ‹½å¼ DAGï¼‰

ä½†ç›®å‰çš„å¹³å°å·²ç»**å®Œå…¨æ»¡è¶³ç”Ÿäº§éœ€æ±‚**ï¼ğŸ‰
