# 模块七：工程实践

> 部署架构、性能优化、监控告警与故障排查指南

---

## 1. 系统架构

### 1.1 生产环境架构

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        Deep Research 生产架构                                    │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  用户层                                                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  Web UI / API Client / CLI                                              │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  接入层                                                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  ┌───────────┐  ┌───────────┐  ┌───────────┐                           │   │
│  │  │  Nginx    │  │   Kong    │  │Rate Limit │                           │   │
│  │  │Load Balance│  │ API GW    │  │  Service  │                           │   │
│  │  └───────────┘  └───────────┘  └───────────┘                           │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  服务层                                                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐        │   │
│  │  │  Agent Service  │  │  Agent Service  │  │  Agent Service  │        │   │
│  │  │  (Instance 1)   │  │  (Instance 2)   │  │  (Instance N)   │        │   │
│  │  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘        │   │
│  │           │                    │                    │                  │   │
│  │           └────────────────────┼────────────────────┘                  │   │
│  │                                │                                       │   │
│  │                                ▼                                       │   │
│  │           ┌─────────────────────────────────────────┐                 │   │
│  │           │          Task Queue (Redis)             │                 │   │
│  │           └─────────────────────────────────────────┘                 │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  模型层                                                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐        │   │
│  │  │  vLLM Server 1  │  │  vLLM Server 2  │  │  vLLM Server N  │        │   │
│  │  │  (GPU Node)     │  │  (GPU Node)     │  │  (GPU Node)     │        │   │
│  │  └─────────────────┘  └─────────────────┘  └─────────────────┘        │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  工具层                                                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  ┌───────────┐  ┌───────────┐  ┌───────────┐  ┌───────────┐           │   │
│  │  │  Search   │  │  Visit    │  │  Scholar  │  │  Python   │           │   │
│  │  │  Service  │  │  Service  │  │  Service  │  │  Sandbox  │           │   │
│  │  └───────────┘  └───────────┘  └───────────┘  └───────────┘           │   │
│  │                                                                        │   │
│  │  ┌────────────────────────────────────────────────────────┐           │   │
│  │  │              Result Cache (Redis)                       │           │   │
│  │  └────────────────────────────────────────────────────────┘           │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  存储层                                                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  ┌───────────┐  ┌───────────┐  ┌───────────┐  ┌───────────┐           │   │
│  │  │PostgreSQL │  │   Redis   │  │    S3     │  │   Kafka   │           │   │
│  │  │ (元数据)  │  │  (缓存)   │  │  (文件)   │  │  (日志)   │           │   │
│  │  └───────────┘  └───────────┘  └───────────┘  └───────────┘           │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 1.2 核心组件说明

| 组件 | 作用 | 技术选型 |
|------|------|----------|
| **API Gateway** | 请求路由、认证鉴权 | Kong / Nginx |
| **Agent Service** | 执行Agent推理逻辑 | Python / FastAPI |
| **Task Queue** | 异步任务队列 | Redis + Celery |
| **Model Server** | LLM推理服务 | vLLM / TGI |
| **Tool Services** | 工具执行服务 | 独立微服务 |
| **Cache** | 结果缓存 | Redis |
| **Storage** | 持久化存储 | PostgreSQL + S3 |

---

## 2. 模型服务部署

### 2.1 vLLM 部署配置

```python
# vLLM 服务启动配置
vllm_config = {
    # 模型配置
    "model": "Qwen/Qwen2.5-32B-Instruct",
    "dtype": "bfloat16",
    "max_model_len": 131072,  # 128K上下文

    # 服务配置
    "host": "0.0.0.0",
    "port": 8000,
    "api_key": "your-api-key",

    # 性能配置
    "tensor_parallel_size": 4,  # 4卡并行
    "gpu_memory_utilization": 0.9,
    "max_num_seqs": 32,  # 最大并发请求数

    # 优化配置
    "enable_prefix_caching": True,  # 前缀缓存
    "enable_chunked_prefill": True,  # 分块预填充
}

# 启动命令
# python -m vllm.entrypoints.openai.api_server \
#   --model Qwen/Qwen2.5-32B-Instruct \
#   --dtype bfloat16 \
#   --max-model-len 131072 \
#   --tensor-parallel-size 4 \
#   --gpu-memory-utilization 0.9 \
#   --enable-prefix-caching \
#   --port 8000
```

### 2.2 多节点负载均衡

```python
class ModelLoadBalancer:
    """模型服务负载均衡器"""

    def __init__(self, servers: List[str]):
        self.servers = servers
        self.health_status = {s: True for s in servers}
        self.request_counts = {s: 0 for s in servers}

    async def get_server(self) -> str:
        """获取可用服务器（最少连接策略）"""
        available = [s for s in self.servers if self.health_status[s]]

        if not available:
            raise Exception("无可用模型服务器")

        # 选择请求最少的服务器
        return min(available, key=lambda s: self.request_counts[s])

    async def request(self, prompt: str, **kwargs) -> str:
        """发送请求"""
        server = await self.get_server()
        self.request_counts[server] += 1

        try:
            result = await self._send_request(server, prompt, **kwargs)
            return result
        finally:
            self.request_counts[server] -= 1

    async def health_check(self):
        """健康检查"""
        while True:
            for server in self.servers:
                try:
                    await self._ping(server)
                    self.health_status[server] = True
                except:
                    self.health_status[server] = False
                    logger.warning(f"服务器 {server} 不健康")

            await asyncio.sleep(30)  # 每30秒检查一次
```

---

## 3. 工具服务设计

### 3.1 搜索服务

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import aiohttp
import asyncio

app = FastAPI()

class SearchRequest(BaseModel):
    query: str
    num_results: int = 10
    filter_year: int = None

class SearchService:
    """搜索服务"""

    def __init__(self, config: dict):
        self.api_key = config["serper_api_key"]
        self.cache = RedisCache(config["redis_url"])
        self.rate_limiter = RateLimiter(qps=10)

    async def search(self, request: SearchRequest) -> List[dict]:
        """执行搜索"""
        # 检查缓存
        cache_key = f"search:{request.query}:{request.filter_year}"
        cached = await self.cache.get(cache_key)
        if cached:
            return cached

        # 限流
        await self.rate_limiter.acquire()

        # 执行搜索
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "https://google.serper.dev/search",
                headers={"X-API-KEY": self.api_key},
                json={
                    "q": request.query,
                    "num": request.num_results
                },
                timeout=aiohttp.ClientTimeout(total=10)
            ) as resp:
                data = await resp.json()

        # 格式化结果
        results = self._format_results(data)

        # 缓存结果（15分钟）
        await self.cache.set(cache_key, results, ttl=900)

        return results

    def _format_results(self, data: dict) -> List[dict]:
        """格式化搜索结果"""
        results = []
        for item in data.get("organic", []):
            results.append({
                "title": item.get("title", ""),
                "url": item.get("link", ""),
                "snippet": item.get("snippet", "")[:200]
            })
        return results


search_service = SearchService(config)

@app.post("/search")
async def search_endpoint(request: SearchRequest):
    try:
        results = await search_service.search(request)
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 3.2 网页访问服务

```python
class VisitService:
    """网页访问服务"""

    def __init__(self, config: dict):
        self.summarizer_url = config["summarizer_url"]
        self.cache = RedisCache(config["redis_url"])
        self.max_content_length = config.get("max_content_length", 50000)

    async def visit(self, url: str, goal: str) -> str:
        """访问网页并提取信息"""
        # 检查缓存
        cache_key = f"visit:{url}:{goal}"
        cached = await self.cache.get(cache_key)
        if cached:
            return cached

        # 获取网页内容
        content = await self._fetch_content(url)

        if not content:
            return f"无法获取网页内容: {url}"

        # 如果内容过长，使用摘要模型
        if len(content) > 4000:
            content = await self._summarize(content, goal)

        # 缓存结果（1小时）
        await self.cache.set(cache_key, content, ttl=3600)

        return content

    async def _fetch_content(self, url: str) -> str:
        """获取网页内容"""
        # 优先使用Jina Reader
        try:
            jina_url = f"https://r.jina.ai/{url}"
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    jina_url,
                    headers={"Accept": "text/plain"},
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as resp:
                    if resp.status == 200:
                        return await resp.text()
        except:
            pass

        # 降级：直接爬取
        return await self._direct_fetch(url)

    async def _summarize(self, content: str, goal: str) -> str:
        """使用摘要模型提取相关信息"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.summarizer_url,
                json={
                    "content": content[:self.max_content_length],
                    "goal": goal
                }
            ) as resp:
                data = await resp.json()
                return data.get("summary", content[:4000])
```

### 3.3 统一工具网关

```python
class ToolGateway:
    """工具网关"""

    def __init__(self, config: dict):
        self.services = {
            "search": config["search_service_url"],
            "visit": config["visit_service_url"],
            "scholar": config["scholar_service_url"],
            "python": config["python_service_url"]
        }
        self.timeout = config.get("timeout", 30)

    async def execute(self, tool_call: dict) -> str:
        """执行工具调用"""
        tool_name = tool_call.get("name")
        arguments = tool_call.get("arguments", {})

        if tool_name not in self.services:
            return f"未知工具: {tool_name}"

        service_url = self.services[tool_name]

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{service_url}/{tool_name}",
                    json=arguments,
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return self._format_response(tool_name, data)
                    else:
                        return f"工具调用失败: HTTP {resp.status}"

        except asyncio.TimeoutError:
            return "工具调用超时，请尝试其他方法"
        except Exception as e:
            return f"工具调用错误: {str(e)}"

    def _format_response(self, tool_name: str, data: dict) -> str:
        """格式化工具响应"""
        if tool_name == "search":
            results = data.get("results", [])
            lines = []
            for i, r in enumerate(results, 1):
                lines.append(f"{i}. {r['title']}")
                lines.append(f"   URL: {r['url']}")
                lines.append(f"   {r['snippet']}")
                lines.append("")
            return "\n".join(lines) if lines else "没有找到结果"

        elif tool_name == "visit":
            return data.get("content", "无法获取内容")

        else:
            return str(data)
```

---

## 4. 缓存策略

### 4.1 多层缓存架构

```python
class CacheManager:
    """多层缓存管理器"""

    def __init__(self, config: dict):
        # L1: 本地内存缓存
        self.local_cache = LRUCache(maxsize=1000)

        # L2: Redis分布式缓存
        self.redis = aioredis.from_url(config["redis_url"])

        # 缓存配置
        self.ttl_config = {
            "search": 900,    # 搜索结果: 15分钟
            "visit": 3600,    # 网页内容: 1小时
            "scholar": 86400, # 学术结果: 1天
        }

    async def get(self, key: str) -> Optional[str]:
        """获取缓存"""
        # 先查本地缓存
        if key in self.local_cache:
            return self.local_cache[key]

        # 再查Redis
        value = await self.redis.get(key)
        if value:
            # 写入本地缓存
            self.local_cache[key] = value
            return value

        return None

    async def set(self, key: str, value: str, category: str = "default"):
        """设置缓存"""
        ttl = self.ttl_config.get(category, 3600)

        # 写入本地缓存
        self.local_cache[key] = value

        # 写入Redis
        await self.redis.setex(key, ttl, value)

    async def invalidate(self, pattern: str):
        """批量失效缓存"""
        # 清理本地缓存
        keys_to_remove = [k for k in self.local_cache if pattern in k]
        for key in keys_to_remove:
            del self.local_cache[key]

        # 清理Redis缓存
        cursor = 0
        while True:
            cursor, keys = await self.redis.scan(cursor, match=f"*{pattern}*")
            if keys:
                await self.redis.delete(*keys)
            if cursor == 0:
                break
```

### 4.2 缓存预热

```python
class CacheWarmer:
    """缓存预热器"""

    def __init__(self, cache: CacheManager, tools: ToolGateway):
        self.cache = cache
        self.tools = tools

    async def warm_common_queries(self, queries: List[str]):
        """预热常见查询"""
        for query in queries:
            cache_key = f"search:{query}"
            if not await self.cache.get(cache_key):
                result = await self.tools.execute({
                    "name": "search",
                    "arguments": {"query": query}
                })
                await self.cache.set(cache_key, result, "search")
                await asyncio.sleep(0.1)  # 避免过快

    async def warm_from_history(self, days: int = 7):
        """从历史请求预热"""
        # 获取最近N天的高频查询
        high_freq_queries = await self._get_high_freq_queries(days)
        await self.warm_common_queries(high_freq_queries)
```

---

## 5. 监控与告警

### 5.1 指标收集

```python
from prometheus_client import Counter, Histogram, Gauge

# 请求指标
REQUEST_COUNT = Counter(
    'agent_requests_total',
    'Total number of requests',
    ['method', 'status']
)

REQUEST_LATENCY = Histogram(
    'agent_request_latency_seconds',
    'Request latency in seconds',
    ['method'],
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30, 60, 120, 300]
)

# 工具调用指标
TOOL_CALLS = Counter(
    'agent_tool_calls_total',
    'Total number of tool calls',
    ['tool', 'status']
)

TOOL_LATENCY = Histogram(
    'agent_tool_latency_seconds',
    'Tool call latency',
    ['tool']
)

# 模型指标
MODEL_TOKENS = Counter(
    'agent_model_tokens_total',
    'Total tokens processed',
    ['type']  # input/output
)

# 任务指标
TASK_SUCCESS_RATE = Gauge(
    'agent_task_success_rate',
    'Task success rate'
)

ACTIVE_TASKS = Gauge(
    'agent_active_tasks',
    'Number of active tasks'
)


class MetricsCollector:
    """指标收集器"""

    @staticmethod
    def record_request(method: str, status: str, latency: float):
        REQUEST_COUNT.labels(method=method, status=status).inc()
        REQUEST_LATENCY.labels(method=method).observe(latency)

    @staticmethod
    def record_tool_call(tool: str, status: str, latency: float):
        TOOL_CALLS.labels(tool=tool, status=status).inc()
        TOOL_LATENCY.labels(tool=tool).observe(latency)

    @staticmethod
    def record_tokens(input_tokens: int, output_tokens: int):
        MODEL_TOKENS.labels(type='input').inc(input_tokens)
        MODEL_TOKENS.labels(type='output').inc(output_tokens)
```

### 5.2 日志系统

```python
import structlog
from datetime import datetime

# 配置结构化日志
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()


class AgentLogger:
    """Agent专用日志器"""

    def __init__(self, task_id: str):
        self.task_id = task_id
        self.logger = logger.bind(task_id=task_id)

    def log_start(self, question: str):
        self.logger.info(
            "task_started",
            question=question[:200],
            timestamp=datetime.now().isoformat()
        )

    def log_step(self, step: int, action: str, observation_len: int):
        self.logger.info(
            "task_step",
            step=step,
            action=action,
            observation_length=observation_len
        )

    def log_tool_call(self, tool: str, args: dict, duration: float, status: str):
        self.logger.info(
            "tool_call",
            tool=tool,
            arguments=args,
            duration_ms=int(duration * 1000),
            status=status
        )

    def log_complete(self, status: str, steps: int, duration: float):
        self.logger.info(
            "task_completed",
            status=status,
            total_steps=steps,
            duration_seconds=duration
        )

    def log_error(self, error: str, traceback: str = None):
        self.logger.error(
            "task_error",
            error=error,
            traceback=traceback
        )
```

### 5.3 告警配置

```yaml
# alertmanager.yml
groups:
  - name: agent_alerts
    rules:
      # 高错误率告警
      - alert: HighErrorRate
        expr: |
          sum(rate(agent_requests_total{status="error"}[5m])) /
          sum(rate(agent_requests_total[5m])) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Agent错误率过高"
          description: "错误率超过10%，当前: {{ $value | humanizePercentage }}"

      # 高延迟告警
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, rate(agent_request_latency_seconds_bucket[5m])) > 60
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Agent响应延迟过高"
          description: "P95延迟超过60秒"

      # 工具调用失败告警
      - alert: ToolCallFailures
        expr: |
          sum(rate(agent_tool_calls_total{status="error"}[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "工具调用失败率上升"

      # 模型服务不可用
      - alert: ModelServiceDown
        expr: up{job="vllm"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "模型服务不可用"
```

---

## 6. 故障排查

### 6.1 常见问题诊断

```python
class DiagnosticTool:
    """诊断工具"""

    def __init__(self, config: dict):
        self.config = config

    async def run_diagnostics(self) -> dict:
        """运行完整诊断"""
        results = {
            "timestamp": datetime.now().isoformat(),
            "checks": {}
        }

        # 检查模型服务
        results["checks"]["model_service"] = await self._check_model_service()

        # 检查工具服务
        results["checks"]["tool_services"] = await self._check_tool_services()

        # 检查缓存
        results["checks"]["cache"] = await self._check_cache()

        # 检查资源使用
        results["checks"]["resources"] = self._check_resources()

        return results

    async def _check_model_service(self) -> dict:
        """检查模型服务"""
        try:
            start = time.time()
            response = await self._test_model_inference()
            latency = time.time() - start

            return {
                "status": "healthy",
                "latency_ms": int(latency * 1000),
                "response_valid": bool(response)
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }

    async def _check_tool_services(self) -> dict:
        """检查工具服务"""
        results = {}
        tools = ["search", "visit", "scholar"]

        for tool in tools:
            try:
                start = time.time()
                await self._test_tool(tool)
                latency = time.time() - start
                results[tool] = {
                    "status": "healthy",
                    "latency_ms": int(latency * 1000)
                }
            except Exception as e:
                results[tool] = {
                    "status": "unhealthy",
                    "error": str(e)
                }

        return results

    def _check_resources(self) -> dict:
        """检查资源使用"""
        import psutil

        return {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent,
            "gpu_memory": self._get_gpu_memory()
        }
```

### 6.2 常见问题与解决方案

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        常见问题排查指南                                          │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  问题1: 模型响应超时                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  症状: 请求长时间无响应，最终超时                                         │   │
│  │                                                                          │   │
│  │  排查步骤:                                                                │   │
│  │  1. 检查GPU利用率: nvidia-smi                                            │   │
│  │  2. 检查模型服务日志: docker logs vllm                                   │   │
│  │  3. 检查队列积压: redis-cli llen task_queue                              │   │
│  │                                                                          │   │
│  │  常见原因:                                                                │   │
│  │  - GPU显存不足                                                           │   │
│  │  - 并发请求过多                                                          │   │
│  │  - 上下文过长                                                            │   │
│  │                                                                          │   │
│  │  解决方案:                                                                │   │
│  │  - 增加GPU资源或减少max_num_seqs                                         │   │
│  │  - 实施请求限流                                                          │   │
│  │  - 启用ReSum缩短上下文                                                   │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  问题2: 工具调用频繁失败                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  症状: 搜索或访问网页返回错误                                             │   │
│  │                                                                          │   │
│  │  排查步骤:                                                                │   │
│  │  1. 检查API配额: 登录Serper/Google后台                                   │   │
│  │  2. 检查网络连通性: curl -I https://google.com                           │   │
│  │  3. 查看错误日志: grep "tool_call.*error" logs/agent.log                 │   │
│  │                                                                          │   │
│  │  常见原因:                                                                │   │
│  │  - API配额耗尽                                                           │   │
│  │  - 网络问题                                                              │   │
│  │  - 目标网站封锁                                                          │   │
│  │                                                                          │   │
│  │  解决方案:                                                                │   │
│  │  - 增加API配额或使用多个API Key                                          │   │
│  │  - 配置代理                                                              │   │
│  │  - 实施重试和降级策略                                                    │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  问题3: 格式错误/输出不规范                                                      │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  症状: 模型输出缺少必要标签，JSON解析失败                                 │   │
│  │                                                                          │   │
│  │  排查步骤:                                                                │   │
│  │  1. 查看原始输出: 检查日志中的model_response                              │   │
│  │  2. 检查system prompt: 确认格式说明完整                                  │   │
│  │  3. 检查temperature: 是否设置过高                                        │   │
│  │                                                                          │   │
│  │  常见原因:                                                                │   │
│  │  - System prompt不清晰                                                   │   │
│  │  - Temperature过高导致随机性大                                           │   │
│  │  - 模型能力不足                                                          │   │
│  │                                                                          │   │
│  │  解决方案:                                                                │   │
│  │  - 优化system prompt，增加示例                                           │   │
│  │  - 降低temperature到0.6-0.7                                              │   │
│  │  - 实施输出格式校验和重试                                                │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  问题4: 内存溢出                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  症状: 服务崩溃，OOM错误                                                  │   │
│  │                                                                          │   │
│  │  排查步骤:                                                                │   │
│  │  1. 检查内存使用: free -h                                                │   │
│  │  2. 检查GPU显存: nvidia-smi                                              │   │
│  │  3. 分析堆内存: 使用memory_profiler                                      │   │
│  │                                                                          │   │
│  │  常见原因:                                                                │   │
│  │  - 上下文过长                                                            │   │
│  │  - 缓存未释放                                                            │   │
│  │  - 内存泄漏                                                              │   │
│  │                                                                          │   │
│  │  解决方案:                                                                │   │
│  │  - 限制上下文长度                                                        │   │
│  │  - 配置缓存过期                                                          │   │
│  │  - 使用IterResearch减少内存占用                                          │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 7. 性能优化

### 7.1 推理优化

```python
class InferenceOptimizer:
    """推理优化器"""

    def __init__(self, config: dict):
        self.config = config

    def optimize_prompt(self, messages: List[dict]) -> List[dict]:
        """优化Prompt"""
        optimized = []

        for msg in messages:
            content = msg["content"]

            # 1. 移除冗余空白
            content = self._remove_redundant_whitespace(content)

            # 2. 截断过长的observation
            if msg["role"] == "user" and "<tool_response>" in content:
                content = self._truncate_observation(content)

            optimized.append({**msg, "content": content})

        return optimized

    def _truncate_observation(self, content: str,
                             max_length: int = 4000) -> str:
        """截断过长的观察结果"""
        if len(content) <= max_length:
            return content

        # 保留开头和结尾
        head_len = max_length * 2 // 3
        tail_len = max_length // 3

        return content[:head_len] + "\n...[内容过长已截断]...\n" + content[-tail_len:]


class BatchProcessor:
    """批处理优化器"""

    def __init__(self, model_client, batch_size: int = 8):
        self.model = model_client
        self.batch_size = batch_size
        self.pending_requests = []

    async def add_request(self, request: dict) -> asyncio.Future:
        """添加请求"""
        future = asyncio.get_event_loop().create_future()
        self.pending_requests.append((request, future))

        if len(self.pending_requests) >= self.batch_size:
            await self._process_batch()

        return future

    async def _process_batch(self):
        """处理批次"""
        if not self.pending_requests:
            return

        batch = self.pending_requests[:self.batch_size]
        self.pending_requests = self.pending_requests[self.batch_size:]

        requests = [r[0] for r in batch]
        futures = [r[1] for r in batch]

        # 批量推理
        results = await self.model.batch_generate(requests)

        # 设置结果
        for future, result in zip(futures, results):
            future.set_result(result)
```

### 7.2 并发优化

```python
class ConcurrencyManager:
    """并发管理器"""

    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.active_tasks = {}

    async def run_with_limit(self, task_id: str,
                            coroutine: Coroutine) -> Any:
        """限制并发执行"""
        async with self.semaphore:
            self.active_tasks[task_id] = time.time()
            try:
                result = await coroutine
                return result
            finally:
                del self.active_tasks[task_id]

    def get_active_count(self) -> int:
        """获取活跃任务数"""
        return len(self.active_tasks)

    async def wait_for_slot(self, timeout: float = 60):
        """等待可用槽位"""
        start = time.time()
        while self.get_active_count() >= self.semaphore._value:
            if time.time() - start > timeout:
                raise TimeoutError("等待并发槽位超时")
            await asyncio.sleep(0.1)
```

---

## 8. 部署检查清单

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        生产部署检查清单                                          │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  基础设施                                                                        │
│  □ GPU服务器配置完成（显存、驱动）                                               │
│  □ Redis集群部署完成                                                            │
│  □ 负载均衡配置完成                                                             │
│  □ SSL证书配置完成                                                              │
│                                                                                 │
│  模型服务                                                                        │
│  □ vLLM服务启动并通过健康检查                                                   │
│  □ 多节点负载均衡配置                                                           │
│  □ GPU利用率监控配置                                                            │
│  □ 自动重启配置                                                                 │
│                                                                                 │
│  工具服务                                                                        │
│  □ 搜索API Key配置并验证                                                        │
│  □ 各工具服务启动并通过测试                                                     │
│  □ 限流配置完成                                                                 │
│  □ 缓存配置完成                                                                 │
│                                                                                 │
│  监控告警                                                                        │
│  □ Prometheus指标收集配置                                                       │
│  □ Grafana仪表盘配置                                                            │
│  □ 告警规则配置并测试                                                           │
│  □ 日志收集配置                                                                 │
│                                                                                 │
│  安全                                                                            │
│  □ API认证配置                                                                  │
│  □ 敏感信息加密存储                                                             │
│  □ 网络隔离配置                                                                 │
│  □ 访问日志记录                                                                 │
│                                                                                 │
│  容灾                                                                            │
│  □ 数据备份策略                                                                 │
│  □ 故障转移配置                                                                 │
│  □ 回滚方案准备                                                                 │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 9. 总结

### 关键要点

1. **架构设计**
   - 微服务架构，组件解耦
   - 多层缓存，减少重复计算
   - 负载均衡，提高可用性

2. **性能优化**
   - 批处理请求
   - 并发控制
   - Prompt优化

3. **监控告警**
   - 全链路指标收集
   - 结构化日志
   - 自动告警

4. **故障排查**
   - 诊断工具
   - 问题分类处理
   - 快速恢复

在最后一个模块中，我们将提供完整的参数配置参考。
