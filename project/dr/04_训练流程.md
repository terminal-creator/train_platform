# 模块四：训练流程

> Agentic CPT、SFT冷启动与强化学习的完整指南

---

## 1. 训练流程总览

### 1.1 为什么需要多阶段训练？

Deep Research Agent的训练不能简单地使用单一方法，原因如下：

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        单一训练方法的局限性                                       │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  只用SFT的问题:                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  1. 模仿学习的上限 = 示范数据的质量                                       │   │
│  │  2. 无法学习"探索"行为，只会复制已见过的模式                              │   │
│  │  3. 对分布外问题泛化能力弱                                                │   │
│  │  4. 容易过拟合到特定的轨迹模式                                            │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  只用RL的问题:                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  1. 奖励极度稀疏：复杂任务中随机探索几乎不可能成功                          │   │
│  │  2. 从零学习工具使用格式非常困难                                           │   │
│  │  3. 训练不稳定，容易崩溃                                                   │   │
│  │  4. 需要海量探索样本，成本极高                                             │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  多阶段训练的优势:                                                               │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  阶段0 (CPT): 预训练注入Agent归纳偏置 → 后续训练更高效                     │   │
│  │  阶段1 (SFT): 学习基本格式和工具使用 → 初始化合理策略                       │   │
│  │  阶段2 (RL):  优化策略最大化奖励 → 突破模仿学习上限                         │   │
│  │                                                                          │   │
│  │  类比：学开车                                                             │   │
│  │  CPT = 了解交通规则和车辆原理                                             │   │
│  │  SFT = 教练示范基本操作                                                   │   │
│  │  RL  = 独立驾驶中不断优化                                                 │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 1.2 完整训练流程图

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        Deep Research Agent 训练流程                              │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  基座模型选择                                                                    │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  Qwen3-7B / 14B / 30B / 72B                                              │   │
│  │  要求: 长上下文支持 (≥64K)、良好的指令遵循能力                            │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  阶段0: Agentic CPT (可选但推荐)      │ 效果: +5~10% 任务成功率                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  Stage 1: 32K上下文，~200B tokens                                        │   │
│  │  Stage 2: 128K上下文，~100B tokens                                       │   │
│  │  数据: FAS (一阶动作合成) + HAS (高阶动作合成)                            │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  阶段1: SFT 冷启动                    │ 效果: 从0到可用                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  数据: 2K~10K 高质量轨迹                                                  │   │
│  │  关键: Mask observation tokens                                           │   │
│  │  目标: 学会工具使用格式和基本推理模式                                      │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  阶段2: 强化学习                       │ 效果: +10~20% 任务成功率                │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  算法: GRPO / DAPO / DUPO                                                │   │
│  │  数据: 动态采样，排除全对/全错                                            │   │
│  │  奖励: 答案正确性 + 格式正确性                                            │   │
│  │  关键: 负样本筛选，防止格式崩溃                                           │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  阶段3: 持续优化 (可选)                                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  数据-策略共生循环                                                        │   │
│  │  在线学习与适应                                                           │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 2. 阶段0: Agentic Continual Pre-training

### 2.1 为什么需要 Agentic CPT？

通用基础模型（如Qwen、LLaMA）虽然具有强大的语言能力，但缺乏**Agent归纳偏置**：

| 能力 | 通用LLM | Agent需求 |
|------|---------|-----------|
| **长程规划** | 较弱 | 强需求 |
| **工具理解** | 有限 | 核心能力 |
| **迭代推理** | 偏弱 | 必备 |
| **错误恢复** | 较弱 | 重要 |
| **状态追踪** | 有限 | 关键 |

Agentic CPT通过在预训练阶段注入这些偏置，让后续的SFT和RL更高效。

### 2.2 AgentFounder 数据合成框架

AgentFounder包含两种数据合成策略：

#### 2.2.1 一阶动作合成 (First-order Action Synthesis, FAS)

FAS从原始数据出发，合成Agent相关的训练数据：

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        FAS 三阶段流程                                            │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  Phase 1: 实体锚定的开放世界知识记忆                                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                          │   │
│  │  原始数据:                                                                │   │
│  │  "France tourist arrivals increased from 3,793k in May 2025 to          │   │
│  │   4,222k in June 2025, showing a 11.3% month-over-month growth."        │   │
│  │                                                                          │   │
│  │  重新表述:                                                                │   │
│  │  Entity: "France"                                                         │   │
│  │  Statement: "Tourist arrivals in France reached 4,222k in June 2025,    │   │
│  │             showing 11.3% growth from the previous month."               │   │
│  │                                                                          │   │
│  │  目的: 建立 (实体 → 陈述性知识) 的映射关系                                │   │
│  │        让模型学会"记住"与实体相关的事实                                   │   │
│  │                                                                          │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  Phase 2: 多风格问题合成                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                          │   │
│  │  基于Phase 1的知识库，合成多种风格的问题:                                  │   │
│  │                                                                          │   │
│  │  事实检索型:                                                              │   │
│  │  Q: "How many tourists visited France in June 2025?"                     │   │
│  │  A: "4,222k tourists visited France in June 2025."                       │   │
│  │                                                                          │   │
│  │  数值计算型:                                                              │   │
│  │  Q: "What was the increase in France's tourist arrivals from May        │   │
│  │      to June 2025?"                                                       │   │
│  │  A: "The increase was 429k (from 3,793k to 4,222k)."                    │   │
│  │                                                                          │   │
│  │  多跳推理型:                                                              │   │
│  │  Q: "Which European country had the highest tourist growth rate in      │   │
│  │      June 2025, and how many visitors did it receive?"                   │   │
│  │  A: "France had 11.3% growth, receiving 4,222k visitors."               │   │
│  │                                                                          │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                          │
│                                      ▼                                          │
│  Phase 3: 规划动作合成                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                          │   │
│  │  给定问题Q，生成K个不同的问题分析和第一步动作:                             │   │
│  │                                                                          │   │
│  │  Q: "Compare France and Spain's tourist arrivals in June 2025"           │   │
│  │                                                                          │   │
│  │  分析1 + 动作1:                                                           │   │
│  │  <think>需要分别查询两国数据，先搜索法国</think>                          │   │
│  │  <action>search("France tourist arrivals June 2025")</action>            │   │
│  │                                                                          │   │
│  │  分析2 + 动作2:                                                           │   │
│  │  <think>可以一次搜索两国比较数据</think>                                  │   │
│  │  <action>search("France Spain tourist comparison June 2025")</action>    │   │
│  │                                                                          │   │
│  │  分析3 + 动作3:                                                           │   │
│  │  <think>先查找欧洲旅游统计报告</think>                                    │   │
│  │  <action>search("European tourism statistics June 2025")</action>        │   │
│  │                                                                          │   │
│  │  关键: 只生成推理和动作，不执行实际API调用                                │   │
│  │        使用LLM-as-Judge进行拒绝采样筛选高质量样本                          │   │
│  │                                                                          │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

#### 2.2.2 高阶动作合成 (High-order Action Synthesis, HAS)

HAS的核心洞察：**轨迹中的每一步都是一个隐藏的决策过程**

```python
class HighOrderActionSynthesis:
    """高阶动作合成"""

    def __init__(self, llm, config: dict):
        self.llm = llm
        self.num_alternatives = config.get("num_alternatives", 3)

    def synthesize(self, trajectory: dict) -> dict:
        """对轨迹进行步级扩展"""
        question = trajectory["question"]
        steps = trajectory["steps"]  # [(think1, action1, obs1), ...]

        expanded_trajectory = []

        for k, (think, action, obs) in enumerate(steps):
            # 构建上下文
            context = self._build_context(question, steps[:k])

            # 生成N个替代方案
            alternatives = self._generate_alternatives(context, think, action)

            # 合并原始方案和替代方案
            all_options = alternatives + [(think, action)]
            random.shuffle(all_options)

            # 记录正确选项的位置
            correct_idx = all_options.index((think, action))

            # 构建决策数据
            expanded_trajectory.append({
                "context": context,
                "options": all_options,
                "correct_idx": correct_idx,
                "observation": obs
            })

        return self._format_for_training(question, expanded_trajectory)

    def _generate_alternatives(self, context: str,
                               original_think: str,
                               original_action: str) -> List[Tuple[str, str]]:
        """生成替代的思考和动作"""
        prompt = f"""给定以下研究上下文，生成{self.num_alternatives}个
不同但合理的下一步思考和动作方案：

上下文:
{context}

原始方案（不要重复）:
思考: {original_think}
动作: {original_action}

请生成{self.num_alternatives}个替代方案，格式如下:
方案1:
思考: ...
动作: ...

方案2:
思考: ...
动作: ...
"""

        response = self.llm.generate(prompt)
        return self._parse_alternatives(response)

    def _format_for_training(self, question: str,
                             expanded: List[dict]) -> dict:
        """格式化为训练数据

        格式: Q → "我有N个方案" → [方案列表] → "我选择方案X" → 观察 → ...
        """
        formatted = f"问题: {question}\n\n"

        for step in expanded:
            # 展示所有选项
            formatted += f"我有{len(step['options'])}个可能的方案:\n"
            for i, (think, action) in enumerate(step['options']):
                formatted += f"方案{i+1}:\n"
                formatted += f"  思考: {think}\n"
                formatted += f"  动作: {action}\n"

            # 标记选择
            formatted += f"\n我选择方案{step['correct_idx'] + 1}\n"

            # 观察结果
            formatted += f"观察: {step['observation']}\n\n"

        return {"text": formatted}
```

**HAS的优势**：

1. **重用次优轨迹**：即使是错误的步骤也可以作为负面样本
2. **防止过拟合**：模型学习的是决策能力，而非特定轨迹
3. **提升泛化**：见过更多选择，面对新情况时决策更好

### 2.3 CPT 训练配置

```python
# Agentic CPT 配置
cpt_config = {
    # Stage 1: 32K上下文
    "stage1": {
        "context_length": 32768,
        "total_tokens": 200_000_000_000,  # 200B
        "batch_size": 4096,
        "learning_rate": 3e-5,
        "warmup_steps": 2000,
        "data_mix": {
            "fas_phase1": 0.3,  # 实体-知识映射
            "fas_phase2": 0.3,  # 问题合成
            "fas_phase3": 0.2,  # 规划动作
            "general_text": 0.2  # 保持通用能力
        }
    },

    # Stage 2: 128K上下文
    "stage2": {
        "context_length": 131072,
        "total_tokens": 100_000_000_000,  # 100B
        "batch_size": 1024,  # 长上下文需要减小batch
        "learning_rate": 1e-5,
        "data_mix": {
            "has_trajectories": 0.5,  # 高阶动作合成
            "long_context_qa": 0.3,  # 长上下文QA
            "general_text": 0.2
        }
    }
}
```

---

## 3. 阶段1: SFT 冷启动

### 3.1 SFT的目标与作用

SFT（Supervised Fine-Tuning）的主要目标是：

1. **学习输出格式**：正确使用`<think>`、`<tool_call>`、`<answer>`等标签
2. **学习工具使用**：理解每个工具的用途和参数格式
3. **学习推理模式**：建立基本的"分析-行动-观察"思维模式
4. **初始化策略**：为RL阶段提供合理的起点

### 3.2 数据准备

#### 3.2.1 轨迹数据格式

```json
{
  "messages": [
    {
      "role": "system",
      "content": "你是一个研究助手，能够使用工具进行深度研究..."
    },
    {
      "role": "user",
      "content": "法国2024年的GDP是多少？"
    },
    {
      "role": "assistant",
      "content": "<think>\n用户询问法国2024年GDP...\n</think>\n<tool_call>\n{\"name\": \"search\", \"arguments\": {\"query\": \"France GDP 2024\"}}\n</tool_call>"
    },
    {
      "role": "user",
      "content": "<tool_response>\n1. France GDP 2024 - World Bank...\n</tool_response>"
    },
    {
      "role": "assistant",
      "content": "<think>\n搜索结果显示法国GDP约为3.03万亿美元...\n</think>\n<answer>\n法国2024年的GDP约为3.03万亿美元。\n</answer>"
    }
  ],
  "metadata": {
    "num_steps": 2,
    "num_tool_calls": 1,
    "quality_score": 0.95
  }
}
```

#### 3.2.2 Loss Mask 的重要性

**关键技术：Mask Observation Tokens**

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    为什么要Mask Observation?                                     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  不Mask的问题:                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  Assistant: <tool_call>{"name": "search"...}</tool_call>                │   │
│  │  User: <tool_response>法国GDP为3.03万亿美元...</tool_response>          │   │
│  │                                                                          │   │
│  │  如果对tool_response计算loss，模型会试图"预测"搜索结果                   │   │
│  │  这会导致:                                                                │   │
│  │  1. 模型学习"编造"搜索结果（幻觉）                                       │   │
│  │  2. 浪费模型容量在记忆具体事实上                                         │   │
│  │  3. 混淆模型应该"生成"还是"等待"的边界                                   │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  Mask后的效果:                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  模型只学习:                                                             │   │
│  │  1. 如何生成正确的思考过程 (<think>)                                     │   │
│  │  2. 如何正确调用工具 (<tool_call>)                                       │   │
│  │  3. 如何基于观察结果生成答案 (<answer>)                                  │   │
│  │                                                                          │   │
│  │  模型不学习:                                                             │   │
│  │  1. 工具返回的具体内容                                                   │   │
│  │  2. 搜索引擎的行为                                                       │   │
│  │  3. 网页的具体内容                                                       │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

#### 3.2.3 实现代码

```python
import torch
from transformers import Trainer, TrainingArguments

class AgentSFTTrainer:
    """Agent SFT训练器"""

    def __init__(self, model, tokenizer, config: dict):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config

    def prepare_dataset(self, trajectories: List[dict]) -> Dataset:
        """准备训练数据集"""
        processed = []

        for traj in trajectories:
            # Tokenize
            input_ids, labels = self._tokenize_with_mask(traj["messages"])

            processed.append({
                "input_ids": input_ids,
                "attention_mask": [1] * len(input_ids),
                "labels": labels
            })

        return Dataset.from_list(processed)

    def _tokenize_with_mask(self, messages: List[dict]) -> Tuple[List[int], List[int]]:
        """Tokenize并创建loss mask"""
        full_text = ""
        label_mask = []  # True = 计算loss, False = mask

        for msg in messages:
            role = msg["role"]
            content = msg["content"]

            if role == "system":
                full_text += f"<|system|>\n{content}\n"
                label_mask.extend([False] * len(self.tokenizer.encode(f"<|system|>\n{content}\n")))

            elif role == "user":
                full_text += f"<|user|>\n{content}\n"
                # User消息不计算loss
                label_mask.extend([False] * len(self.tokenizer.encode(f"<|user|>\n{content}\n")))

            elif role == "assistant":
                full_text += f"<|assistant|>\n{content}\n"
                # Assistant消息计算loss
                label_mask.extend([True] * len(self.tokenizer.encode(f"<|assistant|>\n{content}\n")))

        # Tokenize
        input_ids = self.tokenizer.encode(full_text)

        # 创建labels：mask的位置用-100
        labels = []
        for i, (token_id, should_compute) in enumerate(zip(input_ids, label_mask)):
            if should_compute:
                labels.append(token_id)
            else:
                labels.append(-100)  # -100会被CrossEntropyLoss忽略

        return input_ids, labels

    def train(self, train_dataset: Dataset, eval_dataset: Dataset = None):
        """执行训练"""
        training_args = TrainingArguments(
            output_dir=self.config["output_dir"],
            num_train_epochs=self.config.get("epochs", 3),
            per_device_train_batch_size=self.config.get("batch_size", 4),
            gradient_accumulation_steps=self.config.get("grad_accum", 8),
            learning_rate=self.config.get("lr", 5e-6),
            lr_scheduler_type="cosine",
            warmup_ratio=0.1,
            weight_decay=0.1,
            logging_steps=10,
            save_steps=500,
            eval_steps=500 if eval_dataset else None,
            evaluation_strategy="steps" if eval_dataset else "no",
            bf16=True,
            gradient_checkpointing=True,
            max_grad_norm=1.0,
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )

        trainer.train()
        return trainer
```

### 3.3 SFT 训练配置

```python
# SFT 训练配置
sft_config = {
    # 模型配置
    "model_name": "Qwen/Qwen2.5-7B-Instruct",
    "max_length": 131072,  # 128K上下文

    # 训练配置
    "batch_size": 4,
    "grad_accum": 8,  # 有效batch size = 32
    "epochs": 3,
    "lr": 5e-6,
    "min_lr": 1e-10,
    "warmup_ratio": 0.1,
    "weight_decay": 0.1,

    # 数据配置
    "train_data_path": "data/sft_trajectories.json",
    "min_quality_score": 0.8,

    # 输出配置
    "output_dir": "outputs/sft_checkpoint",
    "save_steps": 500,
}
```

### 3.4 SFT 数据量建议

| 任务复杂度 | 推荐数据量 | 说明 |
|------------|------------|------|
| 简单Agent | 2K-5K | 单工具、少步骤 |
| 中等复杂度 | 5K-8K | 多工具、中等步数 |
| Deep Research | 6K-10K | 多工具、长轨迹 |

**重要**：数据质量比数量更重要。少量高质量数据 > 大量低质量数据。

---

## 4. 阶段2: 强化学习

### 4.1 为什么需要强化学习？

SFT后的模型存在以下局限：

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        SFT模型的局限                                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  1. 行为克隆的天花板                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  SFT模型只会模仿示范数据中的行为模式                                      │   │
│  │  如果示范数据用3步解决问题，模型也倾向于用3步                             │   │
│  │  但某些问题可能用2步更高效，或需要5步更准确                               │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  2. 分布偏移问题                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  训练时的tool_response来自示范数据                                        │   │
│  │  推理时的tool_response来自真实工具调用                                    │   │
│  │  两者分布不同，导致模型在推理时表现下降                                   │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  3. 探索能力缺失                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  SFT模型不会主动探索新的解决策略                                          │   │
│  │  面对未见过的问题类型，容易陷入固定模式                                   │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  RL的解决方案:                                                                   │
│  - 通过奖励信号直接优化"找到正确答案"这个目标                                   │
│  - 在真实环境中采样，消除分布偏移                                               │
│  - 通过探索发现更优策略                                                         │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 4.2 GRPO 算法详解

GRPO (Group Relative Policy Optimization) 是目前最常用的Agent RL算法：

#### 4.2.1 核心思想

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        GRPO 核心思想                                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  传统RL (PPO):                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  需要训练一个Critic网络来估计Value Function                               │   │
│  │  Advantage = R - V(s)                                                    │   │
│  │                                                                          │   │
│  │  问题:                                                                    │   │
│  │  - Critic训练困难，尤其对于长轨迹                                         │   │
│  │  - 增加训练复杂度和计算成本                                               │   │
│  │  - Agent任务中的状态空间复杂，Critic难以准确估计                          │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  GRPO:                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  不需要Critic，使用组内相对优势估计                                       │   │
│  │                                                                          │   │
│  │  对每个问题Q，采样G个轨迹: {τ₁, τ₂, ..., τG}                             │   │
│  │  计算每个轨迹的奖励: {R₁, R₂, ..., RG}                                   │   │
│  │                                                                          │   │
│  │  组内标准化优势:                                                          │   │
│  │  Â_i = (R_i - mean(R)) / std(R)                                          │   │
│  │                                                                          │   │
│  │  直觉:                                                                    │   │
│  │  - 在同一问题的多个尝试中，成功的尝试获得正优势                           │   │
│  │  - 失败的尝试获得负优势                                                   │   │
│  │  - 相对比较消除了问题难度的影响                                           │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

#### 4.2.2 数学公式

**目标函数**：

```
J(θ) = E_Q[E_τ~π_θ[Σ_t min(r_t(θ) × Â_t, clip(r_t(θ), 1-ε, 1+ε) × Â_t)]]

其中:
- r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)  // 重要性采样比率
- Â_t = (R - mean(R_group)) / std(R_group)   // 组内标准化优势
- ε: clip范围，通常0.2-0.28
```

**奖励函数设计**：

```python
def compute_reward(trajectory: dict, ground_truth: str) -> float:
    """计算轨迹奖励"""

    # 格式奖励 (确保输出格式正确)
    format_reward = 1.0 if is_valid_format(trajectory) else 0.0

    # 答案奖励 (核心奖励)
    if not has_answer(trajectory):
        answer_reward = 0.0
    else:
        # 使用LLM-as-Judge评估答案正确性
        answer_reward = llm_judge(
            question=trajectory["question"],
            prediction=extract_answer(trajectory),
            ground_truth=ground_truth
        )

    # 总奖励 (答案正确性为主)
    total_reward = 0.1 * format_reward + 0.9 * answer_reward

    return total_reward
```

#### 4.2.3 完整实现

```python
import torch
import torch.nn.functional as F
from typing import List, Dict, Tuple

class GRPOTrainer:
    """GRPO训练器"""

    def __init__(self, model, tokenizer, tools, config: dict):
        self.model = model
        self.tokenizer = tokenizer
        self.tools = tools
        self.config = config

        # 优化器
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.get("lr", 1e-6),
            weight_decay=config.get("weight_decay", 0.01)
        )

        # 参考模型 (用于KL约束)
        self.ref_model = copy.deepcopy(model)
        self.ref_model.eval()
        for param in self.ref_model.parameters():
            param.requires_grad = False

    def train_step(self, questions: List[dict]) -> Dict[str, float]:
        """单步训练"""

        # 1. 采样轨迹
        rollouts = self._sample_rollouts(questions)

        # 2. 过滤：排除全对或全错的问题
        filtered_rollouts = self._filter_rollouts(rollouts)

        if not filtered_rollouts:
            return {"loss": 0.0, "filtered_ratio": 1.0}

        # 3. 计算优势
        advantages = self._compute_advantages(filtered_rollouts)

        # 4. 计算策略梯度损失
        loss = self._compute_loss(filtered_rollouts, advantages)

        # 5. 优化
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()

        return {
            "loss": loss.item(),
            "filtered_ratio": 1 - len(filtered_rollouts) / len(rollouts),
            "mean_reward": np.mean([r["reward"] for group in filtered_rollouts for r in group])
        }

    def _sample_rollouts(self, questions: List[dict]) -> List[List[dict]]:
        """为每个问题采样G个轨迹"""
        group_size = self.config.get("group_size", 8)
        rollouts = []

        for q in questions:
            group = []
            for _ in range(group_size):
                # 采样单条轨迹
                trajectory = self._sample_trajectory(q)
                # 计算奖励
                reward = compute_reward(trajectory, q["answer"])
                group.append({
                    "question": q["question"],
                    "trajectory": trajectory,
                    "reward": reward,
                    "log_probs": trajectory["log_probs"]
                })
            rollouts.append(group)

        return rollouts

    def _sample_trajectory(self, question: dict) -> dict:
        """采样单条轨迹"""
        messages = [{"role": "user", "content": question["question"]}]
        trajectory = []
        log_probs = []

        for step in range(self.config.get("max_steps", 50)):
            # 生成
            with torch.no_grad():
                output = self.model.generate(
                    self._encode(messages),
                    max_new_tokens=2048,
                    temperature=self.config.get("temperature", 1.0),
                    do_sample=True,
                    return_dict_in_generate=True,
                    output_scores=True
                )

            response = self._decode(output.sequences[0])
            step_log_prob = self._compute_log_prob(output)
            log_probs.append(step_log_prob)

            trajectory.append({"role": "assistant", "content": response})

            # 检查是否完成
            if "<answer>" in response:
                break

            # 执行工具调用
            tool_call = parse_tool_call(response)
            if tool_call:
                observation = self.tools.execute(tool_call)
                messages.append({"role": "assistant", "content": response})
                messages.append({"role": "user", "content": f"<tool_response>{observation}</tool_response>"})
                trajectory.append({"role": "tool", "content": observation})

        return {
            "trajectory": trajectory,
            "log_probs": log_probs
        }

    def _filter_rollouts(self, rollouts: List[List[dict]]) -> List[List[dict]]:
        """过滤全对或全错的问题组"""
        filtered = []

        for group in rollouts:
            rewards = [r["reward"] for r in group]

            # 排除全对 (太简单) 和全错 (可能有问题)
            if 0 < sum(rewards) < len(rewards):
                filtered.append(group)

        return filtered

    def _compute_advantages(self, rollouts: List[List[dict]]) -> List[List[float]]:
        """计算组内标准化优势"""
        advantages = []

        for group in rollouts:
            rewards = [r["reward"] for r in group]
            mean_r = np.mean(rewards)
            std_r = np.std(rewards) + 1e-8

            group_advantages = [(r - mean_r) / std_r for r in rewards]
            advantages.append(group_advantages)

        return advantages

    def _compute_loss(self, rollouts: List[List[dict]],
                      advantages: List[List[float]]) -> torch.Tensor:
        """计算GRPO损失"""
        total_loss = 0.0
        total_tokens = 0

        clip_eps_low = self.config.get("clip_eps_low", 0.2)
        clip_eps_high = self.config.get("clip_eps_high", 0.28)

        for group, group_adv in zip(rollouts, advantages):
            for rollout, adv in zip(group, group_adv):
                # 计算当前策略的log prob
                current_log_prob = self._get_log_prob(rollout["trajectory"])

                # 计算旧策略的log prob (采样时记录的)
                old_log_prob = rollout["log_probs"]

                # 重要性采样比率
                ratio = torch.exp(current_log_prob - old_log_prob)

                # Clipped surrogate objective
                surr1 = ratio * adv
                surr2 = torch.clamp(ratio, 1 - clip_eps_low, 1 + clip_eps_high) * adv

                # 取min (当adv>0时鼓励，当adv<0时惩罚，但都有clip限制)
                loss = -torch.min(surr1, surr2).mean()

                total_loss += loss
                total_tokens += len(rollout["trajectory"])

        return total_loss / len(rollouts)
```

### 4.3 负样本筛选：防止格式崩溃

**关键问题**：在RL训练中，如果模型因为生成过长而被截断（没有生成`<answer>`），这样的样本会被判定为失败。如果简单地将这些样本作为负样本训练，模型可能学会"为了避免被截断而随意给出答案"，导致格式崩溃。

```python
def filter_negative_samples(rollouts: List[dict]) -> List[dict]:
    """
    筛选负样本，排除因截断导致的失败

    关键原则：
    - 保留：明确给出错误答案的样本 (模型有决策)
    - 排除：因长度限制被截断的样本 (模型没机会决策)
    """
    filtered = []

    for rollout in rollouts:
        if rollout["reward"] > 0:
            # 正样本，保留
            filtered.append(rollout)
        else:
            # 负样本，检查是否是被截断
            if has_answer_tag(rollout["trajectory"]):
                # 有答案但错误，保留作为负样本
                filtered.append(rollout)
            elif is_truncated(rollout["trajectory"]):
                # 被截断，排除
                # 不让模型学习"避免被截断"的错误模式
                continue
            else:
                # 其他情况（如格式错误），保留
                filtered.append(rollout)

    return filtered

def is_truncated(trajectory: List[dict]) -> bool:
    """检查轨迹是否因长度被截断"""
    # 检查最后一个assistant消息是否完整
    last_assistant = None
    for msg in reversed(trajectory):
        if msg["role"] == "assistant":
            last_assistant = msg["content"]
            break

    if last_assistant is None:
        return True

    # 检查是否有未闭合的标签
    open_tags = ["<think>", "<tool_call>"]
    close_tags = ["</think>", "</tool_call>", "<answer>"]

    has_open = any(tag in last_assistant for tag in open_tags)
    has_close = any(tag in last_assistant for tag in close_tags)

    # 有开标签但没有闭标签，说明被截断
    return has_open and not has_close
```

### 4.4 ReSum-GRPO：长Horizon优化

对于非常长的轨迹（经历多次摘要），标准GRPO可能不够有效。ReSum-GRPO通过轨迹分割来优化：

```python
class ReSumGRPOTrainer(GRPOTrainer):
    """ReSum-GRPO训练器，支持长horizon轨迹"""

    def _compute_loss(self, rollouts: List[List[dict]],
                      advantages: List[List[float]]) -> torch.Tensor:
        """计算ReSum-GRPO损失"""
        total_loss = 0.0

        for group, group_adv in zip(rollouts, advantages):
            for rollout, adv in zip(group, group_adv):
                # 将轨迹按摘要事件分割
                segments = self._split_by_summary(rollout["trajectory"])

                # 对每个片段应用相同的优势 (Advantage Broadcasting)
                for segment in segments:
                    segment_loss = self._compute_segment_loss(segment, adv)
                    total_loss += segment_loss

        return total_loss / len(rollouts)

    def _split_by_summary(self, trajectory: List[dict]) -> List[List[dict]]:
        """按摘要事件分割轨迹

        摘要事件：当上下文被压缩时，形成新的片段
        """
        segments = []
        current_segment = []

        for msg in trajectory:
            current_segment.append(msg)

            # 检测摘要事件
            if self._is_summary_event(msg):
                segments.append(current_segment)
                current_segment = []

        if current_segment:
            segments.append(current_segment)

        return segments

    def _is_summary_event(self, msg: dict) -> bool:
        """检测是否是摘要事件"""
        if msg["role"] == "user":
            return "<previous_research_summary>" in msg["content"]
        return False
```

### 4.5 RL 训练配置

```python
# RL训练配置
rl_config = {
    # 算法配置
    "algorithm": "GRPO",
    "group_size": 8,           # 每个问题采样的轨迹数
    "clip_eps_low": 0.2,       # PPO clip下界
    "clip_eps_high": 0.28,     # PPO clip上界

    # 采样配置
    "temperature": 1.0,        # 采样温度
    "top_p": 1.0,
    "max_steps": 60,           # 最大工具调用次数
    "max_tokens": 131072,      # 最大token数

    # 训练配置
    "batch_size": 128,         # 问题数
    "mini_batch_size": 32,     # 梯度更新的mini batch
    "lr": 1e-6,
    "weight_decay": 0.01,
    "max_grad_norm": 1.0,

    # 奖励配置
    "reward_weights": {
        "format": 0.1,
        "answer": 0.9
    },

    # 负样本处理
    "filter_truncated": True,   # 过滤被截断的样本

    # 训练轮数
    "num_iterations": 1000,
    "eval_interval": 50,
}
```

---

## 5. 双环境训练框架

### 5.1 为什么需要双环境？

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        双环境训练的必要性                                        │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  只用真实环境的问题:                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  1. 成本高：每次搜索都要调用真实API                                        │   │
│  │  2. 速度慢：网络延迟，QPS限制                                             │   │
│  │  3. 不确定性：搜索结果可能变化                                            │   │
│  │  4. 难以调试：问题难以复现                                                │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  只用模拟环境的问题:                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  1. 分布偏移：模拟和真实环境差异                                          │   │
│  │  2. 覆盖有限：离线数据无法覆盖所有情况                                    │   │
│  │  3. 时效性：无法获取最新信息                                              │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  双环境策略:                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  模拟环境: 用于快速迭代、调参、消融实验                                   │   │
│  │  真实环境: 用于最终训练、验证、部署                                       │   │
│  │                                                                          │   │
│  │  工作流:                                                                  │   │
│  │  1. 在模拟环境中快速验证算法和超参                                        │   │
│  │  2. 确认有效后，在真实环境中训练最终模型                                  │   │
│  │  3. 持续在真实环境中评估和优化                                            │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 5.2 模拟环境实现

```python
class SimulatedEnvironment:
    """基于离线数据的模拟环境"""

    def __init__(self, config: dict):
        # 加载离线知识库 (如Wikipedia dump)
        self.knowledge_base = self._load_knowledge_base(config["kb_path"])

        # 构建搜索索引
        self.search_index = self._build_search_index(self.knowledge_base)

        # 缓存
        self.cache = {}

    def search(self, query: str) -> List[dict]:
        """模拟搜索"""
        # 检查缓存
        if query in self.cache:
            return self.cache[query]

        # BM25搜索
        results = self.search_index.search(query, top_k=10)

        formatted = []
        for r in results:
            formatted.append({
                "title": r["title"],
                "url": f"wiki://{r['id']}",
                "snippet": r["content"][:200]
            })

        self.cache[query] = formatted
        return formatted

    def visit(self, url: str, goal: str) -> str:
        """模拟网页访问"""
        if not url.startswith("wiki://"):
            return "无法访问非维基百科链接"

        doc_id = url.replace("wiki://", "")
        doc = self.knowledge_base.get(doc_id)

        if not doc:
            return "页面不存在"

        # 简单摘要：提取包含目标关键词的段落
        relevant_parts = self._extract_relevant(doc["content"], goal)
        return relevant_parts[:2000]

    def _extract_relevant(self, content: str, goal: str) -> str:
        """提取与目标相关的内容"""
        keywords = goal.lower().split()
        paragraphs = content.split("\n\n")

        relevant = []
        for p in paragraphs:
            if any(kw in p.lower() for kw in keywords):
                relevant.append(p)

        return "\n\n".join(relevant[:5])
```

### 5.3 真实环境接口

```python
class RealEnvironment:
    """真实搜索环境，带有缓存和限流"""

    def __init__(self, config: dict):
        self.search_api_key = config["search_api_key"]
        self.qps_limit = config.get("qps_limit", 10)
        self.cache = LRUCache(maxsize=100000)

        # 限流器
        self.rate_limiter = RateLimiter(self.qps_limit)

        # 重试策略
        self.retry_policy = RetryPolicy(
            max_retries=3,
            backoff_factor=2
        )

    def search(self, query: str) -> List[dict]:
        """执行真实搜索"""
        # 检查缓存
        cache_key = f"search:{query}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 限流等待
        self.rate_limiter.wait()

        # 执行搜索（带重试）
        for attempt in range(self.retry_policy.max_retries):
            try:
                results = self._call_search_api(query)
                self.cache[cache_key] = results
                return results
            except Exception as e:
                if attempt == self.retry_policy.max_retries - 1:
                    return [{"title": "搜索失败", "url": "", "snippet": str(e)}]
                time.sleep(self.retry_policy.backoff_factor ** attempt)

    def _call_search_api(self, query: str) -> List[dict]:
        """调用搜索API"""
        response = requests.post(
            "https://google.serper.dev/search",
            headers={"X-API-KEY": self.search_api_key},
            json={"q": query, "num": 10},
            timeout=10
        )
        response.raise_for_status()

        results = []
        for item in response.json().get("organic", []):
            results.append({
                "title": item.get("title", ""),
                "url": item.get("link", ""),
                "snippet": item.get("snippet", "")[:200]
            })

        return results
```

---

## 6. 训练技巧与最佳实践

### 6.1 常见问题与解决方案

| 问题 | 症状 | 原因 | 解决方案 |
|------|------|------|----------|
| **格式崩溃** | 输出不再遵循格式 | 负样本污染 | 筛选被截断的样本 |
| **重复生成** | 反复输出相似内容 | 熵过低 | 增加temperature，n-gram惩罚 |
| **奖励稀疏** | 训练无进展 | 任务太难 | 增加SFT数据，渐进难度 |
| **训练不稳定** | Loss剧烈波动 | 方差过大 | 增大batch size和group size |
| **过拟合** | 训练好测试差 | 数据多样性不足 | 增加数据多样性，正则化 |

### 6.2 超参数调优建议

```python
# 推荐的超参数范围
hyperparameter_guide = {
    # SFT阶段
    "sft": {
        "lr": (1e-6, 1e-5),      # 学习率
        "batch_size": (16, 64),   # 有效batch size
        "epochs": (2, 5),         # 训练轮数
    },

    # RL阶段
    "rl": {
        "lr": (5e-7, 5e-6),       # 比SFT小
        "group_size": (4, 16),    # 每个问题的采样数
        "clip_eps": (0.1, 0.3),   # PPO clip范围
        "temperature": (0.8, 1.2), # 采样温度
    },

    # 通用
    "general": {
        "max_grad_norm": 1.0,     # 梯度裁剪
        "weight_decay": (0.01, 0.1),
        "warmup_ratio": (0.05, 0.1),
    }
}
```

### 6.3 训练监控指标

```python
class TrainingMonitor:
    """训练监控器"""

    def __init__(self, log_dir: str):
        self.writer = SummaryWriter(log_dir)
        self.metrics_history = defaultdict(list)

    def log_sft_metrics(self, step: int, metrics: dict):
        """记录SFT指标"""
        self.writer.add_scalar("sft/loss", metrics["loss"], step)
        self.writer.add_scalar("sft/perplexity", metrics.get("ppl", 0), step)

    def log_rl_metrics(self, step: int, metrics: dict):
        """记录RL指标"""
        # 核心指标
        self.writer.add_scalar("rl/mean_reward", metrics["mean_reward"], step)
        self.writer.add_scalar("rl/success_rate", metrics["success_rate"], step)

        # 采样质量
        self.writer.add_scalar("rl/filtered_ratio", metrics.get("filtered_ratio", 0), step)
        self.writer.add_scalar("rl/avg_steps", metrics.get("avg_steps", 0), step)

        # 策略变化
        self.writer.add_scalar("rl/policy_kl", metrics.get("kl_div", 0), step)
        self.writer.add_scalar("rl/entropy", metrics.get("entropy", 0), step)

    def should_early_stop(self, metric_name: str,
                          patience: int = 5) -> bool:
        """检查是否应该早停"""
        history = self.metrics_history[metric_name]

        if len(history) < patience:
            return False

        # 检查最近patience步是否有改善
        recent = history[-patience:]
        best_recent = max(recent)
        best_all = max(history)

        return best_recent < best_all * 0.99  # 改善小于1%
```

### 6.4 检查点管理

```python
class CheckpointManager:
    """检查点管理器"""

    def __init__(self, save_dir: str, max_checkpoints: int = 5):
        self.save_dir = save_dir
        self.max_checkpoints = max_checkpoints
        self.checkpoints = []

    def save(self, model, optimizer, step: int, metrics: dict):
        """保存检查点"""
        checkpoint_path = os.path.join(
            self.save_dir,
            f"checkpoint-{step}"
        )

        # 保存模型和优化器状态
        torch.save({
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "step": step,
            "metrics": metrics
        }, checkpoint_path)

        self.checkpoints.append((checkpoint_path, metrics.get("success_rate", 0)))

        # 保留最好的N个检查点
        self._cleanup()

    def _cleanup(self):
        """清理旧检查点，保留最好的"""
        if len(self.checkpoints) > self.max_checkpoints:
            # 按成功率排序
            self.checkpoints.sort(key=lambda x: x[1], reverse=True)

            # 删除最差的
            for path, _ in self.checkpoints[self.max_checkpoints:]:
                if os.path.exists(path):
                    shutil.rmtree(path)

            self.checkpoints = self.checkpoints[:self.max_checkpoints]

    def load_best(self) -> dict:
        """加载最佳检查点"""
        if not self.checkpoints:
            return None

        best_path = self.checkpoints[0][0]
        return torch.load(best_path)
```

---

## 7. 总结

### 关键要点

1. **多阶段训练是必要的**
   - CPT注入Agent偏置
   - SFT学习基本范式
   - RL优化策略

2. **SFT关键技术**
   - Mask observation tokens
   - 高质量数据 > 大量数据

3. **RL关键技术**
   - GRPO：组内相对优势估计
   - 负样本筛选：防止格式崩溃
   - 动态采样：排除全对/全错

4. **双环境策略**
   - 模拟环境：快速迭代
   - 真实环境：最终训练

### 训练路线图

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        推荐训练路线                                              │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  快速验证路线 (1-2周):                                                           │
│  ├── 跳过CPT                                                                    │
│  ├── 使用开源SFT数据 (5K轨迹)                                                   │
│  ├── 小规模RL (500问题, 3轮)                                                    │
│  └── 在GAIA/SimpleQA上验证                                                      │
│                                                                                 │
│  标准训练路线 (2-4周):                                                           │
│  ├── 可选CPT (如果有计算资源)                                                   │
│  ├── 自建SFT数据 (8K轨迹)                                                       │
│  ├── 完整RL训练 (2K问题, 1000步)                                                │
│  └── 在BrowseComp/HLE上验证                                                     │
│                                                                                 │
│  完整训练路线 (1-2月):                                                           │
│  ├── 完整CPT (300B tokens)                                                      │
│  ├── 大规模SFT数据 (10K+轨迹)                                                   │
│  ├── 迭代RL训练 (数据-策略共生)                                                 │
│  └── 全面评估和持续优化                                                         │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

在下一模块中，我们将详细介绍推理阶段的各种范式和优化策略。
