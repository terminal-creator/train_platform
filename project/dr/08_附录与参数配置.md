# 模块八：附录与参数配置

> 完整参数参考、硬件配置、常用工具与资源汇总

---

## 1. 模型选择指南

### 1.1 基座模型推荐

| 模型 | 参数量 | 上下文长度 | 推荐场景 | 显存需求 |
|------|--------|------------|----------|----------|
| **Qwen2.5-7B-Instruct** | 7B | 128K | 轻量部署、快速原型 | 16GB |
| **Qwen2.5-14B-Instruct** | 14B | 128K | 平衡性能与成本 | 32GB |
| **Qwen2.5-32B-Instruct** | 32B | 128K | 标准生产环境 | 64GB |
| **Qwen2.5-72B-Instruct** | 72B | 128K | 高性能需求 | 160GB |
| **Qwen3-30B-A3B** | 30B (3B激活) | 128K | MoE架构，性价比高 | 48GB |
| **Qwen3-235B-A22B** | 235B (22B激活) | 128K | 最高性能 | 多节点 |

### 1.2 选型决策树

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        模型选型决策树                                            │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  预算和硬件约束                                                                  │
│      │                                                                          │
│      ├── 单卡A100 (80GB)?                                                       │
│      │       │                                                                  │
│      │      是 → Qwen2.5-32B 或 Qwen3-30B-A3B                                  │
│      │      否 → 继续                                                           │
│      │                                                                          │
│      ├── 多卡可用 (2-4×A100)?                                                   │
│      │       │                                                                  │
│      │      是 → Qwen2.5-72B                                                    │
│      │      否 → 继续                                                           │
│      │                                                                          │
│      ├── 只有消费级GPU (24GB)?                                                  │
│      │       │                                                                  │
│      │      是 → Qwen2.5-7B 或 14B (量化)                                      │
│      │                                                                          │
│      └── 多节点集群?                                                            │
│              │                                                                  │
│             是 → Qwen3-235B-A22B                                                │
│                                                                                 │
│  性能需求                                                                        │
│      │                                                                          │
│      ├── BrowseComp > 40%?                                                      │
│      │       │                                                                  │
│      │      是 → 至少32B + RL训练                                               │
│      │                                                                          │
│      ├── GAIA > 70%?                                                            │
│      │       │                                                                  │
│      │      是 → 至少30B + 完整SFT+RL                                           │
│      │                                                                          │
│      └── 仅需基础Agent能力?                                                      │
│              │                                                                  │
│             是 → 7B-14B + SFT即可                                               │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 2. 完整参数配置参考

### 2.1 数据构造参数

```python
# 数据构造配置
data_construction_config = {
    # 知识图谱构建 (SailorFog-QA)
    "knowledge_graph": {
        "max_nodes": 500,
        "max_edges": 2000,
        "p_new_node": 0.7,  # 发现新节点概率
        "p_cycle": 0.3,      # 创建循环概率
        "seed_domains": [
            "人物", "组织", "地点", "事件", "作品", "科学概念"
        ]
    },

    # 子图采样
    "subgraph_sampling": {
        "min_nodes": 3,
        "max_nodes": 10,
        "num_samples": 10000,
        "use_wl_hash": True,  # Weisfeiler-Leman同构检测
    },

    # 问题生成
    "question_generation": {
        "apply_fog": True,           # 信息模糊化
        "fog_probability": 0.7,      # 模糊化概率
        "min_hops": 2,               # 最小推理跳数
        "max_hops": 10,              # 最大推理跳数
    },

    # WebFrontier迭代升级
    "webfrontier": {
        "max_iterations": 5,         # 最大迭代次数
        "operations": [
            "knowledge_expansion",
            "concept_abstraction",
            "fact_grounding",
            "computational_formulation"
        ],
        "operation_weights": [0.3, 0.3, 0.2, 0.2]
    },

    # 质量控制
    "quality_control": {
        "baseline_filter": True,     # 无工具能回答则丢弃
        "similarity_threshold": 0.85, # 相似问题过滤阈值
        "min_complexity": 3,         # 最小推理步数
    }
}
```

### 2.2 轨迹采样参数

```python
# 轨迹采样配置
trajectory_sampling_config = {
    # 采样设置
    "sampling": {
        "teacher_model": "gpt-4o",    # 教师模型
        "samples_per_question": 3,    # 每问题采样数
        "temperature": 0.7,
        "top_p": 0.95,
        "max_steps": 50,
        "max_tokens_per_step": 4096,
    },

    # 过滤器配置
    "filters": {
        # 有效性过滤
        "validity": {
            "check_format": True,
            "max_total_tokens": 64000,
            "min_steps": 10,
            "min_tool_calls": 5,
        },

        # 重复过滤
        "repetition": {
            "ngram_n": 10,
            "max_ngram_repeat": 4,
        },

        # 正确性验证
        "correctness": {
            "judge_model": "gpt-4o",
            "require_correct_answer": True,
        },

        # 质量评估
        "quality": {
            "min_quality_score": 0.7,
            "evaluate_dimensions": [
                "logical_coherence",
                "no_hallucination",
                "tool_relevance",
                "non_redundancy"
            ]
        }
    }
}
```

### 2.3 SFT训练参数

```python
# SFT训练配置
sft_training_config = {
    # 模型配置
    "model": {
        "base_model": "Qwen/Qwen2.5-32B-Instruct",
        "max_length": 131072,
        "dtype": "bfloat16",
    },

    # 训练超参数
    "training": {
        "batch_size": 4,
        "gradient_accumulation_steps": 8,
        "effective_batch_size": 32,

        "learning_rate": 5e-6,
        "min_learning_rate": 1e-10,
        "lr_scheduler": "cosine",
        "warmup_ratio": 0.1,

        "num_epochs": 3,
        "weight_decay": 0.1,
        "max_grad_norm": 1.0,
    },

    # 数据配置
    "data": {
        "train_file": "data/sft_train.json",
        "eval_file": "data/sft_eval.json",
        "mask_observation": True,  # 关键：mask tool response
    },

    # 优化配置
    "optimization": {
        "gradient_checkpointing": True,
        "bf16": True,
        "use_flash_attention": True,
    },

    # 保存配置
    "saving": {
        "output_dir": "outputs/sft",
        "save_steps": 500,
        "save_total_limit": 5,
    }
}
```

### 2.4 RL训练参数

```python
# RL训练配置
rl_training_config = {
    # 算法配置
    "algorithm": {
        "name": "GRPO",
        "group_size": 8,           # 每问题采样数
        "clip_eps_low": 0.2,
        "clip_eps_high": 0.28,
        "kl_coef": 0.01,           # KL散度系数
    },

    # 采样配置
    "sampling": {
        "temperature": 1.0,
        "top_p": 1.0,
        "max_steps": 60,
        "max_new_tokens": 4096,
    },

    # 奖励配置
    "reward": {
        "format_weight": 0.1,
        "answer_weight": 0.9,
        "judge_model": "gpt-4o",
    },

    # 训练配置
    "training": {
        "batch_size": 128,         # 问题数
        "mini_batch_size": 32,
        "learning_rate": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,

        "num_iterations": 1000,
        "eval_interval": 50,
    },

    # 负样本处理
    "negative_sample_handling": {
        "filter_truncated": True,   # 过滤被截断的样本
        "filter_all_correct": True, # 过滤全对组
        "filter_all_wrong": True,   # 过滤全错组
    },

    # 环境配置
    "environment": {
        "use_simulated": False,     # 使用真实环境
        "tool_timeout": 30,
        "cache_results": True,
    }
}
```

### 2.5 推理参数

```python
# 推理配置
inference_config = {
    # 通用配置
    "common": {
        "temperature": 0.7,
        "top_p": 0.95,
        "repetition_penalty": 1.1,
        "max_new_tokens": 4096,
    },

    # ReAct模式
    "react": {
        "max_steps": 30,
        "early_stop_on_answer": True,
    },

    # IterResearch模式
    "iterresearch": {
        "max_rounds": 60,
        "max_report_length": 4000,
    },

    # ReSum模式
    "resum": {
        "max_steps": 60,
        "token_budget": 32000,
        "trigger_threshold": 0.85,
        "summarizer_model": "Qwen2.5-7B-Instruct",
    },

    # Research-Synthesis模式
    "research_synthesis": {
        "num_agents": 8,
        "max_rounds_per_agent": 40,
        "synthesis_temperature": 0.3,
        "early_stop_consensus": 0.6,
    },

    # 工具配置
    "tools": {
        "search_timeout": 10,
        "visit_timeout": 30,
        "max_retries": 3,
        "cache_ttl": 900,
    }
}
```

---

## 3. 硬件配置参考

### 3.1 训练硬件需求

| 阶段 | 模型规模 | 最小配置 | 推荐配置 |
|------|----------|----------|----------|
| **SFT** | 7B | 1×A100-40GB | 2×A100-80GB |
| **SFT** | 32B | 4×A100-80GB | 8×A100-80GB |
| **SFT** | 72B | 8×A100-80GB | 16×A100-80GB |
| **RL** | 7B | 2×A100-40GB | 4×A100-80GB |
| **RL** | 32B | 8×A100-80GB | 16×A100-80GB |
| **RL** | 72B | 16×A100-80GB | 32×A100-80GB |

### 3.2 推理硬件需求

| 模型 | 上下文 | 最小配置 | 并发能力 |
|------|--------|----------|----------|
| 7B | 32K | 1×RTX 4090 | ~10 QPS |
| 7B | 128K | 1×A100-40GB | ~5 QPS |
| 32B | 32K | 2×A100-80GB | ~5 QPS |
| 32B | 128K | 4×A100-80GB | ~2 QPS |
| 72B | 32K | 4×A100-80GB | ~2 QPS |
| 72B | 128K | 8×A100-80GB | ~1 QPS |

### 3.3 成本估算

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        训练成本估算（基于云GPU）                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  假设: A100-80GB 云租赁价格 ~$2/小时/卡                                         │
│                                                                                 │
│  SFT训练 (32B模型, 8K轨迹):                                                     │
│  - 配置: 8×A100-80GB                                                           │
│  - 时长: ~24小时                                                                │
│  - 成本: 8 × 24 × $2 = ~$384                                                   │
│                                                                                 │
│  RL训练 (32B模型, 1000步):                                                      │
│  - 配置: 16×A100-80GB                                                          │
│  - 时长: ~72小时                                                                │
│  - 成本: 16 × 72 × $2 = ~$2,304                                                │
│                                                                                 │
│  完整训练流程:                                                                   │
│  - SFT: ~$400                                                                   │
│  - RL: ~$2,500                                                                  │
│  - 评估: ~$200                                                                  │
│  - 总计: ~$3,100                                                                │
│                                                                                 │
│  推理成本 (生产环境):                                                            │
│  - 配置: 4×A100-80GB                                                           │
│  - 月成本: 4 × 720 × $2 = ~$5,760/月                                           │
│  - 或使用按需计费的API服务                                                       │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 4. 常用工具与库

### 4.1 训练框架

| 工具 | 用途 | 链接 |
|------|------|------|
| **transformers** | 模型加载与训练 | huggingface/transformers |
| **trl** | RLHF训练 | huggingface/trl |
| **DeepSpeed** | 分布式训练 | microsoft/DeepSpeed |
| **vLLM** | 高效推理 | vllm-project/vllm |
| **OpenRLHF** | RL训练框架 | OpenRLHF/OpenRLHF |

### 4.2 评估工具

| 工具 | 用途 | 链接 |
|------|------|------|
| **lm-eval** | 标准评估 | EleutherAI/lm-evaluation-harness |
| **GAIA** | GAIA基准 | gaia-benchmark |
| **BrowseComp** | 网页浏览评估 | - |

### 4.3 工具服务

| 服务 | 用途 | 链接/说明 |
|------|------|----------|
| **Serper** | 搜索API | serper.dev |
| **Jina Reader** | 网页内容提取 | r.jina.ai |
| **Semantic Scholar** | 学术搜索 | semanticscholar.org/api |

### 4.4 安装命令参考

```bash
# 基础环境
pip install torch==2.2.0 transformers==4.40.0 accelerate==0.28.0

# 训练工具
pip install trl==0.8.0 deepspeed==0.14.0 peft==0.10.0

# 推理服务
pip install vllm==0.4.0

# 评估工具
pip install lm-eval==0.4.0

# 工具服务依赖
pip install aiohttp redis[hiredis] trafilatura beautifulsoup4

# 监控工具
pip install prometheus-client structlog
```

---

## 5. System Prompt 模板

### 5.1 ReAct模式

```
你是一个专业的深度研究助手，能够通过多轮搜索和分析来回答复杂问题。

## 可用工具
- search(query): 搜索互联网，返回相关网页列表
- visit(url, goal): 访问网页并提取与目标相关的信息
- scholar(query): 搜索学术文献
- python(code): 执行Python代码进行计算

## 输出格式
1. 使用<think>标签进行推理分析
2. 使用<tool_call>标签调用工具，格式为JSON
3. 使用<answer>标签给出最终答案

## 行为准则
1. 不确定的信息必须搜索确认，不要猜测
2. 重要事实尽量从多个来源验证
3. 答案中应标注信息来源
4. 信息足够时立即回答，不要过度搜索

## 示例
<think>
用户询问X的问题，我需要先搜索相关信息。
</think>
<tool_call>
{"name": "search", "arguments": {"query": "..."}}
</tool_call>
```

### 5.2 IterResearch模式

```
你是一个研究助手，使用迭代研究方法回答复杂问题。

## 核心机制
你维护一份研究报告作为中央记忆，每轮更新。临时信息会被丢弃，只保留报告。

## 可用工具
[同上]

## 每轮输出
1. <think>: 分析当前进展（不保存）
2. <report>: 更新研究报告（保存，传递给下轮）
3. <action>: 执行动作或给出答案

## 报告格式
<report>
## 研究进展
### 已确认信息
- [信息]: [来源URL]
### 待验证信息
- [内容]
### 信息缺口
- [需要了解的]
### 当前结论
[基于已有信息的结论]
</report>

## 动作格式
- 工具调用: <action>{"type": "tool", "name": "...", "args": {...}}</action>
- 最终答案: <action>{"type": "final_answer", "content": "..."}</action>
```

---

## 6. 性能基准参考

### 6.1 主流系统性能对比

| 系统 | BrowseComp | HLE | GAIA | 说明 |
|------|------------|-----|------|------|
| OpenAI Deep Research | 51.5% | 26.6% | 67.4% | 闭源 |
| Gemini Deep Research | ~48% | ~24% | ~65% | 闭源 |
| **通义DeepResearch-30B** | 43.4% | 32.9% | 70.9% | 开源 |
| **WebResearcher-30B** | 37.3% | 28.8% | 72.8% | 开源 |
| **WebSailor-V2-30B** | 35.3% | 30.6% | 74.1% | 开源 |
| Claude Sonnet (直接) | ~25% | ~20% | ~55% | 无Agent |
| GPT-4o (直接) | ~28% | ~22% | ~58% | 无Agent |

### 6.2 消融实验参考

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        消融实验结果参考                                          │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  训练阶段消融 (GAIA基准):                                                        │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  只有基座模型:          ~55%                                             │   │
│  │  + SFT:                 ~65%  (+10%)                                     │   │
│  │  + RL:                  ~75%  (+10%)                                     │   │
│  │  + Agentic CPT:         ~77%  (+2%)                                      │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  推理模式消融 (HLE基准):                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  ReAct:                 ~20%                                             │   │
│  │  IterResearch (n=1):    ~28%  (+8%)                                      │   │
│  │  R-S (n=8):             ~33%  (+5%)                                      │   │
│  │  R-S (n=16):            ~35%  (+2%)                                      │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  数据量消融 (SFT数据):                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  1K轨迹:                ~60%                                             │   │
│  │  2K轨迹:                ~63%                                             │   │
│  │  5K轨迹:                ~67%                                             │   │
│  │  8K轨迹:                ~69%                                             │   │
│  │  10K轨迹:               ~70% (收益递减)                                   │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 7. 常见问题FAQ

### Q1: 应该选择哪个基座模型？

**A**: 取决于你的资源和需求：
- 快速原型/资源有限: Qwen2.5-7B
- 生产环境/平衡性能: Qwen2.5-32B 或 Qwen3-30B-A3B
- 追求最高性能: Qwen2.5-72B 或更大

### Q2: SFT数据需要多少？

**A**: 通常2K-10K高质量轨迹即可。关键是质量而非数量。建议从5K开始，根据效果调整。

### Q3: 必须做RL吗？

**A**: 不是必须，但强烈推荐。RL通常能带来10-20%的性能提升。如果只是快速验证，可以先只做SFT。

### Q4: ReAct和IterResearch如何选择？

**A**:
- 简单任务（<15步）: ReAct
- 复杂任务（15-100步）: IterResearch
- 不确定: 从ReAct开始，遇到上下文溢出再切换

### Q5: 如何处理工具调用失败？

**A**:
1. 实施重试策略（指数退避）
2. 配置多个API Key轮换
3. 实现优雅降级（返回友好错误信息）
4. 启用结果缓存减少重复调用

### Q6: 如何提高可靠性？

**A**:
1. 使用Research-Synthesis并行多Agent
2. 降低temperature（0.6-0.7）
3. 实施自我检查机制
4. 多次运行取共识

### Q7: 训练不稳定怎么办？

**A**:
1. 增大batch size和group size
2. 降低learning rate
3. 检查数据质量，过滤异常样本
4. 使用gradient clipping

---

## 8. 参考论文与资源

### 8.1 核心论文

| 论文 | 主题 | 关键贡献 |
|------|------|----------|
| 通义DeepResearch | 完整系统 | 开源DeepResearch系统 |
| WebResearcher | IterResearch | 常量工作空间范式 |
| WebSailor-V2 | 数据合成 | SailorFog-QA方法 |
| O-Researcher | Multi-Agent | RLAIF训练方法 |
| WebShaper | 形式化 | Knowledge Projection |
| ReSum | 上下文管理 | 动态摘要机制 |
| AgentFounder | Agentic CPT | FAS/HAS数据合成 |

### 8.2 相关资源

- **官方代码**: 各论文的GitHub仓库
- **模型权重**: Hugging Face Hub
- **基准数据**: GAIA、BrowseComp官方
- **社区讨论**: 论文对应的Issues

---

## 9. 更新日志

| 日期 | 版本 | 更新内容 |
|------|------|----------|
| 2025-01 | v1.0 | 初始版本 |

---

## 10. 总结

本手册涵盖了构建Deep Research Agent的完整流程：

1. **整体概览**: 理解Deep Research的定位和能力
2. **系统架构**: 设计Agent框架和工具集
3. **数据构造**: 生成高质量训练数据
4. **训练流程**: CPT→SFT→RL完整训练
5. **推理范式**: ReAct/IterResearch/ReSum选择
6. **评估方法**: 基准测试和指标计算
7. **工程实践**: 部署、监控、故障排查
8. **参数配置**: 完整参考和FAQ

希望这份手册能帮助你从零开始构建自己的Deep Research Agent系统。

---

**手册结束**
