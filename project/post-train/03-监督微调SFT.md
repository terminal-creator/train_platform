# ç¬¬ä¸‰ç« ï¼šç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰

> **æ ¸å¿ƒç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤ï¼ŒæŒæ¡å¯¹è¯æ ¼å¼ï¼Œè¾“å‡ºé«˜è´¨é‡å›å¤
>
> **æœ¬ç« ç›®æ ‡**ï¼šæ·±å…¥ç†è§£SFTçš„åŸç†ã€æ•°æ®å‡†å¤‡ã€è®­ç»ƒæŠ€å·§å’Œæ•ˆæœä¼˜åŒ–
>
> **é”€å”®LLMç¤ºä¾‹**ï¼šè®­ç»ƒæ¨¡å‹æˆä¸ºä¸“ä¸šé”€å”®é¡¾é—®

---

## ä¸€ã€SFTæ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ç›‘ç£å¾®è°ƒï¼Ÿ

SFTæ˜¯ä½¿ç”¨**äººå·¥æ ‡æ³¨çš„æŒ‡ä»¤-å›å¤å¯¹**å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šï¼š
1. **ç†è§£æŒ‡ä»¤**ï¼šæ˜ç™½ç”¨æˆ·æƒ³è¦ä»€ä¹ˆ
2. **éµå¾ªæ ¼å¼**ï¼šæŒ‰ç…§æœŸæœ›çš„æ ¼å¼è¾“å‡º
3. **ç”Ÿæˆé«˜è´¨é‡å›å¤**ï¼šå†…å®¹å‡†ç¡®ã€è¡¨è¾¾æ¸…æ™°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SFTè®­ç»ƒåŸç†                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   è¾“å…¥ (Instruction + Input)              è¾“å‡º (Response)               â”‚
â”‚   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”               â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”              â”‚
â”‚                                                                         â”‚
â”‚   "ä½œä¸ºé”€å”®é¡¾é—®ï¼Œè¯·å›ç­”ï¼š         â”€â”€â”€â”€â–¶    "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼              â”‚
â”‚    å®¢æˆ·è¯´'å¤ªè´µäº†'æ€ä¹ˆåŠï¼Ÿ"                 æˆ‘ç†è§£æ‚¨å¯¹ä»·æ ¼çš„è€ƒè™‘...       â”‚
â”‚                                            1. é¦–å…ˆè®©æˆ‘å¸®æ‚¨åˆ†æ...       â”‚
â”‚                                            2. å®é™…ä¸Š..."                â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                        è®­ç»ƒç›®æ ‡                                  â”‚  â”‚
â”‚   â”‚  Loss = -log P(response | instruction, input)                   â”‚  â”‚
â”‚   â”‚  åªè®¡ç®—responseéƒ¨åˆ†çš„lossï¼Œinstructionéƒ¨åˆ†è¢«maskæ‰               â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 SFTçš„å…³é”®ä½œç”¨

| èƒ½åŠ›ç»´åº¦ | SFTå‰ï¼ˆBase Modelï¼‰ | SFTåï¼ˆInstruct Modelï¼‰ |
|---------|---------------------|-------------------------|
| æŒ‡ä»¤ç†è§£ | âŒ æ— æ³•ç†è§£æŒ‡ä»¤æ„å›¾ | âœ… å‡†ç¡®ç†è§£ç”¨æˆ·éœ€æ±‚ |
| å¯¹è¯æ ¼å¼ | âŒ åªä¼šç»­å†™æ–‡æœ¬ | âœ… æŒ‰å¯¹è¯æ ¼å¼å›å¤ |
| è§’è‰²æ‰®æ¼” | âŒ æ— è§’è‰²æ¦‚å¿µ | âœ… å¯æ‰®æ¼”ç‰¹å®šè§’è‰² |
| å®‰å…¨æ€§ | âŒ å¯èƒ½è¾“å‡ºæœ‰å®³å†…å®¹ | âœ… åˆæ­¥çš„å®‰å…¨æ„è¯† |
| æ ¼å¼è¾“å‡º | âŒ æ— æ³•æŒ‰è¦æ±‚æ ¼å¼åŒ– | âœ… æ”¯æŒJSON/Markdownç­‰ |

### 1.3 SFTæ•°æ®çš„æ ¸å¿ƒåŸåˆ™

> **"æ ·æœ¬çš„ç²¾é«“åœ¨äºè´¨é‡è€Œéæ•°é‡ï¼Œå°‘é‡ä½†ç²¾è‰¯çš„æ ·æœ¬å¾€å¾€èƒœè¿‡å¤§æ‰¹ä¸­ä½å“è´¨çš„æ ·æœ¬ã€‚"**
> â€” Meta LIMAè®ºæ–‡

**é»„é‡‘æ³•åˆ™**ï¼š
- **1ä¸‡æ¡é«˜è´¨é‡æ•°æ® > 10ä¸‡æ¡ä½è´¨é‡æ•°æ®**
- **æ•°æ®è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™**
- **å¤šæ ·æ€§æ¯”æ•°é‡æ›´é‡è¦**

---

## äºŒã€SFTæ•°æ®æ ¼å¼è¯¦è§£

### 2.1 Chat Templateï¼ˆå¯¹è¯æ¨¡æ¿ï¼‰

Chat Templateæ˜¯å°†å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„æ–‡æœ¬åºåˆ—çš„è§„åˆ™ã€‚ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„æ¨¡æ¿ã€‚

#### Qwen2.5çš„ChatMLæ ¼å¼

```
<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é”€å”®é¡¾é—®ã€‚<|im_end|>
<|im_start|>user
ä½ ä»¬çš„äº§å“å¤šå°‘é’±ï¼Ÿ<|im_end|>
<|im_start|>assistant
æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼ä¸ºäº†ç»™æ‚¨æ¨èæœ€åˆé€‚çš„æ–¹æ¡ˆ...<|im_end|>
```

#### Llama 3çš„æ ¼å¼

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é”€å”®é¡¾é—®ã€‚<|eot_id|><|start_header_id|>user<|end_header_id|>

ä½ ä»¬çš„äº§å“å¤šå°‘é’±ï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>

æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼...<|eot_id|>
```

#### é€šç”¨æ ¼å¼è½¬æ¢ä»£ç 

```python
"""
Chat Templateå¤„ç†å·¥å…·
æ”¯æŒä¸»æµæ¨¡å‹æ ¼å¼
"""
from transformers import AutoTokenizer
from typing import List, Dict

class ChatTemplateProcessor:
    def __init__(self, model_name: str):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    def apply_template(self, messages: List[Dict], add_generation_prompt: bool = False) -> str:
        """
        å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæ¨¡å‹ç‰¹å®šçš„æ ¼å¼

        Args:
            messages: [{"role": "system/user/assistant", "content": "..."}]
            add_generation_prompt: æ˜¯å¦æ·»åŠ ç”Ÿæˆæç¤ºï¼ˆæ¨ç†æ—¶ä¸ºTrueï¼‰
        """
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=add_generation_prompt
        )

    def prepare_training_sample(self, messages: List[Dict]) -> Dict:
        """
        å‡†å¤‡è®­ç»ƒæ ·æœ¬ï¼Œæ­£ç¡®è®¾ç½®labelsï¼ˆmaskæ‰instructionéƒ¨åˆ†ï¼‰
        """
        # å®Œæ•´å¯¹è¯æ–‡æœ¬
        full_text = self.apply_template(messages, add_generation_prompt=False)

        # Tokenize
        encodings = self.tokenizer(
            full_text,
            truncation=True,
            max_length=2048,
            return_tensors=None
        )

        # åˆ›å»ºlabelsï¼ˆå¤åˆ¶input_idsï¼‰
        labels = encodings["input_ids"].copy()

        # æ‰¾åˆ°assistantå›å¤çš„èµ·å§‹ä½ç½®ï¼Œmaskæ‰ä¹‹å‰çš„token
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡æ¿æ¥å®ç°
        labels = self._mask_instruction_tokens(labels, messages)

        return {
            "input_ids": encodings["input_ids"],
            "attention_mask": encodings["attention_mask"],
            "labels": labels
        }

    def _mask_instruction_tokens(self, labels: List[int], messages: List[Dict]) -> List[int]:
        """
        å°†éassistantéƒ¨åˆ†çš„tokenè®¾ä¸º-100ï¼ˆå¿½ç•¥lossè®¡ç®—ï¼‰

        âš ï¸ è­¦å‘Šï¼šä¸‹é¢çš„ç®€åŒ–å®ç°ä»…ç”¨äºæ¼”ç¤ºåŸç†ï¼
        å®é™…ç”Ÿäº§ä¸­å¿…é¡»ä½¿ç”¨æ¡†æ¶æä¾›çš„æ­£ç¡®å®ç°ï¼Œå¦åˆ™ä¼šå¯¼è‡´è®­ç»ƒå¤±è´¥ï¼

        æ¨èæ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1. ä½¿ç”¨ TRL çš„ DataCollatorForCompletionOnlyLM
        2. ä½¿ç”¨ LLaMA-Factory å†…ç½®çš„ mask å¤„ç†
        3. åŸºäºæ¨¡æ¿ç‰¹æ®Štokenç²¾ç¡®å®šä½ï¼ˆè§ä¸‹æ–¹æ­£ç¡®å®ç°ï¼‰
        """
        # âŒ é”™è¯¯å®ç°ï¼ˆä»…å±•ç¤ºåŸç†ï¼Œä¸å¯ç”¨äºè®­ç»ƒï¼‰ï¼š
        # ç›´æ¥æŒ‰æ–‡æœ¬å†…å®¹é•¿åº¦ä¼°ç®—ä¼šå› ä¸ºspecial tokenã€role headerç­‰å¯¼è‡´é”™ä½
        #
        # âœ… æ­£ç¡®å®ç°æ–¹æ¡ˆï¼šåŸºäºæ¨¡æ¿çš„responseæ ‡è®°å®šä½
        # æ ¸å¿ƒæ€è·¯ï¼šæ‰¾åˆ°assistantå›å¤å¼€å§‹çš„ç‰¹æ®Štokenä½ç½®

        # ä»¥Qwen/ChatMLæ ¼å¼ä¸ºä¾‹
        response_marker = "<|im_start|>assistant\n"
        response_end_marker = "<|im_end|>"

        # è·å–markerçš„token ids
        marker_ids = self.tokenizer.encode(response_marker, add_special_tokens=False)
        end_marker_ids = self.tokenizer.encode(response_end_marker, add_special_tokens=False)

        # åœ¨labelsä¸­æ‰¾åˆ°æ‰€æœ‰assistantå›å¤åŒºé—´
        input_ids = labels.copy()
        i = 0
        in_response = False

        while i < len(input_ids):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…responseå¼€å§‹marker
            if not in_response and self._match_sequence(input_ids, i, marker_ids):
                i += len(marker_ids)  # è·³è¿‡markeræœ¬èº«
                in_response = True
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…responseç»“æŸmarker
            if in_response and self._match_sequence(input_ids, i, end_marker_ids):
                in_response = False
                i += len(end_marker_ids)
                continue

            # éresponseåŒºåŸŸï¼Œmaskæ‰
            if not in_response:
                labels[i] = -100

            i += 1

        return labels

    def _match_sequence(self, ids: List[int], start: int, pattern: List[int]) -> bool:
        """æ£€æŸ¥idsä»startä½ç½®æ˜¯å¦åŒ¹é…pattern"""
        if start + len(pattern) > len(ids):
            return False
        return ids[start:start+len(pattern)] == pattern


# é”€å”®LLMç¤ºä¾‹
def create_sales_training_sample():
    """åˆ›å»ºé”€å”®åœºæ™¯çš„è®­ç»ƒæ ·æœ¬"""
    processor = ChatTemplateProcessor("Qwen/Qwen2.5-7B-Instruct")

    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é”€å”®é¡¾é—®ï¼Œæ“…é•¿ç†è§£å®¢æˆ·éœ€æ±‚ï¼Œæä¾›ä¸“ä¸šå»ºè®®ã€‚ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„æ€åº¦ã€‚"
        },
        {
            "role": "user",
            "content": "ä½ ä»¬è¿™ä¸ªäº§å“å¤šå°‘é’±ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": """æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼ä¸ºäº†ç»™æ‚¨æ¨èæœ€é€‚åˆçš„æ–¹æ¡ˆï¼Œæˆ‘æƒ³å…ˆäº†è§£å‡ ä¸ªé—®é¢˜ï¼š

1. **å›¢é˜Ÿè§„æ¨¡**ï¼šæ‚¨çš„å›¢é˜Ÿå¤§æ¦‚æœ‰å¤šå°‘äººä¼šä½¿ç”¨ï¼Ÿ
2. **æ ¸å¿ƒéœ€æ±‚**ï¼šæ‚¨ä¸»è¦æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ
3. **é¢„ç®—èŒƒå›´**ï¼šæ‚¨çš„é¢„ç®—å¤§æ¦‚åœ¨ä»€ä¹ˆåŒºé—´ï¼Ÿ

äº†è§£è¿™äº›ä¿¡æ¯åï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨å®šåˆ¶æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚æ‚¨æ–¹ä¾¿å‘Šè¯‰æˆ‘å—ï¼Ÿ"""
        }
    ]

    # ç”ŸæˆChatMLæ ¼å¼
    formatted = processor.apply_template(messages)
    print("=" * 50)
    print("ChatMLæ ¼å¼ï¼š")
    print(formatted)

    # å‡†å¤‡è®­ç»ƒæ ·æœ¬
    sample = processor.prepare_training_sample(messages)
    print("\nè®­ç»ƒæ ·æœ¬tokenæ•°é‡ï¼š", len(sample["input_ids"]))

    return sample
```

### 2.2 Loss Maskingè¯¦è§£

> âš ï¸ **å…³é”®è­¦å‘Š**ï¼šLoss Maskingæ˜¯SFTä¸­æœ€å®¹æ˜“å‡ºé”™çš„ç¯èŠ‚ï¼
> é”™è¯¯çš„maskingä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ é”™è¯¯çš„å†…å®¹ï¼Œè®­ç»ƒå½»åº•å¤±æ•ˆã€‚
> **å¿…é¡»åœ¨è®­ç»ƒå‰éªŒè¯maskingæ˜¯å¦æ­£ç¡®**ã€‚

**ä¸ºä»€ä¹ˆè¦Maskï¼Ÿ**
- åªè®­ç»ƒæ¨¡å‹ç”Ÿæˆresponseçš„èƒ½åŠ›
- instructionéƒ¨åˆ†ä¸å‚ä¸lossè®¡ç®—
- é¿å…æ¨¡å‹"å­¦ä¼š"è¾“å‡ºinstruction

#### ğŸ”´ MaskingéªŒè¯æ£€æŸ¥æ¸…å•ï¼ˆå¿…åšï¼ï¼‰

```python
def validate_loss_masking(tokenizer, sample):
    """
    éªŒè¯Loss Maskingæ˜¯å¦æ­£ç¡® â€”â€” è®­ç»ƒå‰å¿…é¡»è¿è¡Œï¼

    æ£€æŸ¥æ ‡å‡†ï¼š
    1. labelsä¸­é-100çš„tokenå¿…é¡»ä¸”åªèƒ½æ˜¯assistantå›å¤å†…å®¹
    2. æ‰€æœ‰system/userå†…å®¹çš„tokenå¿…é¡»è¢«maskï¼ˆ=-100ï¼‰
    3. ç‰¹æ®Štokenï¼ˆrole headerã€åˆ†éš”ç¬¦ï¼‰å¿…é¡»è¢«mask
    """
    input_ids = sample["input_ids"]
    labels = sample["labels"]

    print("=" * 60)
    print("Loss Masking éªŒè¯æŠ¥å‘Š")
    print("=" * 60)

    # è§£ç å¹¶å¯¹é½æ˜¾ç¤º
    for i, (input_id, label) in enumerate(zip(input_ids, labels)):
        token_str = tokenizer.decode([input_id])
        is_trained = label != -100
        marker = "âœ“ TRAIN" if is_trained else "  skip"
        print(f"{i:4d} | {marker} | {repr(token_str)}")

    # ç»Ÿè®¡
    train_count = sum(1 for l in labels if l != -100)
    skip_count = sum(1 for l in labels if l == -100)
    print(f"\nè®­ç»ƒtokenæ•°: {train_count}, è·³è¿‡tokenæ•°: {skip_count}")
    print(f"è®­ç»ƒæ¯”ä¾‹: {train_count / len(labels) * 100:.1f}%")

    # äººå·¥ç¡®è®¤æç¤º
    print("\nâš ï¸ è¯·äººå·¥æ£€æŸ¥ä¸Šè¿°è¾“å‡ºï¼š")
    print("   - æ ‡è®°ä¸ºTRAINçš„tokenæ˜¯å¦éƒ½æ˜¯assistantçš„å›å¤å†…å®¹ï¼Ÿ")
    print("   - system/userçš„å†…å®¹æ˜¯å¦éƒ½è¢«skipï¼Ÿ")
    print("   - ç‰¹æ®Štokenï¼ˆå¦‚<|im_start|>ï¼‰æ˜¯å¦éƒ½è¢«skipï¼Ÿ")

# ä½¿ç”¨ç¤ºä¾‹
# validate_loss_masking(tokenizer, train_dataset[0])
```

#### æ¨èçš„æ­£ç¡®å®ç°æ–¹å¼

| æ–¹å¼ | æ¨èåº¦ | è¯´æ˜ |
|------|--------|------|
| TRL `DataCollatorForCompletionOnlyLM` | â­â­â­â­â­ | æœ€å¯é ï¼Œè‡ªåŠ¨å¤„ç†å„ç§æ¨¡æ¿ |
| LLaMA-Factory å†…ç½®å¤„ç† | â­â­â­â­â­ | æ¡†æ¶è‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€æ‰‹åŠ¨ |
| åŸºäºæ¨¡æ¿tokenç²¾ç¡®å®šä½ | â­â­â­ | éœ€è¦æ·±å…¥ç†è§£æ¨¡æ¿ç»“æ„ |
| æŒ‰æ–‡æœ¬é•¿åº¦è¿‘ä¼¼ä¼°ç®— | âŒ | **ç»å¯¹ç¦æ­¢ï¼ä¼šå¯¼è‡´é”™ä½** |

```python
"""
Loss Maskingçš„ä¸‰ç§ç­–ç•¥å¯¹æ¯”
"""
import torch

def demonstrate_loss_masking():
    """
    æ¼”ç¤ºä¸åŒçš„loss maskingç­–ç•¥
    """
    # ç¤ºä¾‹tokenåºåˆ—ï¼ˆç®€åŒ–ï¼‰
    # [SYSTEM] [USER_TURN_1] [ASSISTANT_TURN_1] [USER_TURN_2] [ASSISTANT_TURN_2]
    tokens = ["<sys>", "ä½ å¥½", "<usr>", "äº§å“å¤šå°‘é’±", "<ast>", "æ„Ÿè°¢å’¨è¯¢", "æ–¹æ¡ˆå¦‚ä¸‹", "<usr>", "æœ‰ä¼˜æƒ å—", "<ast>", "å½“ç„¶æœ‰"]

    print("=" * 60)
    print("ä¸‰ç§Loss Maskingç­–ç•¥å¯¹æ¯”")
    print("=" * 60)

    # ç­–ç•¥1: å®Œå…¨ä¸Maskï¼ˆä¸æ¨èï¼‰
    print("\nç­–ç•¥1: å®Œå…¨ä¸Mask")
    print("è®¡ç®—Lossçš„Token: å…¨éƒ¨")
    print("é—®é¢˜: æ¨¡å‹ä¼šå­¦ä¹ è¾“å‡ºinstructionï¼Œä¸ç¬¦åˆé¢„æœŸ")

    # ç­–ç•¥2: Maskæ‰€æœ‰instructionï¼ˆæ ‡å‡†åšæ³•ï¼‰
    print("\nç­–ç•¥2: Maskæ‰€æœ‰instructionï¼ˆæ¨èï¼‰")
    labels_2 = [-100, -100, -100, -100, 0, 0, 0, -100, -100, 0, 0]  # -100è¡¨ç¤ºå¿½ç•¥
    print(f"Labels: {labels_2}")
    print("è®¡ç®—Lossçš„Token: åªæœ‰assistantå›å¤")
    print("ä¼˜ç‚¹: æ¨¡å‹åªå­¦ä¹ å¦‚ä½•å›å¤")

    # ç­–ç•¥3: åªMaskæ¨¡æ¿tokenï¼ˆç ”ç©¶ä¸­ï¼‰
    print("\nç­–ç•¥3: åªMaskæ¨¡æ¿tokenï¼ˆå®éªŒæ€§ï¼‰")
    labels_3 = [-100, 0, -100, 0, -100, 0, 0, -100, 0, -100, 0]
    print(f"Labels: {labels_3}")
    print("è®¡ç®—Lossçš„Token: instructionå†…å®¹ + assistantå›å¤")
    print("ç ”ç©¶å‘ç°: æŸäº›æƒ…å†µä¸‹æ•ˆæœæ›´å¥½ï¼Œä½†éœ€è¦å®éªŒéªŒè¯")


# TRLåº“ä¸­çš„å®ç°
def trl_style_masking(tokenizer, messages, response_template):
    """
    TRL SFTTraineré£æ ¼çš„maskingå®ç°
    """
    from trl import DataCollatorForCompletionOnlyLM

    # å®šä¹‰responseèµ·å§‹æ ‡è®°
    # å¯¹äºQwen: "<|im_start|>assistant\n"
    # å¯¹äºLlama3: "<|start_header_id|>assistant<|end_header_id|>\n\n"

    collator = DataCollatorForCompletionOnlyLM(
        response_template=response_template,
        tokenizer=tokenizer
    )

    # collatorä¼šè‡ªåŠ¨å¤„ç†labelsï¼Œå°†éresponseéƒ¨åˆ†è®¾ä¸º-100
    return collator
```

### 2.3 å¤šè½®å¯¹è¯æ•°æ®å¤„ç†

```python
"""
å¤šè½®å¯¹è¯çš„å®Œæ•´å¤„ç†æµç¨‹
"""
from typing import List, Dict
import json

class MultiTurnDataProcessor:
    def __init__(self, tokenizer, max_length: int = 2048):
        self.tokenizer = tokenizer
        self.max_length = max_length

    def process_conversation(self, conversation: Dict) -> List[Dict]:
        """
        å¤„ç†ä¸€ä¸ªå¤šè½®å¯¹è¯ï¼Œç”Ÿæˆå¤šä¸ªè®­ç»ƒæ ·æœ¬

        ç­–ç•¥é€‰æ‹©ï¼š
        1. æ•´ä½“ä½œä¸ºä¸€ä¸ªæ ·æœ¬ï¼ˆæ¨èï¼‰
        2. æ¯è½®æ‹†åˆ†ä¸ºç‹¬ç«‹æ ·æœ¬
        3. æ»‘åŠ¨çª—å£æ–¹å¼
        """
        messages = conversation.get("messages", [])

        # ç­–ç•¥1: æ•´ä½“å¤„ç†ï¼ˆæ¨èï¼‰
        return [self._process_full_conversation(messages)]

    def _process_full_conversation(self, messages: List[Dict]) -> Dict:
        """
        å°†å®Œæ•´å¤šè½®å¯¹è¯ä½œä¸ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        """
        # åº”ç”¨chat template
        formatted = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )

        # Tokenize
        encodings = self.tokenizer(
            formatted,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors=None
        )

        # åˆ›å»ºlabels
        labels = self._create_multi_turn_labels(encodings["input_ids"], messages)

        return {
            "input_ids": encodings["input_ids"],
            "attention_mask": encodings["attention_mask"],
            "labels": labels
        }

    def _create_multi_turn_labels(self, input_ids: List[int], messages: List[Dict]) -> List[int]:
        """
        ä¸ºå¤šè½®å¯¹è¯åˆ›å»ºlabels
        æ‰€æœ‰user/systeméƒ¨åˆ†maskï¼Œæ‰€æœ‰assistantéƒ¨åˆ†ä¿ç•™
        """
        labels = [-100] * len(input_ids)

        # æ‰¾åˆ°æ¯ä¸ªassistantå›å¤çš„ä½ç½®
        # è¿™éœ€è¦æ ¹æ®å…·ä½“çš„chat templateæ¥å®ç°
        # ä»¥ä¸‹æ˜¯é€šç”¨é€»è¾‘æ¡†æ¶

        current_pos = 0
        for msg in messages:
            # è·å–è¿™æ¡æ¶ˆæ¯çš„token
            msg_text = msg["content"]
            msg_tokens = self.tokenizer.encode(msg_text, add_special_tokens=False)

            if msg["role"] == "assistant":
                # æ‰¾åˆ°è¿™äº›tokenåœ¨input_idsä¸­çš„ä½ç½®
                # è®¾ç½®labelsä¸ºå®é™…tokenå€¼
                start_pos = self._find_subsequence(input_ids, msg_tokens, current_pos)
                if start_pos != -1:
                    for i, token in enumerate(msg_tokens):
                        if start_pos + i < len(labels):
                            labels[start_pos + i] = input_ids[start_pos + i]
                    current_pos = start_pos + len(msg_tokens)

        return labels

    def _find_subsequence(self, sequence: List[int], subsequence: List[int], start: int = 0) -> int:
        """æ‰¾åˆ°å­åºåˆ—åœ¨åºåˆ—ä¸­çš„èµ·å§‹ä½ç½®"""
        for i in range(start, len(sequence) - len(subsequence) + 1):
            if sequence[i:i+len(subsequence)] == subsequence:
                return i
        return -1


# é”€å”®å¤šè½®å¯¹è¯ç¤ºä¾‹
sales_conversation = {
    "conversation_id": "sales_001",
    "messages": [
        {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šé”€å”®é¡¾é—®ï¼Œå¸®åŠ©å®¢æˆ·é€‰æ‹©åˆé€‚çš„äº§å“æ–¹æ¡ˆã€‚"},
        {"role": "user", "content": "ä½ ä»¬çš„CRMç³»ç»Ÿå¤šå°‘é’±ï¼Ÿ"},
        {"role": "assistant", "content": "æ„Ÿè°¢å’¨è¯¢ï¼ä»·æ ¼å–å†³äºæ‚¨çš„å…·ä½“éœ€æ±‚ã€‚è¯·é—®ï¼š\n1. å›¢é˜Ÿè§„æ¨¡å¤šå¤§ï¼Ÿ\n2. ä¸»è¦ç”¨äºå“ªäº›åœºæ™¯ï¼Ÿ"},
        {"role": "user", "content": "å¤§æ¦‚30äººï¼Œä¸»è¦åšå®¢æˆ·ç®¡ç†å’Œé”€å”®è·Ÿè¿›"},
        {"role": "assistant", "content": "äº†è§£ï¼30äººå›¢é˜Ÿåšå®¢æˆ·ç®¡ç†ï¼Œæ¨èæˆ‘ä»¬çš„ä¸“ä¸šç‰ˆï¼š\n\n**ä¸“ä¸šç‰ˆ** Â¥4,800/æœˆ\n- æ”¯æŒ50ç”¨æˆ·\n- å®Œæ•´CRMåŠŸèƒ½\n- é”€å”®æ¼æ–—åˆ†æ\n- è‡ªåŠ¨åŒ–å·¥ä½œæµ\n\nå¹´ä»˜å¯äº«8æŠ˜ä¼˜æƒ ï¼Œåˆè®¡Â¥46,080/å¹´ã€‚éœ€è¦æˆ‘è¯¦ç»†ä»‹ç»æŸä¸ªåŠŸèƒ½å—ï¼Ÿ"},
        {"role": "user", "content": "æœ‰æ²¡æœ‰æ›´ä¾¿å®œçš„æ–¹æ¡ˆï¼Ÿ"},
        {"role": "assistant", "content": "ç†è§£æ‚¨å¯¹æˆæœ¬çš„è€ƒè™‘ã€‚æˆ‘ä»¬è¿˜æœ‰**åŸºç¡€ç‰ˆ** Â¥2,400/æœˆï¼š\n\n- æ”¯æŒ30ç”¨æˆ·\n- æ ¸å¿ƒCRMåŠŸèƒ½\n- åŸºç¡€æŠ¥è¡¨\n\nä¸è¿‡ä¸“ä¸šç‰ˆçš„é”€å”®æ¼æ–—åˆ†æèƒ½å¸®æ‚¨æå‡20%è½¬åŒ–ç‡ï¼Œä¸€ä¸ªæœˆå¤šæˆäº¤å‡ å•å°±å›æœ¬äº†ã€‚\n\næ‚¨çœ‹è¿™æ ·ï¼Œæˆ‘ç”³è¯·ä¸€ä¸‹é¦–å¹´7æŠ˜ä¼˜æƒ ï¼Œä¸“ä¸šç‰ˆå®é™…åªè¦Â¥40,320ï¼Œæ‚¨è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿ"}
    ],
    "outcome": "won"
}
```

---

## ä¸‰ã€SFTè®­ç»ƒå®æˆ˜

### 3.1 ä½¿ç”¨LLaMA-Factoryè¿›è¡ŒSFT

```yaml
# configs/sales_sft.yaml
# é”€å”®LLM SFTé…ç½®

### æ¨¡å‹é…ç½®
model_name_or_path: Qwen/Qwen2.5-7B-Instruct  # æˆ–ä½¿ç”¨CPTåçš„æ¨¡å‹
trust_remote_code: true

### è®­ç»ƒæ–¹æ³•
stage: sft
do_train: true
finetuning_type: lora  # ä½¿ç”¨LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒ

### LoRAé…ç½®
lora_target: all  # æˆ–æŒ‡å®š: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
lora_rank: 64
lora_alpha: 128  # é€šå¸¸è®¾ä¸ºrankçš„2å€
lora_dropout: 0.05

### æ•°æ®é…ç½®
dataset: sales_sft
template: qwen  # ä½¿ç”¨Qwençš„chat template
cutoff_len: 2048
max_samples: 50000  # æœ€å¤šä½¿ç”¨å¤šå°‘æ ·æœ¬
preprocessing_num_workers: 16

### è®­ç»ƒè¶…å‚æ•°
per_device_train_batch_size: 4
gradient_accumulation_steps: 4  # æœ‰æ•ˆbatch size = 16
num_train_epochs: 3
learning_rate: 5e-5
lr_scheduler_type: cosine
warmup_ratio: 0.1

### ä¼˜åŒ–å™¨
optim: adamw_torch
weight_decay: 0.01
max_grad_norm: 1.0

### ç²¾åº¦
bf16: true

### ä¿å­˜é…ç½®
output_dir: ./output/sales_sft
logging_steps: 10
save_steps: 200
save_total_limit: 5

### è¯„ä¼°é…ç½®ï¼ˆå¯é€‰ï¼‰
val_size: 0.05
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 200
```

```json
// dataset_info.json - å®šä¹‰SFTæ•°æ®é›†
{
  "sales_sft": {
    "file_name": "data/sft/sales_sharegpt.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations",
      "system": "system"
    },
    "tags": {
      "role_tag": "from",
      "content_tag": "value",
      "user_tag": "human",
      "assistant_tag": "gpt"
    }
  }
}
```

```bash
# å¯åŠ¨è®­ç»ƒ
# å•å¡
llamafactory-cli train configs/sales_sft.yaml

# å¤šå¡
CUDA_VISIBLE_DEVICES=0,1,2,3 llamafactory-cli train configs/sales_sft.yaml

# å¸¦è¯„ä¼°çš„è®­ç»ƒ
llamafactory-cli train configs/sales_sft.yaml \
    --val_size 0.05 \
    --eval_strategy steps \
    --eval_steps 200
```

### 3.2 ä½¿ç”¨TRLåº“è¿›è¡ŒSFT

```python
"""
ä½¿ç”¨HuggingFace TRLè¿›è¡ŒSFT
æ›´çµæ´»çš„æ§åˆ¶å’Œè‡ªå®šä¹‰
"""
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig
)
from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

def train_sales_sft():
    # ============ 1. é…ç½® ============
    model_name = "Qwen/Qwen2.5-7B-Instruct"
    output_dir = "./output/sales_sft_trl"

    # ============ 2. åŠ è½½æ¨¡å‹ ============
    # é‡åŒ–é…ç½®ï¼ˆQLoRAï¼‰
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # ============ 3. LoRAé…ç½® ============
    model = prepare_model_for_kbit_training(model)

    lora_config = LoraConfig(
        r=64,                    # LoRA rank
        lora_alpha=128,          # LoRA alpha
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[         # ç›®æ ‡æ¨¡å—
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ]
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # ============ 4. åŠ è½½æ•°æ® ============
    dataset = load_dataset("json", data_files="./data/sft/sales_sharegpt.json")

    def formatting_func(example):
        """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºå¯¹è¯"""
        messages = example["conversations"]
        # è½¬æ¢æ ¼å¼
        formatted_messages = []
        for msg in messages:
            role = "user" if msg["from"] == "human" else "assistant"
            formatted_messages.append({"role": role, "content": msg["value"]})

        # æ·»åŠ systemï¼ˆå¦‚æœæœ‰ï¼‰
        if "system" in example and example["system"]:
            formatted_messages.insert(0, {"role": "system", "content": example["system"]})

        return tokenizer.apply_chat_template(formatted_messages, tokenize=False)

    # ============ 5. æ•°æ®æ•´ç†å™¨ ============
    # è®¾ç½®response templateç”¨äºloss masking
    # Qwenæ ¼å¼: <|im_start|>assistant\n
    response_template = "<|im_start|>assistant\n"

    collator = DataCollatorForCompletionOnlyLM(
        response_template=response_template,
        tokenizer=tokenizer
    )

    # ============ 6. è®­ç»ƒé…ç½® ============
    training_args = SFTConfig(
        output_dir=output_dir,

        # è®­ç»ƒå‚æ•°
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,

        # å­¦ä¹ ç‡
        learning_rate=5e-5,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,

        # ä¼˜åŒ–
        optim="paged_adamw_8bit",  # 8-bitä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜
        weight_decay=0.01,
        max_grad_norm=1.0,

        # ç²¾åº¦
        bf16=True,

        # åºåˆ—é•¿åº¦
        max_seq_length=2048,
        packing=False,  # æ˜¯å¦æ‰“åŒ…çŸ­åºåˆ—

        # æ—¥å¿—å’Œä¿å­˜
        logging_steps=10,
        save_steps=200,
        save_total_limit=5,

        # è¯„ä¼°
        eval_strategy="steps",
        eval_steps=200,
    )

    # ============ 7. è®­ç»ƒ ============
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        formatting_func=formatting_func,
        data_collator=collator,
        tokenizer=tokenizer,
    )

    trainer.train()

    # ============ 8. ä¿å­˜ ============
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)

    print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³: {output_dir}")


if __name__ == "__main__":
    train_sales_sft()
```

### 3.3 ä½¿ç”¨Unslothè¿›è¡Œé«˜æ•ˆSFT

```python
"""
ä½¿ç”¨Unslothè¿›è¡Œ2-5å€åŠ é€Ÿçš„SFTè®­ç»ƒ
æ˜¾å­˜å ç”¨æ›´ä½ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«
"""
from unsloth import FastLanguageModel
from unsloth import is_bfloat16_supported
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

def train_with_unsloth():
    # ============ 1. åŠ è½½æ¨¡å‹ï¼ˆUnslothä¼˜åŒ–ç‰ˆæœ¬ï¼‰============
    max_seq_length = 2048
    dtype = None  # è‡ªåŠ¨æ£€æµ‹
    load_in_4bit = True

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/Qwen2.5-7B-Instruct",  # Unslothä¼˜åŒ–ç‰ˆæœ¬
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
    )

    # ============ 2. æ·»åŠ LoRAé€‚é…å™¨ ============
    model = FastLanguageModel.get_peft_model(
        model,
        r=64,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_alpha=128,
        lora_dropout=0,  # Unslothå»ºè®®è®¾ä¸º0
        bias="none",
        use_gradient_checkpointing="unsloth",  # ä½¿ç”¨Unslothä¼˜åŒ–çš„checkpointing
        random_state=42,
    )

    # ============ 3. å‡†å¤‡æ•°æ® ============
    dataset = load_dataset("json", data_files="./data/sft/sales_sharegpt.json")

    def formatting_prompts_func(examples):
        """æ ¼å¼åŒ–å‡½æ•°"""
        conversations = examples["conversations"]
        texts = []

        for conv in conversations:
            messages = []
            for msg in conv:
                role = "user" if msg["from"] == "human" else "assistant"
                messages.append({"role": role, "content": msg["value"]})

            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
            texts.append(text)

        return {"text": texts}

    dataset = dataset.map(formatting_prompts_func, batched=True)

    # ============ 4. è®­ç»ƒ ============
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset["train"],
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        dataset_num_proc=8,
        packing=False,
        args=TrainingArguments(
            output_dir="./output/sales_sft_unsloth",
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            num_train_epochs=3,
            learning_rate=5e-5,
            lr_scheduler_type="cosine",
            warmup_ratio=0.1,
            bf16=is_bfloat16_supported(),
            logging_steps=10,
            save_steps=200,
            optim="adamw_8bit",
            weight_decay=0.01,
            seed=42,
        ),
    )

    # GPUçŠ¶æ€
    gpu_stats = torch.cuda.get_device_properties(0)
    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
    print(f"GPU: {gpu_stats.name}, æ˜¾å­˜: {max_memory}GB, å·²ç”¨: {start_gpu_memory}GB")

    trainer.train()

    # ============ 5. ä¿å­˜ ============
    # ä¿å­˜LoRAé€‚é…å™¨
    model.save_pretrained("./output/sales_sft_unsloth/lora")

    # åˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    # model.save_pretrained_merged("./output/sales_sft_unsloth/merged", tokenizer)

    # ä¿å­˜ä¸ºGGUFæ ¼å¼ï¼ˆå¯é€‰ï¼Œç”¨äºllama.cppï¼‰
    # model.save_pretrained_gguf("./output/sales_sft_unsloth/gguf", tokenizer, quantization_method="q4_k_m")


if __name__ == "__main__":
    import torch
    train_with_unsloth()
```

---

## å››ã€SFTè¶…å‚æ•°è°ƒä¼˜

### 4.1 å­¦ä¹ ç‡é€‰æ‹©

```python
"""
å­¦ä¹ ç‡é€‰æ‹©æŒ‡å—
"""
learning_rate_guide = {
    "å…¨å‚å¾®è°ƒ (Full Fine-tuning)": {
        "æ¨èå€¼": "1e-5 ~ 2e-5",
        "è¯´æ˜": "å‚æ•°é‡å¤§ï¼Œéœ€è¦å°å­¦ä¹ ç‡é˜²æ­¢éœ‡è¡",
        "ç¤ºä¾‹": 2e-5
    },
    "LoRAå¾®è°ƒ": {
        "æ¨èå€¼": "1e-4 ~ 5e-5",
        "è¯´æ˜": "åªæ›´æ–°å°‘é‡å‚æ•°ï¼Œå¯ä»¥ç”¨ç¨å¤§å­¦ä¹ ç‡",
        "ç¤ºä¾‹": 5e-5
    },
    "QLoRAå¾®è°ƒ": {
        "æ¨èå€¼": "1e-4 ~ 2e-4",
        "è¯´æ˜": "4-bité‡åŒ–åå¯ä»¥ç”¨æ›´å¤§å­¦ä¹ ç‡",
        "ç¤ºä¾‹": 1e-4
    }
}

# å­¦ä¹ ç‡ä¸batch sizeçš„å…³ç³»
# ç»éªŒå…¬å¼ï¼šlr_new = lr_base * sqrt(batch_size_new / batch_size_base)
def adjust_learning_rate(base_lr, base_batch_size, new_batch_size):
    import math
    return base_lr * math.sqrt(new_batch_size / base_batch_size)

# ç¤ºä¾‹ï¼šbase_lr=5e-5, base_batch=8, new_batch=32
# æ–°å­¦ä¹ ç‡ = 5e-5 * sqrt(32/8) = 5e-5 * 2 = 1e-4
```

### 4.2 LoRAå‚æ•°é€‰æ‹©

```python
"""
LoRAè¶…å‚æ•°é€‰æ‹©æŒ‡å—
"""
lora_config_guide = {
    "rank (r)": {
        "ç®€å•ä»»åŠ¡ï¼ˆæ ¼å¼è°ƒæ•´ç­‰ï¼‰": "r=8~16",
        "ä¸­ç­‰ä»»åŠ¡ï¼ˆé¢†åŸŸå¾®è°ƒï¼‰": "r=32~64",
        "å¤æ‚ä»»åŠ¡ï¼ˆå¤§è§„æ¨¡å¾®è°ƒï¼‰": "r=128~256",
        "æ³¨æ„": "QLoRAè®ºæ–‡å‘ç°r=8å’Œr=256æ•ˆæœå·®å¼‚ä¸å¤§ï¼ˆå¦‚æœå…¨å±‚éƒ½ç”¨LoRAï¼‰"
    },
    "alpha (Î±)": {
        "ä¿å®ˆè®¾ç½®": "Î± = rï¼ˆscale factor = 1ï¼‰",
        "æ¨èè®¾ç½®": "Î± = 2rï¼ˆscale factor = 2ï¼‰",
        "æ¿€è¿›è®¾ç½®": "Î± = 4r",
        "è¯´æ˜": "å®é™…ç¼©æ”¾å› å­ = Î±/r"
    },
    "target_modules": {
        "æœ€å°é…ç½®": ["q_proj", "v_proj"],
        "æ¨èé…ç½®": ["q_proj", "k_proj", "v_proj", "o_proj"],
        "å®Œæ•´é…ç½®": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "è¯´æ˜": "QLoRAè®ºæ–‡å»ºè®®å…¨å±‚éƒ½ç”¨LoRAæ•ˆæœæœ€å¥½"
    },
    "dropout": {
        "ä¸€èˆ¬è®¾ç½®": 0.05,
        "æ•°æ®é‡å¤§": 0.0,  # Unslothå»ºè®®
        "æ•°æ®é‡å°": 0.1
    }
}

# ä¸åŒåœºæ™¯çš„LoRAé…ç½®ç¤ºä¾‹
def get_lora_config(scenario: str):
    from peft import LoraConfig

    configs = {
        "å¿«é€ŸéªŒè¯": LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        ),
        "æ ‡å‡†ç”Ÿäº§": LoraConfig(
            r=64,
            lora_alpha=128,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        ),
        "é«˜è´¨é‡": LoraConfig(
            r=128,
            lora_alpha=256,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.0,
            bias="none",
            task_type="CAUSAL_LM"
        )
    }
    return configs.get(scenario)
```

### 4.3 è®­ç»ƒè½®æ•°é€‰æ‹©

```python
"""
è®­ç»ƒè½®æ•°é€‰æ‹©
"""
epoch_guide = {
    "æ•°æ®é‡ < 1k": {
        "æ¨èepochs": "5-10",
        "é£é™©": "å®¹æ˜“è¿‡æ‹Ÿåˆ",
        "å»ºè®®": "ä½¿ç”¨æ›´å¤§dropoutï¼Œæ—©åœ"
    },
    "æ•°æ®é‡ 1k-10k": {
        "æ¨èepochs": "2-5",
        "è¯´æ˜": "æ ‡å‡†é…ç½®"
    },
    "æ•°æ®é‡ 10k-100k": {
        "æ¨èepochs": "1-3",
        "è¯´æ˜": "æ•°æ®é‡å……è¶³ï¼Œä¸éœ€è¦å¤ªå¤šè½®"
    },
    "æ•°æ®é‡ > 100k": {
        "æ¨èepochs": "1-2",
        "è¯´æ˜": "å¯èƒ½1è½®å°±å¤Ÿäº†"
    }
}

# æ—©åœç­–ç•¥
def should_early_stop(eval_losses: list, patience: int = 3):
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
    è¿ç»­patienceæ¬¡è¯„ä¼°lossä¸ä¸‹é™åˆ™åœæ­¢
    """
    if len(eval_losses) < patience + 1:
        return False

    recent = eval_losses[-patience:]
    return all(recent[i] >= recent[i-1] for i in range(1, len(recent)))
```

---

## äº”ã€SFTå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 5.1 è¿‡æ‹Ÿåˆé—®é¢˜

```python
"""
è¿‡æ‹Ÿåˆçš„è¯Šæ–­ä¸è§£å†³
"""
# è¯Šæ–­ä¿¡å·
overfitting_signals = [
    "è®­ç»ƒlossæŒç»­ä¸‹é™ï¼Œä½†éªŒè¯losså¼€å§‹ä¸Šå‡",
    "æ¨¡å‹å¼€å§‹é€å­—å¤è¿°è®­ç»ƒæ•°æ®",
    "å¯¹è®­ç»ƒæ•°æ®å¤–çš„é—®é¢˜å›ç­”è´¨é‡ä¸‹é™",
    "ç”Ÿæˆå†…å®¹å¤šæ ·æ€§é™ä½"
]

# è§£å†³æ–¹æ¡ˆ
solutions = {
    "å‡å°‘è®­ç»ƒè½®æ•°": "ä»3è½®å‡åˆ°1-2è½®",
    "å¢å¤§dropout": "LoRA dropoutä»0.05å¢åˆ°0.1",
    "å‡å°å­¦ä¹ ç‡": "å­¦ä¹ ç‡å‡åŠ",
    "å¢åŠ æ•°æ®å¤šæ ·æ€§": "æ··å…¥æ›´å¤šé€šç”¨æ•°æ®",
    "æ—©åœ": "ç›‘æ§éªŒè¯lossï¼Œè¿ç»­3æ¬¡ä¸ä¸‹é™å°±åœæ­¢",
    "æƒé‡è¡°å‡": "å¢å¤§weight_decay",
}
```

### 5.2 ç¾éš¾æ€§é—å¿˜

```python
"""
ç¾éš¾æ€§é—å¿˜çš„é¢„é˜²
"""
# é¢„é˜²æªæ–½
prevention_measures = {
    "æ•°æ®æ··åˆ": {
        "æ–¹æ³•": "é¢†åŸŸæ•°æ®70% + é€šç”¨æ•°æ®30%",
        "è¯´æ˜": "ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶å­¦ä¹ é¢†åŸŸçŸ¥è¯†"
    },
    "ä½¿ç”¨LoRA": {
        "æ–¹æ³•": "åªæ›´æ–°å°‘é‡å‚æ•°",
        "è¯´æ˜": "å‡å°‘å¯¹åŸå§‹æƒé‡çš„å¹²æ‰°"
    },
    "å°å­¦ä¹ ç‡": {
        "æ–¹æ³•": "ä½¿ç”¨ä¿å®ˆçš„å­¦ä¹ ç‡",
        "è¯´æ˜": "é¿å…å‰§çƒˆæ›´æ–°ç ´ååŸæœ‰çŸ¥è¯†"
    },
    "æ¸è¿›å¼è®­ç»ƒ": {
        "æ–¹æ³•": "å…ˆç”¨é€šç”¨æ•°æ®çƒ­èº«ï¼Œå†åŠ å…¥é¢†åŸŸæ•°æ®",
        "è¯´æ˜": "è®©æ¨¡å‹å¹³æ»‘è¿‡æ¸¡"
    }
}

# æ£€æµ‹é—å¿˜çš„æ–¹æ³•
def check_forgetting(model, tokenizer, general_test_cases: list):
    """
    åœ¨é€šç”¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦é—å¿˜
    """
    results = []
    for case in general_test_cases:
        prompt = case["prompt"]
        expected_keywords = case["expected_keywords"]

        # ç”Ÿæˆå›å¤
        response = generate_response(model, tokenizer, prompt)

        # æ£€æŸ¥å…³é”®è¯æ˜¯å¦å‡ºç°
        score = sum(1 for kw in expected_keywords if kw in response) / len(expected_keywords)
        results.append({"prompt": prompt, "score": score, "response": response})

    avg_score = sum(r["score"] for r in results) / len(results)
    print(f"é€šç”¨èƒ½åŠ›ä¿æŒç‡: {avg_score:.2%}")

    if avg_score < 0.7:
        print("è­¦å‘Šï¼šæ£€æµ‹åˆ°æ˜æ˜¾çš„èƒ½åŠ›é—å¿˜ï¼")

    return results
```

### 5.3 è®­ç»ƒä¸ç¨³å®š

```python
"""
è®­ç»ƒä¸ç¨³å®šçš„è§£å†³æ–¹æ¡ˆ
"""
stability_tips = {
    "Losséœ‡è¡": {
        "ç—‡çŠ¶": "Lossä¸Šä¸‹å‰§çƒˆæ³¢åŠ¨",
        "åŸå› ": "å­¦ä¹ ç‡è¿‡å¤§æˆ–batch sizeè¿‡å°",
        "è§£å†³": "å‡å°å­¦ä¹ ç‡ï¼Œå¢å¤§batch sizeæˆ–gradient accumulation"
    },
    "Lossä¸ä¸‹é™": {
        "ç—‡çŠ¶": "LossåŸºæœ¬ä¸å˜",
        "åŸå› ": "å­¦ä¹ ç‡è¿‡å°æˆ–æ•°æ®é—®é¢˜",
        "è§£å†³": "å¢å¤§å­¦ä¹ ç‡ï¼Œæ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®"
    },
    "Lossçªç„¶çˆ†ç‚¸": {
        "ç—‡çŠ¶": "Lossçªç„¶å˜æˆNaNæˆ–æå¤§å€¼",
        "åŸå› ": "æ•°å€¼ä¸ç¨³å®š",
        "è§£å†³": "ä½¿ç”¨BF16ä»£æ›¿FP16ï¼Œå‡å°å­¦ä¹ ç‡ï¼Œå¢åŠ æ¢¯åº¦è£å‰ª"
    },
    "Lossä¸‹é™ååˆä¸Šå‡": {
        "ç—‡çŠ¶": "å…ˆä¸‹é™åæŒç»­ä¸Šå‡",
        "åŸå› ": "è¿‡æ‹Ÿåˆæˆ–å­¦ä¹ ç‡è°ƒåº¦é—®é¢˜",
        "è§£å†³": "æ—©åœï¼Œä½¿ç”¨cosineè¡°å‡"
    }
}

# ç¨³å®šè®­ç»ƒçš„é…ç½®æ¨¡æ¿
stable_training_config = {
    "learning_rate": 2e-5,  # ä¿å®ˆå­¦ä¹ ç‡
    "warmup_ratio": 0.1,    # 10%é¢„çƒ­
    "lr_scheduler_type": "cosine",
    "max_grad_norm": 1.0,   # æ¢¯åº¦è£å‰ª
    "bf16": True,           # ä½¿ç”¨BF16
    "gradient_accumulation_steps": 8,  # è¶³å¤Ÿå¤§çš„æœ‰æ•ˆbatch
    "weight_decay": 0.01,   # è½»å¾®æƒé‡è¡°å‡
}
```

---

## å…­ã€SFTæ•ˆæœè¯„ä¼°

### 6.1 å¿«é€Ÿè¯„ä¼°æ–¹æ³•

```python
"""
SFTæ•ˆæœå¿«é€Ÿè¯„ä¼°
"""
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class SFTEvaluator:
    def __init__(self, model_path: str):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        self.model.eval()

    def vibe_check(self, test_cases: list):
        """
        Vibe Checkï¼šäººå·¥å¿«é€Ÿæ£€æŸ¥æ¨¡å‹è¾“å‡ºè´¨é‡
        """
        print("=" * 60)
        print("SFTæ•ˆæœ Vibe Check")
        print("=" * 60)

        for i, case in enumerate(test_cases, 1):
            prompt = case["prompt"]
            print(f"\nã€æµ‹è¯• {i}ã€‘{case.get('description', '')}")
            print(f"è¾“å…¥: {prompt}")

            response = self._generate(prompt)
            print(f"è¾“å‡º: {response}")

            # æ£€æŸ¥ç‚¹
            checks = case.get("checks", [])
            for check in checks:
                passed = check["condition"](response)
                status = "âœ…" if passed else "âŒ"
                print(f"  {status} {check['name']}")

            print("-" * 40)

    def evaluate_instruction_following(self, test_cases: list) -> dict:
        """è¯„ä¼°æŒ‡ä»¤éµå¾ªèƒ½åŠ›"""
        results = {
            "total": len(test_cases),
            "passed": 0,
            "failed": 0,
            "details": []
        }

        for case in test_cases:
            response = self._generate(case["prompt"])
            passed = case["validator"](response)

            results["passed" if passed else "failed"] += 1
            results["details"].append({
                "prompt": case["prompt"],
                "response": response,
                "passed": passed
            })

        results["accuracy"] = results["passed"] / results["total"]
        return results

    def _generate(self, prompt: str, max_new_tokens: int = 512) -> str:
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )

        response = self.tokenizer.decode(outputs[0][inputs["input_ids"].size(1):], skip_special_tokens=True)
        return response.strip()


# é”€å”®LLMæµ‹è¯•ç”¨ä¾‹
sales_test_cases = [
    {
        "description": "ä»·æ ¼å’¨è¯¢å¤„ç†",
        "prompt": "ä½ ä»¬çš„äº§å“å¤šå°‘é’±ï¼Ÿ",
        "checks": [
            {"name": "ä¸ç›´æ¥æŠ¥ä»·è€Œæ˜¯å…ˆäº†è§£éœ€æ±‚", "condition": lambda r: "äº†è§£" in r or "éœ€æ±‚" in r or "?" in r},
            {"name": "ä¿æŒä¸“ä¸šå‹å¥½", "condition": lambda r: "æ„Ÿè°¢" in r or "æ‚¨å¥½" in r},
        ]
    },
    {
        "description": "å¼‚è®®å¤„ç† - å¤ªè´µäº†",
        "prompt": "å¤ªè´µäº†ï¼Œèƒ½ä¾¿å®œç‚¹å—ï¼Ÿ",
        "checks": [
            {"name": "ç†è§£å®¢æˆ·é¡¾è™‘", "condition": lambda r: "ç†è§£" in r or "æ˜ç™½" in r},
            {"name": "æä¾›ä»·å€¼åˆ†æ", "condition": lambda r: "ä»·å€¼" in r or "æ•ˆæœ" in r or "èŠ‚çœ" in r},
        ]
    },
    {
        "description": "ç«å“å¯¹æ¯”",
        "prompt": "ä½ ä»¬å’ŒXXå…¬å¸æ¯”æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "checks": [
            {"name": "ä¸è´¬ä½ç«å“", "condition": lambda r: "å·®" not in r and "ä¸å¥½" not in r},
            {"name": "çªå‡ºè‡ªèº«ä¼˜åŠ¿", "condition": lambda r: "ä¼˜åŠ¿" in r or "ç‰¹ç‚¹" in r},
        ]
    },
    {
        "description": "æ ¼å¼è¾“å‡ºèƒ½åŠ›",
        "prompt": "è¯·ç”¨JSONæ ¼å¼åˆ—å‡ºäº§å“çš„ä¸‰ä¸ªä¸»è¦ç‰¹ç‚¹",
        "checks": [
            {"name": "è¾“å‡ºåŒ…å«JSONæ ¼å¼", "condition": lambda r: "{" in r and "}" in r},
        ]
    }
]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    evaluator = SFTEvaluator("./output/sales_sft")
    evaluator.vibe_check(sales_test_cases)
```

### 6.2 è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡

```python
"""
SFTè‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡
"""
import json
from typing import List, Dict

class AutoEvaluator:
    def __init__(self, model_path: str):
        # åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹ï¼ˆå¯ä»¥ç”¨æ›´å¼ºçš„æ¨¡å‹æ¥è¯„ä¼°ï¼‰
        self.judge_model = None  # å¯ä»¥ç”¨GPT-4ä½œä¸ºjudge

    def evaluate_response_quality(self, prompt: str, response: str) -> Dict:
        """
        å¤šç»´åº¦è¯„ä¼°å›å¤è´¨é‡
        """
        scores = {
            "relevance": self._score_relevance(prompt, response),
            "completeness": self._score_completeness(response),
            "professionalism": self._score_professionalism(response),
            "safety": self._score_safety(response),
            "format": self._score_format(response)
        }

        scores["overall"] = sum(scores.values()) / len(scores)
        return scores

    def _score_relevance(self, prompt: str, response: str) -> float:
        """è¯„ä¼°å›å¤ä¸é—®é¢˜çš„ç›¸å…³æ€§"""
        # ç®€å•å®ç°ï¼šæ£€æŸ¥å…³é”®è¯é‡å 
        prompt_words = set(prompt.lower().split())
        response_words = set(response.lower().split())
        overlap = len(prompt_words & response_words)
        return min(1.0, overlap / max(len(prompt_words), 1))

    def _score_completeness(self, response: str) -> float:
        """è¯„ä¼°å›å¤çš„å®Œæ•´æ€§"""
        # åŸºäºé•¿åº¦çš„ç®€å•è¯„ä¼°
        if len(response) < 50:
            return 0.3
        elif len(response) < 100:
            return 0.6
        elif len(response) < 500:
            return 1.0
        else:
            return 0.8  # å¤ªé•¿å¯èƒ½æ‰£åˆ†

    def _score_professionalism(self, response: str) -> float:
        """è¯„ä¼°ä¸“ä¸šæ€§"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸“ä¸šè¡¨è¾¾
        professional_markers = [
            "é¦–å…ˆ", "å…¶æ¬¡", "å¦å¤–", "æ€»ç»“",
            "å»ºè®®", "æ–¹æ¡ˆ", "åˆ†æ", "äº†è§£",
            "ï¼Ÿ", "1.", "2.", "-"  # ç»“æ„åŒ–è¡¨è¾¾
        ]
        score = sum(1 for m in professional_markers if m in response)
        return min(1.0, score / 5)

    def _score_safety(self, response: str) -> float:
        """è¯„ä¼°å®‰å…¨æ€§"""
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å½“å†…å®¹
        unsafe_patterns = [
            "ä¿è¯èµšé’±", "100%æ•ˆæœ", "ç»å¯¹", "è‚¯å®šæ²¡é—®é¢˜",
            "ç«å“å¾ˆå·®", "å…¶ä»–éƒ½æ˜¯åƒåœ¾"
        ]
        for pattern in unsafe_patterns:
            if pattern in response:
                return 0.0
        return 1.0

    def _score_format(self, response: str) -> float:
        """è¯„ä¼°æ ¼å¼è§„èŒƒæ€§"""
        # æ£€æŸ¥æ˜¯å¦æœ‰è‰¯å¥½çš„æ ¼å¼
        format_markers = [
            "\n",     # æ¢è¡Œ
            "ï¼š",     # å†’å·
            "ã€‚",     # å¥å·
            "1.",     # ç¼–å·
            "- ",     # åˆ—è¡¨
        ]
        score = sum(1 for m in format_markers if m in response)
        return min(1.0, score / 3)


def batch_evaluate(evaluator, model, test_data: List[Dict]) -> Dict:
    """æ‰¹é‡è¯„ä¼°"""
    all_scores = []

    for item in test_data:
        response = model.generate(item["prompt"])
        scores = evaluator.evaluate_response_quality(item["prompt"], response)
        all_scores.append(scores)

    # è®¡ç®—å¹³å‡åˆ†
    avg_scores = {}
    for key in all_scores[0].keys():
        avg_scores[key] = sum(s[key] for s in all_scores) / len(all_scores)

    return avg_scores
```

---

## ä¸ƒã€æ€»ç»“ï¼šSFTæœ€ä½³å®è·µæ¸…å•

```markdown
## SFTè®­ç»ƒæ£€æŸ¥æ¸…å•

### æ•°æ®å‡†å¤‡
- [ ] æ•°æ®æ ¼å¼æ­£ç¡®ï¼ˆAlpaca/ShareGPTï¼‰
- [ ] Chat Templateä¸æ¨¡å‹åŒ¹é…
- [ ] æ•°æ®è´¨é‡ç»è¿‡æ£€æŸ¥
- [ ] æ•°æ®é…æ¯”åˆç†ï¼ˆé¢†åŸŸ:é€šç”¨ï¼‰
- [ ] æ•°æ®é‡é€‚ä¸­ï¼ˆ1k-50kä¸ºå®œï¼‰

### è®­ç»ƒé…ç½®
- [ ] å­¦ä¹ ç‡åˆé€‚ï¼ˆLoRA: 5e-5, å…¨å‚: 2e-5ï¼‰
- [ ] LoRAå‚æ•°åˆç†ï¼ˆr=64, Î±=128ï¼‰
- [ ] è®­ç»ƒè½®æ•°é€‚ä¸­ï¼ˆ1-3è½®ï¼‰
- [ ] ä½¿ç”¨BF16ç²¾åº¦
- [ ] æ¢¯åº¦è£å‰ªå¼€å¯

### è®­ç»ƒç›‘æ§
- [ ] ç›‘æ§è®­ç»ƒLossä¸‹é™è¶‹åŠ¿
- [ ] ç›‘æ§éªŒè¯Lossï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰
- [ ] å®šæœŸVibe Checkç”Ÿæˆè´¨é‡
- [ ] ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨

### æ•ˆæœéªŒè¯
- [ ] æŒ‡ä»¤éµå¾ªæµ‹è¯•é€šè¿‡
- [ ] æ ¼å¼è¾“å‡ºæ­£ç¡®
- [ ] é¢†åŸŸçŸ¥è¯†å‡†ç¡®
- [ ] é€šç”¨èƒ½åŠ›ä¿æŒ
```

---

## å‚è€ƒèµ„æº

### å·¥å…·æ¡†æ¶
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - ä¸€ç«™å¼å¾®è°ƒæ¡†æ¶
- [TRL](https://github.com/huggingface/trl) - HuggingFaceå®˜æ–¹
- [Unsloth](https://github.com/unslothai/unsloth) - 2-5å€åŠ é€Ÿ
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) - çµæ´»é…ç½®

### è®ºæ–‡
- [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)
- [QLoRA](https://arxiv.org/abs/2305.14314)

### å®è·µæŒ‡å—
- [Sebastian Raschka - LoRA Tips](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
- [Unsloth Docs](https://docs.unsloth.ai/)

---

> **ä¸‹ä¸€ç« **ï¼š[04-å¥–åŠ±æ¨¡å‹è®­ç»ƒ.md](./04-å¥–åŠ±æ¨¡å‹è®­ç»ƒ.md) - å­¦ä¹ å¦‚ä½•è®­ç»ƒReward Model
