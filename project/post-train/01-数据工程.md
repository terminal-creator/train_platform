# 第一章：数据工程 - 后训练的基石

> **核心观点**：数据质量决定模型上限，数据工程是后训练中最耗时但最关键的环节
>
> **本章目标**：掌握后训练数据的收集、清洗、标注、格式化全流程

---

## 一、数据工程的重要性

### 1.1 业界共识

> "Data cleaning and prep is going to be the hardest part and the most code. No shortcuts."
> — 工业界实践经验

> "样本的精髓在于质量而非数量，少量但精良的样本往往胜过大批中低品质的样本。"
> — Meta LIMA论文

**关键数据**：
- InstructGPT仅用**13k条SFT数据**就取得了突破性效果
- Meta LIMA证明**1000条高质量数据**可以超越50k低质量数据
- 数据质量提升带来的效果 > 10倍数据量增加

### 1.2 数据工程全流程

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         数据工程完整流程                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐          │
│  │ 数据收集  │ ─▶ │ 数据清洗  │ ─▶ │ 数据标注  │ ─▶ │ 格式转换  │          │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘          │
│       │              │               │               │                  │
│       ▼              ▼               ▼               ▼                  │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐          │
│  │ 原始语料  │    │ 去重过滤  │    │ 人工/合成 │    │ 训练格式  │          │
│  │ 爬取采集  │    │ 质量评分  │    │ 质量把控  │    │ 数据验证  │          │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘          │
│                                                                         │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐          │
│  │ 数据增强  │ ─▶ │ 数据混合  │ ─▶ │ 质量评估  │ ─▶ │ 最终数据集│          │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘          │
│       │              │               │               │                  │
│       ▼              ▼               ▼               ▼                  │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐          │
│  │ Self-Inst│    │ 领域+通用 │    │ 自动+人工 │    │ 版本管理  │          │
│  │ Evol-Inst│    │ 配比优化  │    │ 指标监控  │    │ 追溯能力  │          │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘          │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 二、数据收集

### 2.1 销售LLM数据来源

#### 2.1.1 内部数据源

```python
# 销售LLM数据来源规划

data_sources = {
    "历史对话记录": {
        "来源": "CRM系统、在线客服记录、电话录音转写",
        "价值": "真实场景、真实问题",
        "注意": "脱敏处理、隐私合规",
        "预估量": "10万~100万条对话"
    },
    "销售培训材料": {
        "来源": "内部培训PPT、销售手册、FAQ文档",
        "价值": "最佳实践、标准话术",
        "注意": "格式转换、结构化",
        "预估量": "数百份文档"
    },
    "产品文档": {
        "来源": "产品说明书、技术白皮书、案例集",
        "价值": "专业知识、产品细节",
        "注意": "保持更新",
        "预估量": "数GB文本"
    },
    "成交/丢单分析": {
        "来源": "销售复盘报告、客户反馈",
        "价值": "正负样本、关键因素",
        "注意": "标注质量",
        "预估量": "数千份报告"
    }
}
```

#### 2.1.2 外部数据源

```python
external_sources = {
    "公开对话数据集": [
        "ShareGPT (70k对话)",
        "OpenAssistant (66k对话)",
        "Dolly (15k指令)",
        "Alpaca (52k指令)"
    ],
    "领域相关数据": [
        "电商客服对话数据集",
        "Bitext客服数据集 (3.57M tokens)",
        "行业论坛问答"
    ],
    "合成数据": [
        "使用GPT-4生成高质量对话",
        "Self-Instruct方法扩展",
        "Evol-Instruct复杂化"
    ]
}
```

### 2.2 数据收集代码示例

```python
"""
销售对话数据收集Pipeline
"""
import json
import hashlib
from pathlib import Path
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class RawConversation:
    """原始对话数据结构"""
    conversation_id: str
    source: str  # crm, chat, phone
    timestamp: str
    participants: List[str]  # ["sales", "customer"]
    messages: List[Dict]  # [{"role": "sales", "content": "..."}]
    outcome: str  # won, lost, pending
    metadata: Dict  # 产品、金额等

class SalesDataCollector:
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.seen_hashes = set()  # 用于去重

    def collect_from_crm(self, crm_export_path: str) -> List[RawConversation]:
        """从CRM系统导出的数据中收集对话"""
        conversations = []

        with open(crm_export_path, 'r', encoding='utf-8') as f:
            for line in f:
                record = json.loads(line)

                # 数据脱敏
                record = self._anonymize(record)

                # 去重检查
                content_hash = self._compute_hash(record)
                if content_hash in self.seen_hashes:
                    continue
                self.seen_hashes.add(content_hash)

                # 构建对话对象
                conv = RawConversation(
                    conversation_id=record['id'],
                    source='crm',
                    timestamp=record['created_at'],
                    participants=['sales', 'customer'],
                    messages=self._parse_messages(record['messages']),
                    outcome=record.get('deal_status', 'unknown'),
                    metadata={
                        'product': record.get('product_name'),
                        'amount': record.get('deal_amount'),
                        'industry': record.get('customer_industry')
                    }
                )
                conversations.append(conv)

        return conversations

    def _anonymize(self, record: Dict) -> Dict:
        """数据脱敏：移除PII信息"""
        pii_fields = ['customer_name', 'phone', 'email', 'address', 'id_card']
        for field in pii_fields:
            if field in record:
                record[field] = '[REDACTED]'

        # 替换对话中的敏感信息
        if 'messages' in record:
            for msg in record['messages']:
                msg['content'] = self._mask_pii(msg['content'])

        return record

    def _mask_pii(self, text: str) -> str:
        """使用正则表达式掩盖敏感信息"""
        import re

        # 手机号
        text = re.sub(r'1[3-9]\d{9}', '[PHONE]', text)
        # 邮箱
        text = re.sub(r'\S+@\S+\.\S+', '[EMAIL]', text)
        # 身份证
        text = re.sub(r'\d{17}[\dXx]', '[ID]', text)

        return text

    def _compute_hash(self, record: Dict) -> str:
        """计算内容哈希用于去重"""
        content = json.dumps(record.get('messages', []), ensure_ascii=False, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()

    def _parse_messages(self, raw_messages: List) -> List[Dict]:
        """解析原始消息格式"""
        parsed = []
        for msg in raw_messages:
            parsed.append({
                'role': 'sales' if msg.get('is_agent') else 'customer',
                'content': msg['text'].strip(),
                'timestamp': msg.get('time')
            })
        return parsed

    def save_raw_data(self, conversations: List[RawConversation], filename: str):
        """保存原始数据"""
        output_path = self.output_dir / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            for conv in conversations:
                f.write(json.dumps(conv.__dict__, ensure_ascii=False) + '\n')

        print(f"Saved {len(conversations)} conversations to {output_path}")


# 使用示例
if __name__ == "__main__":
    collector = SalesDataCollector("./data/raw")

    # 从不同来源收集数据
    crm_data = collector.collect_from_crm("./exports/crm_conversations.jsonl")

    # 保存原始数据
    collector.save_raw_data(crm_data, "crm_raw.jsonl")
```

---

## 三、数据清洗

### 3.1 清洗流程概览

```
原始数据 → 基础清洗 → 去重 → 质量过滤 → 长度过滤 → 内容过滤 → 清洗后数据
            │          │       │          │          │
            ▼          ▼       ▼          ▼          ▼
         Unicode    Hash/    质量评分    太短/太长  有害内容
         修复       语义      模型打分   过滤       检测
```

### 3.2 数据清洗代码实现

```python
"""
数据清洗Pipeline - 销售LLM专用
"""
import re
import json
from typing import List, Dict, Tuple, Optional
from collections import Counter
import hashlib
from datasketch import MinHash, MinHashLSH

class SalesDataCleaner:
    def __init__(self):
        # 质量阈值配置
        self.config = {
            'min_turns': 2,           # 最少对话轮数
            'max_turns': 50,          # 最多对话轮数
            'min_msg_length': 5,      # 单条消息最短长度
            'max_msg_length': 2000,   # 单条消息最长长度
            'min_total_length': 50,   # 对话总长度最小值
            'max_repetition_ratio': 0.3,  # 最大重复比例
            'jaccard_threshold': 0.8,  # MinHash相似度阈值
        }

        # 初始化MinHash LSH用于去重
        self.lsh = MinHashLSH(threshold=self.config['jaccard_threshold'], num_perm=128)
        self.seen_hashes = {}

    def clean_pipeline(self, conversations: List[Dict]) -> List[Dict]:
        """完整清洗流程"""
        print(f"开始清洗，原始数据量: {len(conversations)}")

        # Step 1: 基础清洗
        conversations = [self._basic_clean(c) for c in conversations]
        print(f"基础清洗后: {len(conversations)}")

        # Step 2: 去除空对话
        conversations = [c for c in conversations if c is not None]
        print(f"去除空对话后: {len(conversations)}")

        # Step 3: 去重
        conversations = self._deduplicate(conversations)
        print(f"去重后: {len(conversations)}")

        # Step 4: 质量过滤
        conversations = [c for c in conversations if self._quality_check(c)]
        print(f"质量过滤后: {len(conversations)}")

        # Step 5: 内容过滤
        conversations = [c for c in conversations if self._content_filter(c)]
        print(f"内容过滤后: {len(conversations)}")

        return conversations

    def _basic_clean(self, conv: Dict) -> Optional[Dict]:
        """基础清洗：修复Unicode、去除特殊字符"""
        try:
            messages = conv.get('messages', [])
            cleaned_messages = []

            for msg in messages:
                content = msg.get('content', '')

                # Unicode标准化
                import unicodedata
                content = unicodedata.normalize('NFKC', content)

                # 去除控制字符
                content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', content)

                # 合并连续空白
                content = re.sub(r'\s+', ' ', content)

                # 去除首尾空白
                content = content.strip()

                if content:  # 只保留非空消息
                    cleaned_messages.append({
                        'role': msg['role'],
                        'content': content
                    })

            if len(cleaned_messages) >= self.config['min_turns']:
                conv['messages'] = cleaned_messages
                return conv
            return None

        except Exception as e:
            print(f"清洗错误: {e}")
            return None

    def _deduplicate(self, conversations: List[Dict]) -> List[Dict]:
        """
        使用MinHash LSH进行近似去重

        ⚠️ 中文去重注意事项：
        - 中文不能用 text.split() 分词，因为中文词之间没有空格
        - 必须使用：字符级n-gram / 中文分词器 / embedding近邻
        """
        unique_conversations = []

        for conv in conversations:
            # 将对话转为文本
            text = ' '.join([m['content'] for m in conv['messages']])

            # 计算MinHash
            minhash = MinHash(num_perm=128)

            # ✅ 正确做法：使用字符级n-gram（适用于中英文混合）
            # 而不是 text.split()（对中文无效！）
            ngrams = self._get_char_ngrams(text, n=3)
            for ngram in ngrams:
                minhash.update(ngram.encode('utf-8'))

            # 查询是否存在近似重复
            conv_id = conv.get('conversation_id', str(len(unique_conversations)))

            result = self.lsh.query(minhash)
            if not result:  # 没有找到相似的
                self.lsh.insert(conv_id, minhash)
                unique_conversations.append(conv)

        return unique_conversations

    def _get_char_ngrams(self, text: str, n: int = 3) -> List[str]:
        """
        获取字符级n-gram（适用于中英文混合文本）

        为什么用字符n-gram而不是词级：
        - 中文分词器（jieba等）有性能开销且可能切分错误
        - 字符n-gram对中英文都有效
        - 对于去重任务，字符n-gram足够捕捉相似性
        """
        # 移除空白字符，保留有意义的内容
        text = ''.join(text.split())
        if len(text) < n:
            return [text] if text else []

        return [text[i:i+n] for i in range(len(text) - n + 1)]

    def _quality_check(self, conv: Dict) -> bool:
        """质量检查"""
        messages = conv.get('messages', [])

        # 检查轮数
        if not (self.config['min_turns'] <= len(messages) <= self.config['max_turns']):
            return False

        # 检查每条消息长度
        for msg in messages:
            content = msg['content']
            if not (self.config['min_msg_length'] <= len(content) <= self.config['max_msg_length']):
                return False

        # 检查总长度
        total_length = sum(len(m['content']) for m in messages)
        if total_length < self.config['min_total_length']:
            return False

        # 检查重复率
        all_text = ' '.join([m['content'] for m in messages])
        if self._check_repetition(all_text):
            return False

        return True

    def _check_repetition(self, text: str) -> bool:
        """
        检查文本重复率

        ⚠️ 使用字符n-gram而非word n-gram，以正确处理中文
        """
        # 移除空白
        text = ''.join(text.split())
        if len(text) < 30:  # 太短不检测
            return False

        # 使用字符级5-gram检测重复（中英文通用）
        n = 5
        ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]
        ngram_counts = Counter(ngrams)

        # 计算重复比例
        repeated = sum(c - 1 for c in ngram_counts.values() if c > 1)
        repetition_ratio = repeated / len(ngrams) if ngrams else 0

        return repetition_ratio > self.config['max_repetition_ratio']

    def _content_filter(self, conv: Dict) -> bool:
        """内容过滤：检测有害内容"""
        messages = conv.get('messages', [])
        all_text = ' '.join([m['content'] for m in messages])

        # 敏感词检测（示例）
        sensitive_patterns = [
            r'竞争对手\w*垃圾',  # 不当竞品评价
            r'保证\w*赚钱',      # 虚假承诺
            r'100%\w*效果',      # 夸大宣传
        ]

        for pattern in sensitive_patterns:
            if re.search(pattern, all_text):
                return False

        return True

    def compute_quality_score(self, conv: Dict) -> float:
        """计算对话质量分数 (0-1)"""
        scores = []
        messages = conv.get('messages', [])

        # 1. 对话完整性 (有开头有结尾)
        if len(messages) >= 3:
            scores.append(1.0)
        else:
            scores.append(0.5)

        # 2. 销售回复质量
        sales_msgs = [m for m in messages if m['role'] == 'sales']
        if sales_msgs:
            avg_sales_len = sum(len(m['content']) for m in sales_msgs) / len(sales_msgs)
            # 50-500字符为理想长度
            if 50 <= avg_sales_len <= 500:
                scores.append(1.0)
            elif avg_sales_len < 50:
                scores.append(0.5)
            else:
                scores.append(0.7)

        # 3. 对话交互性 (双方都有多次发言)
        customer_msgs = [m for m in messages if m['role'] == 'customer']
        if len(sales_msgs) >= 2 and len(customer_msgs) >= 2:
            scores.append(1.0)
        else:
            scores.append(0.6)

        # 4. 结果导向 (成交对话加分)
        if conv.get('outcome') == 'won':
            scores.append(1.0)
        elif conv.get('outcome') == 'lost':
            scores.append(0.7)  # 失败案例也有学习价值
        else:
            scores.append(0.5)

        return sum(scores) / len(scores)


# 使用示例
if __name__ == "__main__":
    cleaner = SalesDataCleaner()

    # 加载原始数据
    with open('./data/raw/crm_raw.jsonl', 'r') as f:
        raw_data = [json.loads(line) for line in f]

    # 执行清洗
    cleaned_data = cleaner.clean_pipeline(raw_data)

    # 计算质量分数并排序
    for conv in cleaned_data:
        conv['quality_score'] = cleaner.compute_quality_score(conv)

    cleaned_data.sort(key=lambda x: x['quality_score'], reverse=True)

    # 保存清洗后的数据
    with open('./data/cleaned/sales_cleaned.jsonl', 'w') as f:
        for conv in cleaned_data:
            f.write(json.dumps(conv, ensure_ascii=False) + '\n')

    print(f"清洗完成，最终数据量: {len(cleaned_data)}")
    print(f"平均质量分数: {sum(c['quality_score'] for c in cleaned_data) / len(cleaned_data):.2f}")
```

### 3.3 质量评分进阶：使用LLM打分

```python
"""
使用LLM进行数据质量评分
参考NVIDIA Nemotron-4 340B的5维度评分方法
"""
from openai import OpenAI
import json

class LLMQualityScorer:
    def __init__(self):
        self.client = OpenAI()

        self.scoring_prompt = """你是一个专业的销售对话质量评估专家。
请对以下销售对话进行评分，从以下5个维度打分（每个维度1-5分）：

1. **帮助性 (Helpfulness)**: 销售回复是否有效解答客户问题？
2. **正确性 (Correctness)**: 信息是否准确无误？
3. **连贯性 (Coherence)**: 对话是否流畅自然？
4. **专业性 (Complexity)**: 回复是否体现专业知识？
5. **简洁性 (Verbosity)**: 表达是否清晰简洁（非冗余）？

【对话内容】
{conversation}

请以JSON格式返回评分结果：
{{
    "helpfulness": <1-5>,
    "correctness": <1-5>,
    "coherence": <1-5>,
    "complexity": <1-5>,
    "verbosity": <1-5>,
    "overall": <1-5>,
    "reason": "<简要评价>"
}}
"""

    def score_conversation(self, conv: Dict) -> Dict:
        """使用LLM对单个对话评分"""
        # 格式化对话
        conv_text = self._format_conversation(conv)

        # 调用LLM
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",  # 或使用更便宜的模型
            messages=[
                {"role": "user", "content": self.scoring_prompt.format(conversation=conv_text)}
            ],
            response_format={"type": "json_object"},
            temperature=0.1
        )

        try:
            scores = json.loads(response.choices[0].message.content)
            return scores
        except:
            return {"overall": 3, "reason": "评分失败"}

    def _format_conversation(self, conv: Dict) -> str:
        """格式化对话为可读文本"""
        lines = []
        for msg in conv.get('messages', []):
            role = "销售" if msg['role'] == 'sales' else "客户"
            lines.append(f"{role}: {msg['content']}")
        return '\n'.join(lines)

    def batch_score(self, conversations: List[Dict], sample_size: int = 100) -> List[Dict]:
        """批量评分（采样）"""
        import random

        # 如果数据量大，随机采样
        if len(conversations) > sample_size:
            sampled = random.sample(conversations, sample_size)
        else:
            sampled = conversations

        scored = []
        for i, conv in enumerate(sampled):
            print(f"评分进度: {i+1}/{len(sampled)}")
            scores = self.score_conversation(conv)
            conv['llm_scores'] = scores
            scored.append(conv)

        return scored


# 质量过滤示例
def filter_by_quality(conversations: List[Dict],
                      min_overall_score: float = 3.5) -> List[Dict]:
    """根据LLM评分过滤低质量数据"""
    return [c for c in conversations
            if c.get('llm_scores', {}).get('overall', 0) >= min_overall_score]
```

---

## 四、数据格式与转换

### 4.1 Alpaca格式（单轮指令）

```json
{
    "instruction": "作为销售顾问，请回答客户关于产品退货的问题。",
    "input": "如果我买了不满意可以退吗？",
    "output": "当然可以！我们提供7天无理由退货服务。只要商品保持完好，包装完整，您可以在收货后7天内申请退货。退货流程也很简单：\n\n1. 在APP中提交退货申请\n2. 我们会在24小时内审核通过\n3. 预约快递上门取件\n4. 商品签收后3个工作日内退款到账\n\n请问您是想了解退货的具体原因要求，还是有其他问题？"
}
```

### 4.2 ShareGPT格式（多轮对话）

```json
{
    "conversations": [
        {
            "from": "system",
            "value": "你是一个专业的销售顾问，擅长了解客户需求并提供合适的解决方案。保持友好、专业的态度。"
        },
        {
            "from": "human",
            "value": "你们这个软件多少钱？"
        },
        {
            "from": "gpt",
            "value": "感谢您的咨询！我们的价格方案是根据您的实际需求来定制的，这样可以确保您只为真正需要的功能付费。\n\n方便问几个问题吗？\n1. 您的团队大概有多少人会使用？\n2. 您主要想解决什么问题？\n3. 您期望的预算范围是多少？\n\n了解这些后，我可以为您推荐最适合的方案。"
        },
        {
            "from": "human",
            "value": "大概20个人用，主要是想做客户管理，预算在5万以内"
        },
        {
            "from": "gpt",
            "value": "明白了！20人团队做客户管理，5万预算，我推荐我们的「专业版」，非常适合您：\n\n**专业版方案** (¥3,800/月，年付可享8折)\n- ✅ 支持30个用户账号\n- ✅ 完整的客户管理功能\n- ✅ 销售漏斗分析\n- ✅ 自动化提醒\n- ✅ 7×24小时客服支持\n\n年费算下来是36,480元，在您的预算范围内，还能留出余地做一些定制化配置。\n\n您看要不要我安排一个15分钟的演示，让您直观感受下产品？"
        }
    ]
}
```

### 4.3 偏好数据格式（DPO/RLHF）

```json
{
    "prompt": "客户说'太贵了'，作为销售你会怎么回应？",
    "chosen": "我理解您对价格的考虑，这很正常。不过让我帮您算一笔账：\n\n我们的方案虽然一次性投入是15万，但：\n1. 使用寿命5年以上，平均每年只要3万\n2. 能帮您节省2个人工成本，一年就是20万\n3. 效率提升带来的隐性收益更是难以估量\n\n其实很多客户最开始也觉得贵，但用了之后都说'真值'。\n\n这样，我申请一下，看能不能给您争取一个首单优惠？",
    "rejected": "我们的价格已经很合理了，比其他家都便宜。您要不就别买了。"
}
```

### 4.4 格式转换代码

```python
"""
数据格式转换工具
支持：原始对话 → Alpaca / ShareGPT / Preference 格式
"""
from typing import List, Dict
import json

class DataFormatConverter:
    def __init__(self, system_prompt: str = None):
        self.system_prompt = system_prompt or "你是一个专业的销售顾问，擅长理解客户需求并提供专业建议。"

    def to_alpaca(self, conv: Dict) -> List[Dict]:
        """
        将多轮对话转换为Alpaca格式
        每个customer-sales对生成一条训练数据
        """
        alpaca_data = []
        messages = conv.get('messages', [])

        # 构建对话上下文
        context = []
        for i, msg in enumerate(messages):
            if msg['role'] == 'customer':
                # 找到对应的sales回复
                if i + 1 < len(messages) and messages[i+1]['role'] == 'sales':
                    # 构建instruction
                    if context:
                        instruction = f"【对话历史】\n{self._format_context(context)}\n\n【当前客户问题】请回答以下客户问题："
                    else:
                        instruction = "作为销售顾问，请回答以下客户问题："

                    alpaca_item = {
                        "instruction": instruction,
                        "input": msg['content'],
                        "output": messages[i+1]['content'],
                        "system": self.system_prompt
                    }
                    alpaca_data.append(alpaca_item)

            # 更新上下文
            context.append(msg)

        return alpaca_data

    def to_sharegpt(self, conv: Dict) -> Dict:
        """将对话转换为ShareGPT格式"""
        conversations = []

        # 添加system prompt
        conversations.append({
            "from": "system",
            "value": self.system_prompt
        })

        # 转换消息
        for msg in conv.get('messages', []):
            role = "human" if msg['role'] == 'customer' else "gpt"
            conversations.append({
                "from": role,
                "value": msg['content']
            })

        return {"conversations": conversations}

    def to_chatml(self, conv: Dict) -> str:
        """转换为ChatML格式的字符串"""
        chatml = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n"

        for msg in conv.get('messages', []):
            role = "user" if msg['role'] == 'customer' else "assistant"
            chatml += f"<|im_start|>{role}\n{msg['content']}<|im_end|>\n"

        return chatml

    def to_preference(self, conv: Dict, generate_rejected: callable = None) -> Dict:
        """
        转换为偏好数据格式
        需要提供生成rejected回复的函数（或已有rejected数据）
        """
        messages = conv.get('messages', [])
        preference_data = []

        context = []
        for i, msg in enumerate(messages):
            if msg['role'] == 'customer' and i + 1 < len(messages):
                sales_response = messages[i+1]
                if sales_response['role'] == 'sales':
                    # 构建prompt
                    if context:
                        prompt = f"【对话历史】\n{self._format_context(context)}\n\n【客户】{msg['content']}\n\n请作为销售回复："
                    else:
                        prompt = f"【客户】{msg['content']}\n\n请作为销售回复："

                    # chosen是实际的优质回复
                    chosen = sales_response['content']

                    # 生成rejected（低质量回复）
                    if generate_rejected:
                        rejected = generate_rejected(prompt)
                    else:
                        # 简单的rejected生成策略
                        rejected = self._generate_simple_rejected(chosen)

                    preference_data.append({
                        "prompt": prompt,
                        "chosen": chosen,
                        "rejected": rejected
                    })

            context.append(msg)

        return preference_data

    def _format_context(self, context: List[Dict]) -> str:
        """格式化对话上下文"""
        lines = []
        for msg in context[-4:]:  # 只保留最近4轮
            role = "客户" if msg['role'] == 'customer' else "销售"
            lines.append(f"【{role}】{msg['content']}")
        return '\n'.join(lines)

    def _generate_simple_rejected(self, chosen: str) -> str:
        """简单的rejected生成策略（实际应用中应使用更复杂的方法）"""
        # 策略1: 过于简短
        if len(chosen) > 100:
            return chosen[:50] + "..."

        # 策略2: 生硬拒绝
        return "不好意思，这个我不太清楚，您可以自己查一下。"


# 批量转换示例
def batch_convert(input_file: str, output_dir: str, format_type: str = 'sharegpt'):
    """批量转换数据格式"""
    converter = DataFormatConverter()

    # 读取清洗后的数据
    with open(input_file, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]

    # 转换
    converted = []
    for conv in data:
        if format_type == 'alpaca':
            converted.extend(converter.to_alpaca(conv))
        elif format_type == 'sharegpt':
            converted.append(converter.to_sharegpt(conv))
        elif format_type == 'preference':
            converted.extend(converter.to_preference(conv))

    # 保存
    output_file = f"{output_dir}/sales_{format_type}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(converted, f, ensure_ascii=False, indent=2)

    print(f"转换完成: {len(converted)} 条数据 -> {output_file}")
    return converted


# 使用示例
if __name__ == "__main__":
    # 转换为不同格式
    batch_convert('./data/cleaned/sales_cleaned.jsonl', './data/formatted', 'sharegpt')
    batch_convert('./data/cleaned/sales_cleaned.jsonl', './data/formatted', 'alpaca')
```

---

## 五、数据增强与合成

### 5.1 Self-Instruct方法

```python
"""
Self-Instruct: 使用LLM生成更多训练数据
"""
from openai import OpenAI
import json
import random

class SelfInstructGenerator:
    def __init__(self):
        self.client = OpenAI()

        # 种子数据（高质量示例）
        self.seed_examples = [
            {
                "instruction": "客户表示产品太贵",
                "response": "理解您的顾虑..."
            },
            # 添加更多种子...
        ]

    def generate_new_instructions(self, num_samples: int = 100) -> List[Dict]:
        """生成新的instruction"""
        prompt = """你是一个销售培训专家。请生成5个销售场景中常见的客户问题或情况。

参考示例：
{examples}

要求：
1. 场景要多样化（产品咨询、价格谈判、售后问题等）
2. 要贴近真实销售场景
3. 难度适中

请以JSON数组格式返回：
[
    {{"scenario": "场景描述", "customer_question": "客户的具体问题"}},
    ...
]
"""

        generated = []
        batch_size = 5

        for i in range(num_samples // batch_size):
            # 随机选择几个示例
            examples = random.sample(self.seed_examples, min(3, len(self.seed_examples)))
            examples_text = json.dumps(examples, ensure_ascii=False, indent=2)

            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": prompt.format(examples=examples_text)}
                ],
                response_format={"type": "json_object"},
                temperature=0.9  # 高温度增加多样性
            )

            try:
                new_instructions = json.loads(response.choices[0].message.content)
                generated.extend(new_instructions)
            except:
                continue

        return generated

    def generate_responses(self, instructions: List[Dict]) -> List[Dict]:
        """为生成的instruction生成高质量response"""
        prompt = """你是一个顶级销售顾问。请针对以下客户场景，给出专业、有效的销售回复。

场景：{scenario}
客户问题：{question}

要求：
1. 回复要专业、友好
2. 要有解决问题的具体方案
3. 要引导客户继续对话
4. 长度适中（100-300字）

请直接给出销售回复："""

        results = []
        for inst in instructions:
            response = self.client.chat.completions.create(
                model="gpt-4o",  # 使用更强的模型生成response
                messages=[
                    {"role": "user", "content": prompt.format(
                        scenario=inst['scenario'],
                        question=inst['customer_question']
                    )}
                ],
                temperature=0.7
            )

            results.append({
                "instruction": f"场景：{inst['scenario']}",
                "input": inst['customer_question'],
                "output": response.choices[0].message.content
            })

        return results
```

### 5.2 Evol-Instruct方法

```python
"""
Evol-Instruct: 进化式数据增强
通过增加复杂度和深度来提升数据质量
"""

class EvolInstructGenerator:
    def __init__(self):
        self.client = OpenAI()

        # 进化策略
        self.evolution_strategies = {
            "add_constraints": """请将以下销售场景变得更复杂，添加额外的约束条件：
原始场景：{original}
可添加的约束：预算限制、时间压力、竞品比较、多决策人等
返回进化后的场景：""",

            "increase_depth": """请将以下销售问题深化，使其需要更专业的销售技巧来回答：
原始问题：{original}
可深化的方向：涉及更多技术细节、更复杂的需求分析、更深的价值挖掘等
返回深化后的问题：""",

            "add_reasoning": """请将以下销售回复改写，加入更多推理过程和逻辑分析：
原始回复：{original}
要求：展示销售顾问的思考过程，分析客户需求，给出有理有据的建议
返回改进后的回复：""",

            "multi_turn": """请将以下单轮对话扩展为3-5轮的完整销售对话：
原始对话：
客户：{customer}
销售：{sales}
要求：对话要自然流畅，展示销售技巧的运用
返回完整对话（JSON格式）："""
        }

    def evolve_data(self, data: List[Dict], strategy: str, num_iterations: int = 1) -> List[Dict]:
        """执行数据进化"""
        evolved_data = []
        prompt_template = self.evolution_strategies.get(strategy)

        if not prompt_template:
            raise ValueError(f"Unknown strategy: {strategy}")

        for item in data:
            current = item

            for _ in range(num_iterations):
                if strategy == "multi_turn":
                    prompt = prompt_template.format(
                        customer=current.get('input', ''),
                        sales=current.get('output', '')
                    )
                else:
                    prompt = prompt_template.format(
                        original=json.dumps(current, ensure_ascii=False)
                    )

                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.8
                )

                evolved_text = response.choices[0].message.content

                # 解析进化结果
                if strategy == "multi_turn":
                    try:
                        current = json.loads(evolved_text)
                    except:
                        current['evolved_conversation'] = evolved_text
                else:
                    current = {**current, 'evolved': evolved_text}

            evolved_data.append(current)

        return evolved_data
```

---

## 六、数据配比策略

### 6.1 配比原则

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         数据配比最佳实践                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  场景一：领域模型 (SalesGPT)                                            │
│  ├── 领域数据 : 通用数据 = 7:3 或 4:1                                   │
│  ├── 高质量 : 普通质量 = 1:4 (高质量数据重复采样)                        │
│  └── 成交对话 : 未成交对话 = 6:4                                        │
│                                                                         │
│  场景二：通用Chat模型                                                    │
│  ├── 各领域均匀分布                                                     │
│  ├── Temperature-scaled mixing (T=2) 效果最好                           │
│  └── 避免Equal mixing (效果最差)                                        │
│                                                                         │
│  场景三：数据量有限 (<10k)                                               │
│  ├── 全部使用高质量数据                                                 │
│  ├── 配合数据增强扩充                                                   │
│  └── 使用Chat模型作为基座 (而非Base模型)                                │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 6.2 销售LLM数据配比示例

```python
"""
销售LLM数据混合配比
"""
import random
from typing import List, Dict

class DataMixer:
    def __init__(self):
        self.config = {
            # 数据来源配比
            "source_ratio": {
                "real_conversations": 0.6,    # 真实对话
                "synthetic_data": 0.25,       # 合成数据
                "general_data": 0.15          # 通用对话数据
            },

            # 场景配比
            "scenario_ratio": {
                "product_inquiry": 0.25,      # 产品咨询
                "price_negotiation": 0.20,    # 价格谈判
                "objection_handling": 0.20,   # 异议处理
                "closing": 0.15,              # 成交促单
                "after_sales": 0.10,          # 售后服务
                "general": 0.10               # 通用对话
            },

            # 质量配比 (高质量数据过采样)
            "quality_sampling": {
                "high": 3,     # 高质量数据采样3次
                "medium": 1,   # 中等质量采样1次
                "low": 0       # 低质量数据不使用
            }
        }

    def mix_datasets(self,
                     real_data: List[Dict],
                     synthetic_data: List[Dict],
                     general_data: List[Dict],
                     target_size: int) -> List[Dict]:
        """
        按配比混合多个数据集
        """
        mixed = []

        # 计算各来源的目标数量
        real_target = int(target_size * self.config['source_ratio']['real_conversations'])
        synthetic_target = int(target_size * self.config['source_ratio']['synthetic_data'])
        general_target = int(target_size * self.config['source_ratio']['general_data'])

        # 按质量过采样
        real_sampled = self._quality_based_sampling(real_data, real_target)
        synthetic_sampled = random.sample(synthetic_data, min(synthetic_target, len(synthetic_data)))
        general_sampled = random.sample(general_data, min(general_target, len(general_data)))

        # 合并
        mixed = real_sampled + synthetic_sampled + general_sampled

        # 打乱顺序
        random.shuffle(mixed)

        return mixed

    def _quality_based_sampling(self, data: List[Dict], target_size: int) -> List[Dict]:
        """基于质量的采样"""
        # 按质量分组
        high_quality = [d for d in data if d.get('quality_score', 0) >= 0.8]
        medium_quality = [d for d in data if 0.5 <= d.get('quality_score', 0) < 0.8]

        sampled = []

        # 高质量数据过采样
        for _ in range(self.config['quality_sampling']['high']):
            sampled.extend(high_quality)

        # 中等质量数据
        sampled.extend(medium_quality)

        # 如果不够，随机采样补充
        if len(sampled) < target_size:
            additional = random.choices(high_quality + medium_quality,
                                       k=target_size - len(sampled))
            sampled.extend(additional)
        elif len(sampled) > target_size:
            sampled = random.sample(sampled, target_size)

        return sampled

    def balance_by_scenario(self, data: List[Dict]) -> List[Dict]:
        """按场景类型平衡数据"""
        # 按场景分组
        by_scenario = {}
        for item in data:
            scenario = item.get('scenario', 'general')
            if scenario not in by_scenario:
                by_scenario[scenario] = []
            by_scenario[scenario].append(item)

        # 按配比采样
        balanced = []
        total_target = len(data)

        for scenario, ratio in self.config['scenario_ratio'].items():
            target_count = int(total_target * ratio)
            if scenario in by_scenario:
                sampled = random.choices(by_scenario[scenario],
                                        k=min(target_count, len(by_scenario[scenario]) * 2))
                balanced.extend(sampled[:target_count])

        random.shuffle(balanced)
        return balanced


# 使用示例
if __name__ == "__main__":
    mixer = DataMixer()

    # 加载各类数据
    with open('./data/formatted/real_conversations.json') as f:
        real_data = json.load(f)
    with open('./data/formatted/synthetic_data.json') as f:
        synthetic_data = json.load(f)
    with open('./data/formatted/general_chat.json') as f:
        general_data = json.load(f)

    # 混合数据
    mixed_data = mixer.mix_datasets(
        real_data=real_data,
        synthetic_data=synthetic_data,
        general_data=general_data,
        target_size=10000
    )

    # 场景平衡
    balanced_data = mixer.balance_by_scenario(mixed_data)

    # 保存最终数据集
    with open('./data/final/sales_sft_train.json', 'w') as f:
        json.dump(balanced_data, f, ensure_ascii=False, indent=2)

    print(f"最终数据集大小: {len(balanced_data)}")
```

---

## 七、数据质量检查清单

### 7.1 上线前必检项

```markdown
## 数据质量检查清单

### 基础检查
- [ ] 数据去重完成，无完全重复样本
- [ ] 近似重复检测完成（Jaccard相似度 < 0.8）
- [ ] 所有字段非空且格式正确
- [ ] 编码统一为UTF-8

### 内容检查
- [ ] 无PII信息（姓名、电话、邮箱、身份证等）
- [ ] 无敏感词和有害内容
- [ ] 无竞品不当评价
- [ ] 无虚假承诺和夸大宣传

### 质量检查
- [ ] 平均质量分数 > 0.7
- [ ] 低质量样本（<0.5分）占比 < 5%
- [ ] 人工抽检合格率 > 95%

### 分布检查
- [ ] 场景分布符合预期配比
- [ ] 对话长度分布合理
- [ ] 领域/通用数据配比正确

### 格式检查
- [ ] 符合目标格式要求（Alpaca/ShareGPT）
- [ ] Chat Template正确
- [ ] 特殊token使用正确
```

### 7.2 数据文档模板

```markdown
# 数据集文档

## 基本信息
- 数据集名称：SalesGPT-SFT-v1.0
- 创建日期：2024-xx-xx
- 数据量：10,000条
- 格式：ShareGPT

## 数据来源
| 来源 | 占比 | 数量 |
|------|------|------|
| CRM真实对话 | 60% | 6,000 |
| 合成数据 | 25% | 2,500 |
| 通用对话 | 15% | 1,500 |

## 质量统计
- 平均质量分数：0.78
- 高质量（>0.8）占比：35%
- 中等质量（0.5-0.8）占比：60%
- 低质量（<0.5）占比：5%

## 处理流程
1. 原始数据收集：50,000条
2. 去重后：35,000条
3. 质量过滤后：15,000条
4. 最终采样：10,000条

## 使用建议
- 推荐配合Qwen2.5-7B-Instruct基座使用
- SFT训练建议epoch：2-3
- 建议学习率：5e-5 (LoRA)

## 已知问题
- 部分售后场景数据偏少，后续版本补充
- 价格相关对话可能需要根据实际产品更新
```

---

## 八、常见问题与解决方案

### Q1: 数据量不够怎么办？

**方案**：
1. 使用Self-Instruct/Evol-Instruct生成合成数据
2. 从通用数据集（Alpaca、ShareGPT）中筛选相关数据
3. 使用更大的预训练模型生成高质量回复
4. 降低数据质量要求，但增加数据增强

### Q2: 如何处理多语言数据？

**方案**：
1. 统一做语言检测，标记语言类型
2. 按语言分开处理，最后按比例混合
3. 中文为主场景可丢弃其他语言
4. 使用翻译增强多语言覆盖

### Q3: 如何保证标注一致性？

**方案**：
1. 制定详细的标注指南
2. 标注员培训和校准
3. 多人标注取共识
4. 定期抽检和反馈

### Q4: 数据更新策略？

**方案**：
1. 建立数据版本管理
2. 增量数据定期合并
3. 建立数据质量监控
4. 模型性能下降时触发数据更新

---

## 参考资源

### 开源数据集
- [ShareGPT](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)
- [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1)
- [Bitext Customer Support](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset)

### 数据处理工具
- [datasketch](https://github.com/ekzhu/datasketch) - MinHash去重
- [cleanlab](https://github.com/cleanlab/cleanlab) - 数据质量检测
- [argilla](https://github.com/argilla-io/argilla) - 数据标注平台

### 参考论文
- [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)
- [Self-Instruct](https://arxiv.org/abs/2212.10560)
- [WizardLM: Evol-Instruct](https://arxiv.org/abs/2304.12244)

---

> **下一章**：[02-继续预训练CPT.md](./02-继续预训练CPT.md) - 学习如何注入领域知识
