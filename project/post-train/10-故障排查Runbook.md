# 第十章：故障排查 Runbook（后训练全链路）

> **目标**：把后训练里最常见、最致命的坑，统一收敛成一张"**现象 → 根因 → 排查 → 修复 → 验证**"表，方便你在出问题时快速定位与闭环。
>
> **使用方式**：先用"现象"定位最像的条目 → 按"排查步骤"从上到下做最小复现/最小证据 → 优先用"修复动作"里的低成本手段止血 → 按"验证标准"确认真正修好（而不是"感觉好了"）。

---

## 快速分级（建议）

- **S0（致命）**：训练崩溃 / NaN / 死锁 / 严重安全问题 / 数据泄漏（隐私）/ 线上事故。
- **S1（严重）**：质量大幅下降、对齐跑偏、线上离线严重不一致。
- **S2（中等）**：效果提升停滞、部分能力退化、长尾问题增多。
- **S3（性能/成本）**：吞吐、延迟、显存、成本不达标。

> 表格里我用 `[阶段][Sx]` 标注归属阶段与严重级别；同一现象可能跨多个阶段（以最常见根因为主）。

---

## 故障排查总表（现象 → 根因 → 排查 → 修复 → 验证）

### 一、数据阶段问题

| 现象 | 可能根因 | 排查步骤 | 修复动作 | 验证标准 |
|------|----------|----------|----------|----------|
| `[数据][S0]` 训练启动就报错/样本被过滤成空 | 数据schema不匹配；字段名/role tag错；json/jsonl损坏；编码问题 | 1) 抽20条样本肉眼检查字段与role值<br>2) 跑"数据加载→模板→tokenize→collator"最小链路并打印首条样本<br>3) 统计过滤原因 | 统一schema（显式写data contract）；加数据校验脚本；修复编码（UTF-8） | 数据加载后样本数符合预期；过滤率可解释；可生成首个batch |
| `[数据][S1]` 离线评测很高，线上效果差 | train↔eval泄漏/近重复；评测prompt与训练高度重合；LLM-as-Judge偏差 | 1) 做train↔eval近重复检测<br>2) 抽样人工核对评测集<br>3) 用另一套评测集复测 | 清理泄漏并冻结评测集版本；增加回归红线集（不可训练）；引入OOD评测 | 泄漏率<0.1%；不同judge/prompt下结论一致；线上核心指标与离线趋势一致 |
| `[数据][S1]` 中文去重几乎无效 | `split()`对中文不切词；MinHash特征构建不合理；阈值未校准 | 1) 统计完全重复vs近重复<br>2) 检查去重特征：是否是中文字符n-gram<br>3) 画相似度分布校准阈值 | 中文用字符3~5-gram或分词后n-gram；embedding近邻检索+阈值 | 近重复占比<1~3%；抽检误杀率可接受；模型不再复读训练样本 |
| `[数据][S0]` 数据中出现PII/敏感信息 | 脱敏漏做/规则不全；日志回流直接入库 | 1) 全量跑PII detector<br>2) 抽样人工复核<br>3) 追溯来源 | 增加脱敏与阻断（数据落地前）；补齐正则与ML检测；历史数据回溯清理 | PII命中数=0；抽检0漏检；线上审计无新增 |
| `[数据][S1]` 合成数据占比高后风格单一/幻觉上升 | 合成数据distribution偏；缺少真实失败样本；合成prompt模板单一 | 1) 统计真实vs合成占比<br>2) 抽样对比语言特征<br>3) 看幻觉在合成样本中是否更高 | 降低合成占比；提升真实数据权重；多模板合成；加入hard negative | 语言多样性上升；事实错误率下降；人工抽检"像模型"比例下降 |
| `[数据][S1]` 领域能力提升但通用能力下降 | 数据配比过激；训练过久；学习率过大 | 1) 用通用回归集测试<br>2) 看领域:通用配比<br>3) 检查是否只喂领域数据 | 混入通用replay数据；降低训练强度；用PEFT/冻结底层 | 通用回归集跌幅<5~10%；领域提升保留 |

---

### 二、SFT阶段问题

| 现象 | 可能根因 | 排查步骤 | 修复动作 | 验证标准 |
|------|----------|----------|----------|----------|
| `[SFT][S0]` Loss Masking错误：模型输出instruction | 用`text.split()`估算mask位置（多轮必错）；未考虑chat template特殊token | 1) 打印token-label对齐<br>2) 验证assistant回复token是否为TRAIN<br>3) 验证user/system是否为skip | 使用TRL的DataCollatorForCompletionOnlyLM；或基于模板marker精确定位 | 所有assistant token为TRAIN；所有user/system为skip；特殊token为skip |
| `[SFT][S1]` Chat Template不匹配：输出格式异常/不停止 | 训练template与模型原生不一致；eos_token设置错误；pad_token设为eos | 1) 打印tokenizer.chat_template<br>2) 检查eos/pad token<br>3) 对比训练和推理格式 | 使用模型原生template；正确设置pad_token（用unk_token，不用eos） | 推理输出格式正确；能正常停止；无乱码 |
| `[SFT][S1]` 灾难性遗忘：通用能力骤降 | 纯领域数据训练；学习率过高；epoch过多 | 1) 用标准benchmark评估<br>2) 人工检查核心能力<br>3) 对比训练前后 | 领域:通用配比70:30或80:20；降低学习率到1e-5；使用LoRA | 通用benchmark跌幅<5%；核心能力保留 |
| `[SFT][S2]` 过度知识注入：指令遵循能力下降 | SFT数据包含大量纯知识内容；数据格式像"问答"而非"对话" | 1) 检查数据是否对话式<br>2) 测试指令遵循能力 | 知识注入在CPT阶段完成；SFT数据保持对话式；使用RAG补充知识 | 指令遵循测试通过；回复风格正常 |
| `[SFT][S0]` Loss NaN/训练崩溃 | 学习率过大；数据有异常（超长/乱码）；BF16溢出 | 1) 检查崩溃batch的数据<br>2) 检查Grad Norm<br>3) 检查学习率 | 降低学习率；过滤异常数据；开启float32优化器累积 | 训练稳定运行；Loss平滑下降 |

---

### 三、RM/偏好数据问题

| 现象 | 可能根因 | 排查步骤 | 修复动作 | 验证标准 |
|------|----------|----------|----------|----------|
| `[RM][S1]` RM有Length Bias：长回复总是高分 | 训练数据中长回复被标为chosen；RM过拟合长度模式 | 1) 测试不同长度回复的分数<br>2) 计算长度-分数相关性 | 在reward计算时做长度归一化；数据中平衡长短回复 | 长度相关性<0.3；不同长度回复按质量排序 |
| `[RM][S1]` 标注一致性差：IAA<0.67 | 标注指南模糊；标注员未校准；歧义场景未定义 | 1) 计算Cohen's/Fleiss' Kappa<br>2) 分析不一致样本<br>3) 检查标注指南 | 完善标注指南（加决策树）；标注员校准测试；低一致样本复核 | Krippendorff's Alpha≥0.67；抽检一致率>80% |
| `[RM][S1]` Chosen和Rejected差异太小 | 数据生成时两者质量接近；标注员对差异不敏感 | 1) 估算质量gap分布<br>2) 抽检差异最小的样本 | 过滤gap<0.3的数据对；使用Hard Negative Mining | gap均值>0.5；训练后模型有明显区分 |

---

### 四、DPO阶段问题

| 现象 | 可能根因 | 排查步骤 | 修复动作 | 验证标准 |
|------|----------|----------|----------|----------|
| `[DPO][S0]` KL Divergence爆炸（>1.0） | 学习率过高；β值过小；偏好数据质量差 | 1) 监控KL曲线（正常0.01~0.5）<br>2) 检查输出变化<br>3) 检查数据质量 | 降低学习率到2e-6；增大β到0.2；过滤差异小的数据对 | KL稳定在0.01~0.5；输出质量保持 |
| `[DPO][S2]` Verbosity问题：回复变冗长 | 偏好数据中长回复被标为chosen；人类偏好"详细" | 1) 统计回复长度变化<br>2) 检查数据中长度分布 | 添加长度惩罚；数据中包含简洁的chosen；使用SimPO | 回复长度增长<30%；内容不冗余 |
| `[DPO][S2]` 多样性下降：回复单调 | β值过大；训练数据风格单一；过度优化 | 1) 计算distinct-n<br>2) 检查是否使用固定句式 | 适当降低β；增加数据多样性；早停 | distinct-2>0.3；不同问题有不同风格回复 |

---

### 五、RLHF/GRPO阶段问题

| 现象 | 可能根因 | 排查步骤 | 修复动作 | 验证标准 |
|------|----------|----------|----------|----------|
| `[RLHF][S1]` Reward Hacking：reward升但质量降 | RM本身有偏差；模型发现RM漏洞；训练时间过长 | 1) 监控reward与人工评估相关性<br>2) 检查输出是否有固定pattern | 定期更新RM；增大KL惩罚到0.02；基于人工评估早停；使用ensemble RM | 人工评估与reward趋势一致；无作弊pattern |
| `[GRPO][S0]` Entropy Collapse：输出多样性消失 | 学习率过高；KL约束过弱；优势估计方差大 | 1) 监控entropy（正常>0.5）<br>2) 检查组内生成多样性 | 增加entropy bonus；增大sampling temperature到1.2；降低学习率到1e-6 | entropy>0.5；组内生成有差异 |
| `[RLHF][S1]` 训练不稳定/突然崩溃 | Policy gradient方差大；遇到异常数据；学习率过高 | 1) 检查崩溃前指标<br>2) 检查是否有极端reward值 | 更严格梯度裁剪（0.5）；更长warmup；过滤异常数据 | 训练稳定运行；无突然崩溃 |

---

### 六、RLVR/推理训练问题

| 现象 | 可能根因 | 排查步骤 | 修复动作 | 验证标准 |
|------|----------|----------|----------|----------|
| `[RLVR][S1]` 答案猜测：跳过推理直接猜答案 | 推理过程未强制；格式奖励权重过高 | 1) 检查推理过程长度<br>2) 检查推理步骤数 | 要求推理过程完整；答案猜测惩罚；检测无效推理 | 推理长度>100字；步骤>3步 |
| `[RLVR][S1]` 模式记忆：对新题泛化差 | 记住训练题答案；题目多样性不足 | 1) 测试OOD题目<br>2) 检查训练题多样性 | 增加题目多样性；使用动态生成题目；OOD泛化测试 | OOD准确率与训练集差距<10% |

---

### 七、推理/部署问题

| 现象 | 可能根因 | 排查步骤 | 修复动作 | 验证标准 |
|------|----------|----------|----------|----------|
| `[推理][S0]` OOM：CUDA out of memory | 并发过高；序列过长；KV Cache过大 | 1) 监控GPU显存<br>2) 检查max_model_len设置 | 降低gpu_memory_utilization到0.8；限制max_model_len；使用量化 | 高并发下不OOM；显存使用稳定 |
| `[推理][S1]` 输出无限循环/不停止 | eos_token设置错误；训练数据无结束符；stop tokens缺失 | 1) 检查eos_token<br>2) 检查stop配置<br>3) 检查训练数据 | 配置正确的stop tokens；确保训练数据包含结束符 | 输出能正常停止；无无限循环 |
| `[推理][S2]` 复读机：重复输出 | 训练数据有重复模式；temperature过低 | 1) 检查训练数据<br>2) 检查生成参数 | 使用repetition_penalty=1.1；检查训练数据 | 无重复输出；内容流畅 |
| `[推理][S3]` 延迟高/吞吐低 | batch size不合理；未启用优化 | 1) 检查batch配置<br>2) 检查是否用PagedAttention | 调优batch size；启用Speculative Decoding；使用vLLM | 达到目标QPS/延迟 |

---

## 紧急恢复SOP

### S0问题（必须1小时内响应）

```bash
# 1. 线上事故：立即回滚
kubectl rollout undo deployment/your-llm

# 2. 训练崩溃：定位并回滚checkpoint
grep -i "error\|nan\|oom" training.log | tail -20
cp outputs/checkpoint-XXX outputs/latest

# 3. 数据泄露：阻断+清理
# 停止相关pipeline，清理已落地数据
```

### S1问题（4小时内响应）

```bash
# 1. 对比基线确认问题
python eval.py --model current --baseline last_good

# 2. 二分法定位
# 缩小范围：数据？模型？配置？

# 3. 修复验证
# staging环境验证后再上线
```

---

## 补充阅读

- **[10-故障排查与踩坑指南.md](./10-故障排查与踩坑指南.md)**：详细的踩坑案例、代码示例、Tokenizer陷阱、Packing技巧、工程黑魔法

---

> **记住**：最好的debug是prevention —— 在训练前做好验证！
