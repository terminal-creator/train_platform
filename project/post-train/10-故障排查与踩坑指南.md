# 第十章：故障排查与踩坑指南

> **本章目标**：系统化整理后训练各阶段的常见故障、诊断方法和解决方案，以及那些教科书上不写、但能决定模型生死的工程细节
>
> **使用方式**：遇到问题时，先定位到对应阶段，然后按"现象→诊断→根因→解决"的流程排查
>
> **归因总表**：如需快速定位问题，请参考 [10-故障排查Runbook.md](./10-故障排查Runbook.md)

---

## 一、故障排查总览

### 1.1 问题分类

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        后训练常见问题分类                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐   │
│  │  训练崩溃   │  │  效果不佳   │  │  能力退化   │  │  部署问题   │   │
│  │             │  │             │  │             │  │             │   │
│  │ • Loss NaN  │  │ • 准确率低  │  │ • 灾难遗忘  │  │ • OOM      │   │
│  │ • 梯度爆炸  │  │ • 不遵循指令│  │ • 变笨了    │  │ • 速度慢   │   │
│  │ • OOM      │  │ • 幻觉严重  │  │ • 拒绝回答  │  │ • 输出异常  │   │
│  │ • 收敛停滞  │  │ • 风格跑偏  │  │ • 多样性降  │  │ • 延迟高   │   │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 1.2 排查黄金法则

1. **先确认数据**：80%的问题出在数据上
2. **看训练曲线**：Loss、KL、Reward等指标的变化趋势
3. **人工检查输出**：数值正常不代表质量正常
4. **对比基线**：与训练前/上一版本对比
5. **二分法定位**：缩小问题范围，逐步排查

---

## 二、Tokenizer：隐形杀手

Tokenizer 是大模型训练中最容易被忽视、却最容易导致毁灭性后果的环节。

### 2.1 Padding Side 的铁律

| 场景 | Padding 方向 | 原因 | 后果 |
| :--- | :--- | :--- | :--- |
| **训练 (SFT)** | `Right Padding` | 配合 `ignore_index=-100`，对齐方便 | 无（只要 mask 正确） |
| **推理 (Generation)** | **`Left Padding`** | 生成是向右生长的，pad 在左边不影响 positional embedding | **乱码/停止**：如果 pad 在右边，模型会以为句子已经结束或者接在 pad 后面生成 |

```python
# 正确的推理设置
tokenizer.padding_side = "left"
tokenizer.pad_token = tokenizer.eos_token # 如果没有 pad token
```

### 2.2 BOS/EOS 的幽灵

不同的基座模型对 BOS (Begin of Sentence) 和 EOS (End of Sentence) 的处理完全不同。

*   **Llama 3 / Mistral**：通常**不需要**手动添加 BOS，Tokenizer 会自动处理。如果强行添加，会导致输入变成 `[BOS] [BOS] User: ...`，模型会困惑。
*   **Qwen 2.5**：对 system prompt 极其敏感，必须严格遵循 chat template。

**避坑检查清单**：
1.  **训练前打印**：解码 `input_ids` 的前 10 个 token，人眼确认是否有双重 BOS。
2.  **推理一致性**：训练时如果加了 EOS，推理时的 stop token 必须包含 EOS。

### 2.3 新增 Special Tokens 的陷阱

当你为 Tool Use 或特定格式新增 `<|sales_start|>` 这样的 token 时：

1.  **Resize 是必须的**：`model.resize_token_embeddings(len(tokenizer))`
2.  **初始化是关键**：**千万不要随机初始化**新 token 的 embedding！随机初始化的向量模长可能与预训练向量差异巨大，导致 Loss 开局爆炸。

```python
# 黑魔法：用相似词的均值初始化新 token
new_token_id = tokenizer.convert_tokens_to_ids("<|sales_start|>")
# 比如用 "sales" 和 "start" 的均值
ref_ids = tokenizer.convert_tokens_to_ids(["sales", "start"])
with torch.no_grad():
    model.model.embed_tokens.weight[new_token_id] = \
        model.model.embed_tokens.weight[ref_ids].mean(dim=0)
```

---

## 三、高效训练：Packing 的正确姿势

为了提高训练效率，我们通常会将多条短数据拼接（Pack）成一条长数据（例如 4096 长度）。这里有两个巨坑。

### 3.1 交叉污染 (Cross-contamination)

如果只是简单的拼接，Attention 机制会让后面的样本看到前面样本的内容（Causal Mask 是下三角矩阵）。

**正确做法**：使用 **Block Diagonal Mask**（块对角掩码）或 **Document Masking**。

```
常规 Causal Mask:       Packing 正确 Mask (Block Diagonal):
[1 0 0] Sample A        [1 0 0] Sample A (只看自己)
[1 1 0] Sample A        [1 1 0] Sample A
[1 1 1] Sample B (错!)  [0 0 1] Sample B (看不见 A)
```

### 3.2 Position ID 重置

拼接后的第二条样本，Position ID 应该从 0 开始，还是接着上一条？

*   **Rotary Embedding (RoPE)**：RoPE 对绝对位置敏感。
*   **正确做法**：**Reset Position IDs**。每条样本的 Position ID 都应该重新从 0 计数，否则模型会认为第二条样本是在极远的上下文中，破坏语义理解。

---

## 四、训练监控：看图诊病

Loss 曲线是模型的"心电图"，不仅要看下降，还要看形态。

### 4.1 Loss Spike (突刺) 的鉴别

Loss 突然从 1.5 飙升到 10.0，然后又慢慢降下来，或者变成 NaN。

| 可能原因 | 诊断方法 | 解决方案 |
| :--- | :--- | :--- |
| **脏数据** | 检查 Spike 那个 batch 的数据 | 通常是超长无意义字符串（乱码、Base64），**过滤数据** |
| **BF16 溢出** | 检查 Grad Norm 是否瞬间极大 | 开启 `float32` 优化器累积，或调小 LR |
| **LR 过大** | Spike 频繁出现 | 降低 Learning Rate |

### 4.2 梯度范数 (Grad Norm) 的信号

*   **长期 < 0.1**：模型"躺平"了，学习率太低，或者数据没有区分度。
*   **长期 > Clipping Threshold (如 1.0)**：模型一直在"悬崖边缘"行走，建议调小 LR 或检查数据分布。
*   **突然变为 0**：死神经元（Dead ReLU/GeLU）或者 Loss Scale 问题。

### 4.3 Eval Loss 上升 ≠ 过拟合

在 SFT 中，有时 Train Loss 下降，但 Eval Loss 上升。这不一定是过拟合。

*   **解释**：SFT 的目标是让模型学会"格式"和"风格"，这导致模型输出分布剧烈偏移（Shift），使得在通用 Eval 集上的 Perplexity 上升。
*   **对策**：不要只看 Eval Loss，要看 **Token Accuracy** 或者具体的 **Benchmark 分数**。如果 Benchmark 分数没掉，Eval Loss 上升是可以接受的。

---

## 五、SFT 阶段踩坑

### 5.1 Loss Masking 错误

| 严重程度 | 🔴 严重 - 会导致训练完全失效 |
|----------|------------------------------|

**现象**：
- 训练Loss正常下降，但模型输出instruction而非response
- 模型"学会"了重复用户的问题
- 模型输出包含role header（如`<|im_start|>user`）

**诊断方法**：
```python
# 验证脚本（训练前必跑！）
def check_masking(tokenizer, sample):
    input_ids = sample["input_ids"]
    labels = sample["labels"]

    print("Token对齐检查：")
    for i, (iid, lid) in enumerate(zip(input_ids, labels)):
        token = tokenizer.decode([iid])
        trained = "TRAIN" if lid != -100 else "skip "
        print(f"{i:4d} | {trained} | {repr(token)}")

# 检查清单：
# [ ] 所有assistant回复的token标记为TRAIN
# [ ] 所有user/system内容的token标记为skip
# [ ] 特殊token（role header）标记为skip
```

**根因**：
- 使用`text.split()`近似估算mask位置（对多轮对话必错）
- 未考虑chat template的特殊token
- 不同模型的template差异未处理

**解决方案**：
```python
# ✅ 正确方案1：使用TRL内置collator
from trl import DataCollatorForCompletionOnlyLM

response_template = "<|im_start|>assistant\n"
collator = DataCollatorForCompletionOnlyLM(
    response_template=response_template,
    tokenizer=tokenizer
)

# ✅ 正确方案2：使用LLaMA-Factory
# 框架自动处理，无需手动实现

# ✅ 正确方案3：基于模板marker精确定位
# 见03-监督微调SFT.md中的实现
```

### 5.2 Chat Template 不匹配

| 严重程度 | 🟡 中等 - 影响训练效果 |
|----------|------------------------|

**现象**：
- 训练效果差于预期
- 推理时输出格式异常
- 模型不停止生成（无限输出）

**诊断方法**：
```python
# 检查tokenizer的chat_template
print(tokenizer.chat_template)

# 检查特殊token
print(f"eos_token: {tokenizer.eos_token} -> {tokenizer.eos_token_id}")
print(f"pad_token: {tokenizer.pad_token} -> {tokenizer.pad_token_id}")

# 对比训练和推理的格式是否一致
train_text = tokenizer.apply_chat_template(messages, tokenize=False)
print(train_text)
```

**根因**：
- 训练时使用的template与模型原生template不一致
- eos_token设置错误，导致生成不停止
- pad_token未设置或设置为eos_token（会导致提前停止）

**解决方案**：
```python
# 1. 使用模型原生template
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# 不要手动覆盖chat_template

# 2. 正确设置pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.unk_token  # 或添加新token
    # ❌ 不要用：tokenizer.pad_token = tokenizer.eos_token

# 3. 确保训练和推理使用相同的tokenizer配置
tokenizer.save_pretrained(output_dir)
```

### 5.3 灾难性遗忘

| 严重程度 | 🟡 中等 - 通用能力下降 |
|----------|------------------------|

**现象**：
- 领域任务变好了，但通用任务变差了
- 数学/代码能力骤降
- 中文能力下降（或反之）

**诊断方法**：
```python
# 用标准benchmark评估
# 训练前后分别测试：MMLU、GSM8K、HumanEval等

# 快速人工检查
test_prompts = [
    "1+1等于几？",
    "写一个Python快速排序",
    "用英语介绍自己",
    "解释什么是机器学习",
]
# 对比训练前后的回答质量
```

**根因**：
- 纯领域数据训练，缺少通用数据
- 学习率过高
- 训练epoch过多（过拟合）

**解决方案**：
```yaml
# 1. 数据配比
#    领域数据:通用数据 = 70%:30% 或 80%:20%

# 2. 使用Instruct模型作为基座（而非Base模型）
#    已经有通用能力，更不容易遗忘

# 3. 降低学习率
learning_rate: 1e-5  # 而非2e-5

# 4. 早停
#    监控eval_loss，开始上升就停止

# 5. 使用LoRA（减少参数更新范围）
finetuning_type: lora
lora_rank: 64
```

**黑魔法：NEFTune (Noisy Embedding Finetuning)**

一个极其简单但有效的技巧：**在 Embedding 层输出加入均匀分布的随机噪声**。

```python
# 实现 NEFTune
def neftune_forward_hook(module, args, output):
    if module.training:
        dims = torch.tensor(output.size(1) * output.size(2))
        mag_norm = 5.0 / torch.sqrt(dims) # 噪声系数通常 5-10
        output = output + torch.zeros_like(output).uniform_(-mag_norm, mag_norm)
    return output

model.get_input_embeddings().register_forward_hook(neftune_forward_hook)
```
*   **效果**：防止模型过度拟合 SFT 数据的微小细节，显著提升对话鲁棒性。

**黑魔法：Model Soups / SWA (Weight Averaging)**

不要只取 `checkpoint-best`。往往**平均最后几个 epoch 的权重**，泛化能力更好。

```python
# 简单的权重平均
avg_state_dict = {}
for path in checkpoint_paths:
    state_dict = torch.load(path)
    for k, v in state_dict.items():
        if k not in avg_state_dict:
            avg_state_dict[k] = v.clone()
        else:
            avg_state_dict[k] += v
for k in avg_state_dict:
    avg_state_dict[k] /= len(checkpoint_paths)
```

### 5.4 过度知识注入

| 严重程度 | 🟡 中等 - 指令遵循能力下降 |
|----------|---------------------------|

**现象**：
- 模型开始"背书"，回复像百科全书
- 不回答用户问题，自顾自地输出知识
- 指令遵循能力下降

**根因**：
- SFT数据中包含大量纯知识内容
- 数据格式像"问答对"而非"对话"

**解决方案**：
- 知识注入应在CPT阶段完成，而非SFT
- SFT数据应该是**对话式**的，包含澄清、追问、多轮交互
- 使用RAG补充知识，而非硬塞进模型

---

## 六、DPO 阶段踩坑

### 6.1 KL Divergence 爆炸

| 严重程度 | 🔴 严重 - 训练失控 |
|----------|-------------------|

**现象**：
- KL divergence快速增长（>1.0）
- 模型输出变得奇怪/无意义
- Loss先下降后上升

**诊断方法**：
```python
# 监控KL曲线
# 正常范围：0.01 ~ 0.5
# 危险信号：> 1.0

# 检查输出变化
for step in [0, 100, 500, 1000]:
    load_checkpoint(step)
    print(f"Step {step}:")
    print(model.generate(test_prompt))
```

**根因**：
- 学习率过高
- β值过小（约束太弱）
- 偏好数据质量差（chosen和rejected差异不明显）

**解决方案**：
```yaml
# 1. 降低学习率
learning_rate: 2e-6  # 从5e-6降到2e-6

# 2. 增大β值
pref_beta: 0.2  # 从0.1增到0.2

# 3. 检查数据质量
#    chosen和rejected应该有明显差异
#    去除chosen_score和rejected_score差异小于0.5的样本
```

### 6.2 Verbosity 问题（回复变冗长）

| 严重程度 | 🟡 中等 - 用户体验下降 |
|----------|------------------------|

**现象**：
- 训练后模型回复明显变长
- 重复表达同一个意思
- 添加不必要的礼貌用语

**诊断方法**：
```python
# 统计回复长度变化
before_lengths = [len(r) for r in responses_before]
after_lengths = [len(r) for r in responses_after]

print(f"Before: mean={np.mean(before_lengths):.0f}")
print(f"After: mean={np.mean(after_lengths):.0f}")
# 如果增长>30%，说明有verbosity问题
```

**根因**：
- 偏好数据中长回复被标记为chosen
- 人类标注者倾向于认为详细=好

**解决方案**：
```python
# 1. 在偏好数据中平衡长短回复
# 2. 添加长度惩罚
def length_normalized_reward(reward, length, target_length=200):
    penalty = abs(length - target_length) / target_length * 0.1
    return reward - penalty

# 3. 使用SimPO（内置长度归一化）
```

### 6.3 多样性下降

| 严重程度 | 🟡 中等 - 回复单调 |
|----------|-------------------|

**现象**：
- 不同问题得到相似的回复
- 模型倾向于使用固定句式
- distinct-n指标下降

**诊断方法**：
```python
def compute_distinct_n(responses, n=2):
    """计算n-gram多样性"""
    all_ngrams = []
    for resp in responses:
        tokens = resp.split()
        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
        all_ngrams.extend(ngrams)

    unique = len(set(all_ngrams))
    total = len(all_ngrams)
    return unique / total if total > 0 else 0

# distinct-2 < 0.3 说明多样性不足
```

**根因**：
- β值过大，模型不敢偏离reference
- 训练数据风格单一
- 过度优化（训练太久）

**解决方案**：
- 适当降低β值
- 增加训练数据多样性
- 早停（监控distinct-n）

---

## 七、RLHF/GRPO 阶段踩坑

### 7.1 Reward Hacking

| 严重程度 | 🔴 严重 - 训练目标被绕过 |
|----------|-------------------------|

**现象**：
- Reward持续上升，但输出质量下降
- 模型学会了特定的"作弊"模式
- 人工评估与自动评估不一致

**诊断方法**：
```python
# 1. 监控reward与人工评估的相关性
# 2. 检查输出是否有固定pattern

# 常见hacking模式
hacking_patterns = [
    "过度使用'我理解您的问题'等礼貌用语",
    "回复越来越长（length hacking）",
    "反复强调同一个点",
    "使用特定的关键词/格式",
]
```

**根因**：
- Reward Model本身有偏差
- 模型发现了RM的漏洞
- 训练时间过长

**解决方案**：
```python
# 1. 定期更新/重训Reward Model
# 2. 增大KL惩罚
kl_coef: 0.02  # 从0.01增到0.02

# 3. 早停（基于人工评估而非reward）
# 4. 使用ensemble of reward models
```

**黑魔法：奖励工程复兴 (Reward Engineering)**

在 DeepSeek-R1 之后，大家意识到：**规则比模型更可靠**。

对于销售场景，很多好坏标准是可以写成正则表达式的：
*   是否包含"您好"？
*   是否在 5 句话内询问了需求？
*   是否没有包含敏感词？

**混合奖励公式**：
$$ R_{total} = \alpha \cdot R_{model} + \beta \cdot R_{rule} + \gamma \cdot R_{format} $$

*   $R_{model}$：RM 模型打分（语义层面）
*   $R_{rule}$：正则匹配得分（硬性业务规则）
*   $R_{format}$：格式惩罚（XML 标签是否闭合等）

这种方法比纯 RM 训练更稳定，且容易人为干预控制。

### 7.2 Entropy Collapse（熵坍塌）

| 严重程度 | 🔴 严重 - 输出多样性消失 |
|----------|-------------------------|

**现象**：
- 组内生成的多个回复几乎相同
- 模型变得过度自信
- 探索能力消失，无法生成新的模式

**诊断方法**：
```python
# 监控entropy
# 正常范围：> 0.5
# 危险信号：< 0.3

# 检查组内多样性
for prompt in test_prompts:
    responses = [generate(prompt, temperature=1.0) for _ in range(5)]
    # 如果responses几乎相同，说明entropy collapse
```

**根因**：
- 学习率过高
- KL约束过弱
- 优势估计方差过大

**解决方案**：
```python
# 1. 增加entropy bonus
entropy_coef: 0.01  # 添加entropy正则

# 2. 增大sampling temperature
sampling_temperature: 1.2  # 从1.0增到1.2

# 3. 降低学习率
learning_rate: 1e-6  # 从5e-6降到1e-6

# 4. 增大KL约束
kl_coef: 0.01  # 从0.001增到0.01
```

### 7.3 训练不稳定/突然崩溃

| 严重程度 | 🟡 中等 - 需要重启训练 |
|----------|------------------------|

**现象**：
- 训练开始正常，某个step突然Loss爆炸
- Reward先上升后突然下降
- 梯度出现NaN

**诊断方法**：
```python
# 检查崩溃前的指标
# - 梯度范数是否异常增大
# - KL是否异常增大
# - 是否有极端的reward值

# 常见崩溃点
# - 切换到新的数据batch
# - 学习率warmup结束
# - 遇到特别长/短的序列
```

**根因**：
- Policy gradient方差大
- 遇到异常数据
- 学习率过高

**解决方案**：
```python
# 1. 更严格的梯度裁剪
max_grad_norm: 0.5  # 从1.0降到0.5

# 2. 使用更小的学习率 + 更长的warmup
warmup_ratio: 0.1

# 3. 过滤异常数据
# - 移除过长/过短的样本
# - 移除reward值极端的样本

# 4. 定期保存checkpoint
save_steps: 50  # 频繁保存以便回滚
```

---

## 八、推理/部署阶段踩坑

### 8.1 显存溢出（OOM）

| 严重程度 | 🔴 严重 - 服务不可用 |
|----------|---------------------|

**现象**：
- `CUDA out of memory`错误
- 高并发时服务崩溃
- 长序列处理失败

**诊断方法**：
```bash
# 监控GPU显存使用
watch -n 1 nvidia-smi

# 检查KV Cache大小
# KV Cache = 2 * num_layers * batch * seq_len * hidden * bytes
```

**解决方案**：
```python
# 1. 降低gpu_memory_utilization
gpu_memory_utilization=0.8  # 从0.9降到0.8

# 2. 限制最大序列长度
max_model_len=2048  # 根据实际需求设置

# 3. 使用量化
# AWQ/GPTQ 4bit量化可节省约75%显存

# 4. 使用PagedAttention（vLLM默认启用）
```

### 8.2 输出无限循环

| 严重程度 | 🟡 中等 - 需要强制截断 |
|----------|------------------------|

**现象**：
- 模型不停止生成
- 重复相同的内容
- 一直生成到max_tokens

**根因**：
- eos_token设置错误
- 训练数据中没有正确的结束符
- stop tokens配置缺失

**解决方案**：
```python
# 1. 检查eos_token
print(tokenizer.eos_token)  # 应该有值

# 2. 配置stop tokens
sampling_params = SamplingParams(
    stop=["<|im_end|>", "<|endoftext|>"],
    stop_token_ids=[tokenizer.eos_token_id],
)

# 3. 检查训练数据是否包含结束符
```

### 8.3 模型"复读机"

| 严重程度 | 🟡 中等 - 输出质量差 |
|----------|---------------------|

**现象**：
- 同一个词/句子重复多次
- 输出陷入循环

**解决方案**：
```python
# 1. 使用repetition_penalty
sampling_params = SamplingParams(
    repetition_penalty=1.1,  # >1.0 抑制重复
)

# 2. 使用presence_penalty和frequency_penalty
sampling_params = SamplingParams(
    presence_penalty=0.5,
    frequency_penalty=0.5,
)

# 3. 检查训练数据是否有重复问题
```

### 8.4 推理加速：Speculative Decoding

销售场景的回复往往有很多"套话"（如"好的，我明白了，请稍等..."）。这种场景是 **Speculative Decoding** 的绝佳用武之地。

*   **原理**：用一个极小的 Draft Model (如 100M 参数) 快速生成 token，然后用大模型 (7B) 并行验证。
*   **收益**：在套话多的场景下，可以实现 **2-3 倍** 的推理加速，且**完全无损**（输出结果与大模型贪婪解码完全一致）。
*   **vLLM 支持**：vLLM 原生支持投机采样，只需配置 draft model 即可。

```bash
# vLLM 启用投机采样
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --speculative-model Qwen/Qwen2.5-0.5B-Instruct \
    --num-speculative-tokens 5
```

---

## 九、数据相关踩坑

### 9.1 训练/测试数据泄漏

| 严重程度 | 🔴 严重 - 评估结果不可信 |
|----------|-------------------------|

**现象**：
- 测试集表现异常好
- 模型"记住"了答案而非学会能力

**诊断方法**：
```python
# 检查数据重叠
def check_leakage(train_data, test_data):
    train_set = set(hash(json.dumps(d, sort_keys=True)) for d in train_data)
    leaked = []
    for d in test_data:
        if hash(json.dumps(d, sort_keys=True)) in train_set:
            leaked.append(d)
    print(f"泄漏数量: {len(leaked)}/{len(test_data)}")
    return leaked
```

**解决方案**：
- 严格的数据划分流程
- 使用时间戳划分（旧数据训练，新数据测试）
- 定期更换测试集

### 9.2 数据格式不一致

| 严重程度 | 🟡 中等 - 训练效果差 |
|----------|---------------------|

**现象**：
- 同一任务有多种不同的格式
- 模型学会了错误的格式
- 输出不稳定

**解决方案**：
```python
# 1. 统一数据格式
def standardize_format(data):
    for item in data:
        # 统一role名称
        item["role"] = item["role"].lower()
        if item["role"] in ["human", "customer"]:
            item["role"] = "user"
        if item["role"] in ["ai", "bot", "agent"]:
            item["role"] = "assistant"
    return data

# 2. 数据验证
def validate_data(data):
    for item in data:
        assert item["role"] in ["system", "user", "assistant"]
        assert len(item["content"]) > 0
```

---

## 十、快速排查清单

### 10.1 训练开始前

- [ ] 数据格式正确？运行验证脚本
- [ ] Loss masking正确？打印token-label对齐
- [ ] 显存估算足够？用公式计算
- [ ] 基线测试完成？记录训练前指标
- [ ] Tokenizer设置正确？检查BOS/EOS/PAD

### 10.2 训练过程中

- [ ] Loss曲线正常？平滑下降，无突跳
- [ ] KL在合理范围？0.01~0.5
- [ ] 梯度范数正常？无突然增大
- [ ] 定期人工检查输出？数值正常≠质量正常

### 10.3 训练结束后

- [ ] 对比基线？各维度指标变化
- [ ] 人工评估？覆盖核心场景
- [ ] 回归测试？关键case不能退化
- [ ] A/B测试？线上效果验证

---

## 十一、紧急恢复 SOP

### 11.1 训练崩溃恢复

```bash
# 1. 定位崩溃点
grep -i "error\|nan\|oom" training.log | tail -20

# 2. 回滚到最近的checkpoint
cp outputs/checkpoint-XXX outputs/latest

# 3. 降低学习率重启
# 修改配置文件后重启训练

# 4. 如果仍然崩溃，检查数据
# 可能是某个batch有问题
```

### 11.2 模型变笨恢复

```bash
# 1. 确认是否真的变笨（对比基线）
python eval.py --model outputs/checkpoint-XXX --benchmark mmlu

# 2. 如果确认退化，回滚到早期checkpoint
# 通常中间某个checkpoint效果最好

# 3. 分析原因
# - 训练epoch过多？
# - 数据配比不合理？
# - 学习率过高？

# 4. 调整配置重训
```

### 11.3 线上事故处理

```bash
# 1. 立即回滚到上一版本
kubectl rollout undo deployment/sales-llm

# 2. 检查问题
# - 是模型问题还是服务问题？
# - 是所有请求都有问题还是特定请求？

# 3. 修复并验证
# 在staging环境验证后再上线

# 4. 复盘
# 记录问题原因和解决方案
```

---

## 本章小结

后训练踩坑的核心经验：

1. **数据问题占80%**：格式、质量、泄漏、配比
2. **看曲线不如看输出**：数值正常不代表质量正常
3. **小步快跑**：频繁保存checkpoint，方便回滚
4. **对比基线**：任何改动都要量化对比
5. **人工检查必不可少**：自动指标会骗人
6. **Tokenizer是隐形杀手**：Padding Side、BOS/EOS、Special Token都要小心
7. **黑魔法有时候很有效**：NEFTune、Model Soups、规则奖励

**记住**：最好的debug是prevention —— 在训练前做好验证！

---

## 相关阅读

- **[10-故障排查Runbook.md](./10-故障排查Runbook.md)**：归因总表（现象→根因→排查→修复→验证），快速定位问题
