# 第十一章：前沿技术

> **本章目标**：掌握后训练领域的最新进展和前沿技术
>
> **覆盖内容**：推理增强训练、Agent能力训练、长上下文扩展、模型合并
>
> **适用读者**：已掌握基础后训练技术，希望进一步提升模型能力的工程师

---

## 一、推理增强训练（Reasoning Training）

### 1.1 从CoT到推理RL：演进路径

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        推理能力训练演进                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  阶段1: Prompt Engineering (2022)                                           │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                             │
│  方法：Chain-of-Thought Prompting                                           │
│  局限：依赖prompt，不改变模型本身能力                                        │
│                                                                             │
│  阶段2: CoT Fine-tuning (2023)                                              │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                               │
│  方法：用带推理过程的数据做SFT                                               │
│  局限：推理质量受限于训练数据                                                │
│                                                                             │
│  阶段3: RLVR - RL with Verifiable Rewards (2024)                            │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                   │
│  方法：用可验证的奖励（数学正确性、代码执行）做RL                             │
│  代表：DeepSeek-R1、Qwen-Math                                               │
│  突破：推理能力可以通过RL自动涌现                                            │
│                                                                             │
│  阶段4: Test-Time Compute Scaling (2025)                                    │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                       │
│  方法：推理时动态分配更多计算资源                                            │
│  代表：o1、DeepSeek-R1                                                      │
│  趋势：Train-time与Test-time compute的平衡                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 1.2 DeepSeek-R1训练方法解析

> **核心发现**：纯RL训练可以让推理能力自动涌现，无需SFT引导

```python
"""
DeepSeek-R1训练流程复现
基于GRPO + 可验证奖励
"""
from dataclasses import dataclass
from typing import List, Dict, Optional
import torch

@dataclass
class R1TrainingConfig:
    """R1风格训练配置"""
    # 基础配置
    model_name: str = "deepseek-base-7b"

    # GRPO配置
    num_samples_per_prompt: int = 16  # 每个问题采样16个回复
    max_response_length: int = 32768  # 长推理链
    temperature: float = 1.0

    # RL配置
    kl_coef: float = 0.001  # 非常小的KL约束
    clip_ratio: float = 10.0  # 较大的clip范围

    # 奖励配置
    reward_type: str = "verifiable"  # 可验证奖励
    format_reward_weight: float = 0.1  # 格式奖励权重


class VerifiableRewardCalculator:
    """可验证奖励计算器"""

    def __init__(self, task_type: str):
        self.task_type = task_type

    def calculate_reward(
        self,
        question: str,
        response: str,
        ground_truth: str
    ) -> Dict:
        """
        计算可验证奖励

        关键：奖励完全基于结果正确性，不依赖主观判断
        """
        rewards = {}

        if self.task_type == "math":
            # 数学题：提取答案并验证
            predicted_answer = self._extract_math_answer(response)
            correct_answer = self._extract_math_answer(ground_truth)

            # 正确性奖励（核心）
            rewards["correctness"] = 1.0 if predicted_answer == correct_answer else 0.0

            # 格式奖励（辅助）
            rewards["format"] = self._check_format(response)

        elif self.task_type == "code":
            # 代码题：执行测试用例
            test_results = self._execute_code_tests(response, ground_truth)

            rewards["correctness"] = test_results["pass_rate"]
            rewards["format"] = 1.0 if "```" in response else 0.5

        # 综合奖励
        total_reward = (
            rewards["correctness"] * 0.9 +
            rewards.get("format", 0) * 0.1
        )

        return {
            "total": total_reward,
            "components": rewards
        }

    def _extract_math_answer(self, text: str) -> Optional[str]:
        """从文本中提取数学答案"""
        import re

        # 尝试匹配 \boxed{answer} 格式
        boxed_match = re.search(r'\\boxed\{([^}]+)\}', text)
        if boxed_match:
            return boxed_match.group(1).strip()

        # 尝试匹配 "答案是/答案为" 格式
        answer_match = re.search(r'答案[是为：:]\s*([^\n。]+)', text)
        if answer_match:
            return answer_match.group(1).strip()

        # 尝试匹配最后一个数字
        numbers = re.findall(r'-?\d+\.?\d*', text)
        if numbers:
            return numbers[-1]

        return None

    def _check_format(self, response: str) -> float:
        """检查推理格式"""
        score = 0.0

        # 包含思考过程标记
        if "<think>" in response or "让我思考" in response:
            score += 0.3

        # 包含步骤分解
        if re.search(r'(第[一二三四五]步|步骤\d|Step \d)', response):
            score += 0.3

        # 包含最终答案标记
        if "\\boxed" in response or "答案" in response:
            score += 0.4

        return min(1.0, score)

    def _execute_code_tests(self, code: str, test_cases: str) -> Dict:
        """执行代码测试"""
        import subprocess
        import tempfile
        import os

        # 提取代码块
        code_match = re.search(r'```python\n(.*?)```', code, re.DOTALL)
        if not code_match:
            return {"pass_rate": 0.0, "error": "No code block found"}

        code_content = code_match.group(1)

        # 写入临时文件并执行
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code_content + "\n" + test_cases)
                temp_path = f.name

            result = subprocess.run(
                ['python', temp_path],
                capture_output=True,
                timeout=10
            )

            os.unlink(temp_path)

            if result.returncode == 0:
                return {"pass_rate": 1.0}
            else:
                return {"pass_rate": 0.0, "error": result.stderr.decode()}

        except subprocess.TimeoutExpired:
            return {"pass_rate": 0.0, "error": "Timeout"}
        except Exception as e:
            return {"pass_rate": 0.0, "error": str(e)}


class R1StyleTrainer:
    """R1风格训练器"""

    def __init__(self, config: R1TrainingConfig):
        self.config = config
        self.reward_calculator = VerifiableRewardCalculator("math")

    def train_step(
        self,
        model,
        ref_model,
        prompts: List[str],
        ground_truths: List[str]
    ) -> Dict:
        """
        单步训练

        核心流程：
        1. 对每个prompt采样多个回复
        2. 计算可验证奖励
        3. 组内相对排序得到advantage
        4. GRPO更新
        """
        all_advantages = []
        all_log_probs = []

        for prompt, gt in zip(prompts, ground_truths):
            # 1. 采样多个回复
            responses = self._sample_responses(
                model, prompt,
                n=self.config.num_samples_per_prompt
            )

            # 2. 计算每个回复的奖励
            rewards = []
            for response in responses:
                reward_info = self.reward_calculator.calculate_reward(
                    prompt, response, gt
                )
                rewards.append(reward_info["total"])

            # 3. 组内归一化得到advantage
            rewards = torch.tensor(rewards)
            mean_reward = rewards.mean()
            std_reward = rewards.std() + 1e-8
            advantages = (rewards - mean_reward) / std_reward

            # 4. 计算log probs
            log_probs = self._compute_log_probs(model, prompt, responses)
            ref_log_probs = self._compute_log_probs(ref_model, prompt, responses)

            all_advantages.append(advantages)
            all_log_probs.append((log_probs, ref_log_probs))

        # 5. GRPO损失计算
        loss = self._compute_grpo_loss(all_advantages, all_log_probs)

        return {
            "loss": loss,
            "mean_reward": sum(r.mean().item() for r in all_advantages) / len(all_advantages)
        }

    def _sample_responses(
        self,
        model,
        prompt: str,
        n: int
    ) -> List[str]:
        """采样多个回复"""
        responses = []

        # 构造推理prompt
        reasoning_prompt = f"""请仔细思考以下问题，展示完整的推理过程。

问题：{prompt}

请在<think>和</think>标签中展示你的思考过程，然后给出最终答案。"""

        for _ in range(n):
            response = model.generate(
                reasoning_prompt,
                max_length=self.config.max_response_length,
                temperature=self.config.temperature,
                do_sample=True
            )
            responses.append(response)

        return responses

    def _compute_log_probs(self, model, prompt, responses):
        """计算log概率"""
        # 实现略
        pass

    def _compute_grpo_loss(self, advantages, log_probs_pairs):
        """计算GRPO损失"""
        total_loss = 0

        for advs, (log_probs, ref_log_probs) in zip(advantages, log_probs_pairs):
            # 重要性权重
            ratio = torch.exp(log_probs - ref_log_probs)

            # Clipped ratio
            clipped_ratio = torch.clamp(
                ratio,
                1 - self.config.clip_ratio,
                1 + self.config.clip_ratio
            )

            # Policy loss
            policy_loss = -torch.min(
                ratio * advs,
                clipped_ratio * advs
            ).mean()

            # KL penalty
            kl = (ref_log_probs - log_probs).mean()

            total_loss += policy_loss + self.config.kl_coef * kl

        return total_loss / len(advantages)
```

### 1.3 RLVR的Reward Hacking风险

> ⚠️ **警告**：RLVR并非"无Hacking"，只是Hacking方式不同

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     RLVR中的Reward Hacking类型                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  类型                  表现                          对策                    │
│  ━━━━                  ━━━━                          ━━━━                    │
│                                                                             │
│  答案猜测              跳过推理直接猜答案            要求推理过程完整        │
│  (Answer Guessing)     格式正确但推理错误                                   │
│                                                                             │
│  模式记忆              记住训练题的答案              增加题目多样性          │
│  (Pattern Memorizing)  对新题泛化能力差              使用动态生成的题目      │
│                                                                             │
│  格式套利              用大量格式词堆砌              降低格式奖励权重        │
│  (Format Gaming)       "让我仔细思考..."重复多次     检测无效推理            │
│                                                                             │
│  长度套利              生成超长但无效的推理链        长度惩罚/归一化         │
│  (Length Gaming)       希望"蒙对"答案                                      │
│                                                                             │
│  捷径推理              找到绕过真正推理的捷径        对抗性测试集            │
│  (Shortcut Reasoning)  如通过答案格式倒推            OOD泛化测试             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

```python
"""
RLVR Hacking检测与缓解
"""
class RLVRHackingDetector:
    """RLVR Hacking检测器"""

    def detect_answer_guessing(self, response: str) -> bool:
        """检测答案猜测"""
        # 检查推理过程长度
        think_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)
        if think_match:
            thinking = think_match.group(1)
            # 推理过程过短
            if len(thinking) < 100:
                return True
            # 推理步骤过少
            if thinking.count('\n') < 3:
                return True
        return False

    def detect_format_gaming(self, response: str) -> bool:
        """检测格式套利"""
        # 检测重复的格式词
        filler_patterns = [
            r'让我(仔细|认真)思考',
            r'首先.*其次.*最后',
            r'第一步.*第二步.*第三步'
        ]

        for pattern in filler_patterns:
            matches = re.findall(pattern, response)
            if len(matches) > 2:  # 同一模式重复超过2次
                return True

        return False

    def compute_reasoning_quality(self, response: str, question: str) -> float:
        """
        计算推理质量分数
        用于过滤低质量但答案正确的样本
        """
        score = 0.0

        # 1. 推理与问题的相关性
        question_keywords = set(jieba.cut(question))
        response_keywords = set(jieba.cut(response))
        overlap = len(question_keywords & response_keywords) / len(question_keywords)
        score += overlap * 0.3

        # 2. 推理步骤的逻辑性（使用LLM评估）
        logic_score = self._evaluate_logic(response)
        score += logic_score * 0.4

        # 3. 数学表达式的使用
        math_expressions = re.findall(r'\$.*?\$|\\[.*?\\]', response)
        if math_expressions:
            score += 0.3

        return min(1.0, score)


class RLVRRewardShaping:
    """RLVR奖励塑造 - 缓解Hacking"""

    def shaped_reward(
        self,
        correctness: float,
        response: str,
        question: str
    ) -> float:
        """
        塑造后的奖励函数

        不仅看结果正确性，还看推理过程质量
        """
        detector = RLVRHackingDetector()

        # 基础正确性奖励
        reward = correctness

        # 惩罚Hacking行为
        if detector.detect_answer_guessing(response):
            reward *= 0.5  # 答案猜测惩罚

        if detector.detect_format_gaming(response):
            reward *= 0.7  # 格式套利惩罚

        # 奖励高质量推理
        reasoning_quality = detector.compute_reasoning_quality(response, question)
        reward = reward * 0.7 + reasoning_quality * 0.3

        return reward
```

### 1.4 推理训练数据构造

```python
"""
推理训练数据构造策略
"""
class ReasoningDataGenerator:
    """推理数据生成器"""

    def __init__(self):
        self.client = OpenAI()

    def generate_cot_data(
        self,
        questions: List[str],
        answers: List[str]
    ) -> List[Dict]:
        """
        生成Chain-of-Thought训练数据
        """
        cot_data = []

        for q, a in zip(questions, answers):
            # 使用强模型生成推理过程
            prompt = f"""请详细解答以下问题，展示完整的推理过程。

问题：{q}

要求：
1. 分步骤解答，每步都要有清晰的逻辑
2. 使用<think></think>标签包裹思考过程
3. 最后用\\boxed{{}}给出答案

请开始解答："""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )

            reasoning = response.choices[0].message.content

            # 验证答案正确性
            extracted_answer = self._extract_answer(reasoning)
            if self._verify_answer(extracted_answer, a):
                cot_data.append({
                    "question": q,
                    "reasoning": reasoning,
                    "answer": a
                })

        return cot_data

    def generate_diverse_solutions(
        self,
        question: str,
        answer: str,
        n_solutions: int = 5
    ) -> List[str]:
        """
        为同一问题生成多种解法
        增加训练数据多样性
        """
        solutions = []

        approaches = [
            "请用代数方法解答",
            "请用几何直观解答",
            "请用逆向思维解答",
            "请用分类讨论解答",
            "请用特殊值法验证后给出通解"
        ]

        for approach in approaches[:n_solutions]:
            prompt = f"""问题：{question}

{approach}，展示完整推理过程，最后用\\boxed{{}}给出答案。"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )

            solution = response.choices[0].message.content
            if self._verify_answer(self._extract_answer(solution), answer):
                solutions.append(solution)

        return solutions
```

---

## 二、Agent能力训练

### 2.1 Agent能力体系

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Agent能力体系                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                           ┌─────────────┐                                   │
│                           │   规划能力   │                                   │
│                           │  Planning   │                                   │
│                           └──────┬──────┘                                   │
│                                  │                                          │
│              ┌───────────────────┼───────────────────┐                      │
│              │                   │                   │                      │
│              ▼                   ▼                   ▼                      │
│       ┌─────────────┐     ┌─────────────┐     ┌─────────────┐              │
│       │  工具调用    │     │  记忆管理   │     │  反思纠错   │              │
│       │ Tool Use    │     │   Memory    │     │ Reflection  │              │
│       └─────────────┘     └─────────────┘     └─────────────┘              │
│                                                                             │
│  ═══════════════════════════════════════════════════════════════════════   │
│                                                                             │
│  训练方法：                                                                 │
│                                                                             │
│  1. Tool Use SFT      - 工具调用格式学习                                    │
│  2. ReAct Training    - 推理+行动交替训练                                   │
│  3. Trajectory RL     - 完整轨迹强化学习                                    │
│  4. Self-Play         - Agent自我对弈提升                                   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.2 工具调用训练

```python
"""
工具调用（Function Calling）训练
"""
from typing import List, Dict, Any
import json

# 工具定义格式
TOOL_DEFINITIONS = [
    {
        "name": "search_product",
        "description": "搜索产品信息",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "搜索关键词"},
                "category": {"type": "string", "description": "产品类别"}
            },
            "required": ["query"]
        }
    },
    {
        "name": "check_inventory",
        "description": "检查库存",
        "parameters": {
            "type": "object",
            "properties": {
                "product_id": {"type": "string"},
                "location": {"type": "string"}
            },
            "required": ["product_id"]
        }
    },
    {
        "name": "create_order",
        "description": "创建订单",
        "parameters": {
            "type": "object",
            "properties": {
                "product_id": {"type": "string"},
                "quantity": {"type": "integer"},
                "customer_id": {"type": "string"}
            },
            "required": ["product_id", "quantity", "customer_id"]
        }
    }
]


class ToolCallDataGenerator:
    """工具调用训练数据生成器"""

    def __init__(self, tools: List[Dict]):
        self.tools = tools
        self.tool_names = [t["name"] for t in tools]

    def generate_sft_sample(
        self,
        user_query: str,
        expected_tool: str,
        expected_args: Dict,
        tool_result: str,
        final_response: str
    ) -> Dict:
        """
        生成工具调用SFT样本

        格式：用户问题 -> 工具调用 -> 工具结果 -> 最终回复
        """
        # 构造系统提示
        system_prompt = f"""你是一个销售助手，可以使用以下工具：

{json.dumps(self.tools, ensure_ascii=False, indent=2)}

当需要使用工具时，请使用以下格式：
<tool_call>
{{"name": "工具名", "arguments": {{参数}}}}
</tool_call>

收到工具结果后，请根据结果回答用户。"""

        # 构造对话
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query},
            {"role": "assistant", "content": f"""我需要查询相关信息来回答您的问题。

<tool_call>
{json.dumps({"name": expected_tool, "arguments": expected_args}, ensure_ascii=False)}
</tool_call>"""},
            {"role": "tool", "content": tool_result, "name": expected_tool},
            {"role": "assistant", "content": final_response}
        ]

        return {
            "messages": messages,
            "tools": self.tools
        }

    def generate_multi_tool_sample(
        self,
        user_query: str,
        tool_sequence: List[Dict]  # [{"tool": ..., "args": ..., "result": ...}, ...]
    ) -> Dict:
        """
        生成多工具调用样本

        训练模型学习工具链式调用
        """
        system_prompt = self._build_system_prompt()

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]

        for i, tool_call in enumerate(tool_sequence):
            # Assistant决定调用工具
            reasoning = f"第{i+1}步，我需要{tool_call.get('reasoning', '获取信息')}。"
            messages.append({
                "role": "assistant",
                "content": f"""{reasoning}

<tool_call>
{json.dumps({"name": tool_call["tool"], "arguments": tool_call["args"]}, ensure_ascii=False)}
</tool_call>"""
            })

            # 工具返回结果
            messages.append({
                "role": "tool",
                "content": tool_call["result"],
                "name": tool_call["tool"]
            })

        # 最终回复
        messages.append({
            "role": "assistant",
            "content": tool_sequence[-1].get("final_response", "任务完成。")
        })

        return {"messages": messages, "tools": self.tools}


class ToolCallEvaluator:
    """工具调用评估器"""

    def evaluate(
        self,
        predicted_call: Dict,
        expected_call: Dict
    ) -> Dict:
        """评估工具调用正确性"""
        results = {
            "tool_name_correct": False,
            "args_correct": False,
            "overall_correct": False
        }

        # 工具名称
        if predicted_call.get("name") == expected_call.get("name"):
            results["tool_name_correct"] = True

        # 参数
        pred_args = predicted_call.get("arguments", {})
        exp_args = expected_call.get("arguments", {})

        if pred_args == exp_args:
            results["args_correct"] = True
        else:
            # 部分匹配
            matching_args = sum(
                1 for k, v in exp_args.items()
                if pred_args.get(k) == v
            )
            results["args_match_rate"] = matching_args / len(exp_args) if exp_args else 1.0

        results["overall_correct"] = (
            results["tool_name_correct"] and results["args_correct"]
        )

        return results
```

### 2.3 ReAct训练

```python
"""
ReAct (Reasoning + Acting) 训练
让模型学会交替进行推理和行动
"""

REACT_PROMPT_TEMPLATE = """你是一个能够使用工具的AI助手。

可用工具：
{tools}

请按照以下格式回答问题：

Thought: 思考当前应该做什么
Action: 选择要使用的工具
Action Input: 工具的输入参数
Observation: 工具返回的结果
... (可以重复多次)
Thought: 我现在知道最终答案了
Final Answer: 最终回答

问题：{question}
"""

class ReActDataGenerator:
    """ReAct训练数据生成器"""

    def generate_react_trajectory(
        self,
        question: str,
        trajectory: List[Dict]
    ) -> str:
        """
        生成ReAct轨迹数据

        trajectory格式：
        [
            {"thought": "...", "action": "...", "action_input": "...", "observation": "..."},
            ...
            {"thought": "...", "final_answer": "..."}
        ]
        """
        output = ""

        for step in trajectory:
            if "action" in step:
                output += f"""Thought: {step['thought']}
Action: {step['action']}
Action Input: {step['action_input']}
Observation: {step['observation']}
"""
            else:
                output += f"""Thought: {step['thought']}
Final Answer: {step['final_answer']}
"""

        return output

    def generate_from_logs(
        self,
        interaction_logs: List[Dict]
    ) -> List[Dict]:
        """
        从真实交互日志生成训练数据

        这是最高质量的数据来源
        """
        training_data = []

        for log in interaction_logs:
            if log.get("success", False):  # 只用成功的轨迹
                training_data.append({
                    "question": log["question"],
                    "react_trajectory": self.generate_react_trajectory(
                        log["question"],
                        log["trajectory"]
                    ),
                    "tools_used": log.get("tools_used", [])
                })

        return training_data


class AgentTrajectoryRL:
    """Agent轨迹级别的强化学习"""

    def __init__(self, reward_model):
        self.reward_model = reward_model

    def compute_trajectory_reward(
        self,
        question: str,
        trajectory: List[Dict],
        final_answer: str,
        ground_truth: str
    ) -> float:
        """
        计算完整轨迹的奖励

        考虑因素：
        1. 最终答案正确性
        2. 工具使用效率
        3. 推理质量
        """
        reward = 0.0

        # 1. 答案正确性（主要奖励）
        if self._check_answer_correct(final_answer, ground_truth):
            reward += 1.0
        else:
            reward -= 0.5

        # 2. 工具使用效率
        n_tool_calls = sum(1 for t in trajectory if "action" in t)
        if n_tool_calls <= 3:  # 高效
            reward += 0.2
        elif n_tool_calls > 5:  # 低效
            reward -= 0.1

        # 3. 推理质量
        thoughts = [t.get("thought", "") for t in trajectory]
        thought_quality = self._evaluate_thought_quality(thoughts)
        reward += thought_quality * 0.3

        return reward

    def _evaluate_thought_quality(self, thoughts: List[str]) -> float:
        """评估推理质量"""
        # 简化实现：检查推理的连贯性和相关性
        quality = 0.0

        for thought in thoughts:
            if len(thought) > 20:  # 不是太短
                quality += 0.2
            if any(kw in thought for kw in ["因为", "所以", "需要", "应该"]):
                quality += 0.1

        return min(1.0, quality)
```

### 2.4 Agent训练最佳实践

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     Agent训练检查清单                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  数据准备                                                                   │
│  □ 工具定义清晰、无歧义                                                     │
│  □ 训练数据覆盖单工具和多工具场景                                           │
│  □ 包含工具调用失败和重试的样本                                             │
│  □ 包含不需要工具的纯对话样本（避免过度调用）                               │
│                                                                             │
│  训练策略                                                                   │
│  □ 先SFT学习格式，再RL优化效果                                              │
│  □ 工具调用准确率 > 90% 才进入RL                                            │
│  □ RL时使用真实工具执行结果作为反馈                                         │
│                                                                             │
│  评估指标                                                                   │
│  □ 工具选择准确率                                                           │
│  □ 参数提取准确率                                                           │
│  □ 任务完成率                                                               │
│  □ 工具调用效率（越少越好，在正确的前提下）                                 │
│                                                                             │
│  常见问题                                                                   │
│  □ 过度调用工具 → 增加"不需要工具"的负样本                                 │
│  □ 工具幻觉（调用不存在的工具）→ 严格限制工具列表                           │
│  □ 参数格式错误 → 增加格式校验和错误恢复训练                                │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 三、长上下文扩展训练

### 3.1 长上下文挑战

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     长上下文训练挑战                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. 位置编码外推问题                                                        │
│     原始RoPE在超出训练长度后性能急剧下降                                    │
│                                                                             │
│  2. 注意力稀释 (Attention Dilution)                                         │
│     上下文变长后，关键信息的注意力被稀释                                    │
│                                                                             │
│  3. Lost in the Middle                                                      │
│     模型倾向于关注开头和结尾，忽略中间内容                                  │
│                                                                             │
│  4. 计算和显存压力                                                          │
│     Self-attention是O(n²)复杂度                                             │
│                                                                             │
│  5. 长文本训练数据稀缺                                                      │
│     高质量的超长文档较少                                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 位置编码扩展方法

```python
"""
长上下文位置编码扩展技术
"""
import torch
import math

class RoPEScaling:
    """RoPE位置编码扩展方法"""

    @staticmethod
    def linear_scaling(
        position_ids: torch.Tensor,
        scale_factor: float
    ) -> torch.Tensor:
        """
        线性缩放 - 最简单的方法

        将位置ID除以scale_factor
        例如：训练长度4k，想扩展到16k，scale_factor=4
        """
        return position_ids / scale_factor

    @staticmethod
    def ntk_scaling(
        dim: int,
        base: float = 10000,
        scale_factor: float = 1.0
    ) -> torch.Tensor:
        """
        NTK-aware Scaling (Neural Tangent Kernel)

        调整RoPE的base，保持高频信息
        比线性缩放效果更好
        """
        # 调整base
        new_base = base * (scale_factor ** (dim / (dim - 2)))

        # 计算新的频率
        inv_freq = 1.0 / (new_base ** (torch.arange(0, dim, 2).float() / dim))

        return inv_freq

    @staticmethod
    def yarn_scaling(
        dim: int,
        original_max_pos: int,
        target_max_pos: int,
        base: float = 10000,
        beta_fast: float = 32,
        beta_slow: float = 1
    ) -> torch.Tensor:
        """
        YaRN (Yet another RoPE extensioN)

        结合NTK和线性插值，根据频率分段处理
        - 高频：线性插值
        - 低频：NTK缩放
        """
        scale = target_max_pos / original_max_pos

        # 计算每个维度的缩放因子
        freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))

        # 分段处理
        low_freq_factor = []
        high_freq_factor = []

        for i, f in enumerate(freq):
            wavelength = 2 * math.pi / f
            if wavelength < original_max_pos / beta_fast:
                # 高频：不缩放
                low_freq_factor.append(1.0)
            elif wavelength > original_max_pos / beta_slow:
                # 低频：完全缩放
                low_freq_factor.append(scale)
            else:
                # 中间：插值
                smooth = (wavelength - original_max_pos / beta_fast) / \
                        (original_max_pos / beta_slow - original_max_pos / beta_fast)
                low_freq_factor.append((1 - smooth) + smooth * scale)

        return torch.tensor(low_freq_factor)


class LongContextTrainer:
    """长上下文训练器"""

    def __init__(
        self,
        model,
        original_max_length: int = 4096,
        target_max_length: int = 32768,
        scaling_method: str = "yarn"
    ):
        self.model = model
        self.original_max_length = original_max_length
        self.target_max_length = target_max_length
        self.scaling_method = scaling_method

        # 应用位置编码扩展
        self._apply_rope_scaling()

    def _apply_rope_scaling(self):
        """应用RoPE缩放"""
        scale_factor = self.target_max_length / self.original_max_length

        # 修改模型配置
        self.model.config.rope_scaling = {
            "type": self.scaling_method,
            "factor": scale_factor
        }

        # 更新max_position_embeddings
        self.model.config.max_position_embeddings = self.target_max_length

    def progressive_training(
        self,
        dataset,
        length_stages: List[int] = [8192, 16384, 32768]
    ):
        """
        渐进式长度训练

        从短到长逐步增加上下文长度
        """
        for stage_length in length_stages:
            print(f"Training stage: {stage_length} tokens")

            # 过滤合适长度的数据
            stage_data = [
                d for d in dataset
                if len(d["input_ids"]) <= stage_length
            ]

            # 训练
            self._train_stage(stage_data, stage_length)

            # 评估
            eval_result = self._evaluate_length(stage_length)
            print(f"Stage {stage_length} eval: {eval_result}")

    def _train_stage(self, data, max_length):
        """训练单个阶段"""
        # 配置训练参数
        training_args = {
            "max_length": max_length,
            "learning_rate": 2e-5,  # 长上下文用较小学习率
            "num_train_epochs": 1,
            "gradient_checkpointing": True,  # 必须开启
            "per_device_train_batch_size": 1,  # batch size通常为1
            "gradient_accumulation_steps": 16,
        }

        # 训练
        pass  # 实现略
```

### 3.3 长上下文数据构造

```python
"""
长上下文训练数据构造
"""
class LongContextDataGenerator:
    """长上下文数据生成器"""

    def __init__(self):
        pass

    def generate_needle_in_haystack(
        self,
        context_length: int,
        needle: str,
        needle_position: float = 0.5  # 0-1，表示needle在文档中的位置
    ) -> Dict:
        """
        大海捞针测试数据

        在长文本中插入关键信息，测试检索能力
        """
        # 生成填充文本
        filler_text = self._generate_filler(context_length - len(needle))

        # 插入needle
        insert_pos = int(len(filler_text) * needle_position)
        full_text = filler_text[:insert_pos] + needle + filler_text[insert_pos:]

        # 生成问题
        question = self._generate_question_about_needle(needle)

        return {
            "context": full_text,
            "question": question,
            "answer": needle,
            "needle_position": needle_position
        }

    def generate_multi_doc_qa(
        self,
        documents: List[str],
        question: str,
        relevant_doc_indices: List[int]
    ) -> Dict:
        """
        多文档问答数据

        模拟真实的长文档理解场景
        """
        # 拼接文档
        full_context = ""
        for i, doc in enumerate(documents):
            full_context += f"\n\n=== 文档 {i+1} ===\n{doc}"

        return {
            "context": full_context,
            "question": question,
            "relevant_docs": relevant_doc_indices,
            "n_documents": len(documents)
        }

    def generate_long_form_completion(
        self,
        prompt: str,
        target_length: int
    ) -> Dict:
        """
        长文本生成数据

        训练模型生成长且连贯的内容
        """
        # 使用强模型生成
        response = generate_long_response(prompt, target_length)

        return {
            "prompt": prompt,
            "completion": response,
            "target_length": target_length
        }

    def _generate_filler(self, length: int) -> str:
        """生成填充文本"""
        # 使用维基百科或其他无关文本
        filler_sources = [
            "这是一段关于历史的文字...",
            "科学研究表明...",
            # ... 更多填充模板
        ]

        filler = ""
        while len(filler) < length:
            filler += random.choice(filler_sources)

        return filler[:length]
```

---

## 四、模型合并（Model Merging）

### 4.1 模型合并概述

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        模型合并技术概览                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  应用场景：                                                                 │
│  - 合并不同领域微调的模型，获得多领域能力                                   │
│  - 合并不同任务的checkpoint，平衡多任务性能                                 │
│  - 不需要额外训练，零成本获得能力组合                                       │
│                                                                             │
│  主要方法：                                                                 │
│  ┌─────────────┬─────────────────────────────────────────────────────────┐ │
│  │   方法      │   特点                                                   │ │
│  ├─────────────┼─────────────────────────────────────────────────────────┤ │
│  │ Average     │ 简单平均，baseline方法                                   │ │
│  │ SLERP       │ 球面线性插值，保持权重方向                               │ │
│  │ TIES        │ 剪枝+符号解决冲突，处理参数冲突                          │ │
│  │ DARE        │ 随机丢弃+重缩放，减少干扰                                │ │
│  │ Task Vector │ 任务向量算术，可组合/相减                                │ │
│  └─────────────┴─────────────────────────────────────────────────────────┘ │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.2 合并方法实现

```python
"""
模型合并方法实现
"""
import torch
import numpy as np
from typing import List, Dict
from collections import OrderedDict

class ModelMerger:
    """模型合并器"""

    @staticmethod
    def average_merge(
        state_dicts: List[OrderedDict],
        weights: List[float] = None
    ) -> OrderedDict:
        """
        加权平均合并

        最简单的方法，但对参数冲突处理不好
        """
        if weights is None:
            weights = [1.0 / len(state_dicts)] * len(state_dicts)

        assert len(weights) == len(state_dicts)
        assert abs(sum(weights) - 1.0) < 1e-6

        merged = OrderedDict()

        for key in state_dicts[0].keys():
            merged[key] = sum(
                w * sd[key].float()
                for w, sd in zip(weights, state_dicts)
            )

        return merged

    @staticmethod
    def slerp_merge(
        state_dict_1: OrderedDict,
        state_dict_2: OrderedDict,
        t: float = 0.5
    ) -> OrderedDict:
        """
        SLERP (Spherical Linear Interpolation) 合并

        在球面上插值，保持权重向量的方向特性
        比简单平均效果更好
        """
        def slerp(v0, v1, t):
            """球面线性插值"""
            v0_flat = v0.flatten().float()
            v1_flat = v1.flatten().float()

            # 归一化
            v0_norm = v0_flat / (v0_flat.norm() + 1e-8)
            v1_norm = v1_flat / (v1_flat.norm() + 1e-8)

            # 计算夹角
            dot = torch.clamp(torch.dot(v0_norm, v1_norm), -1.0, 1.0)
            theta = torch.acos(dot)

            # 如果角度太小，退化为线性插值
            if theta.abs() < 1e-6:
                return (1 - t) * v0 + t * v1

            # SLERP公式
            sin_theta = torch.sin(theta)
            result = (
                torch.sin((1 - t) * theta) / sin_theta * v0_flat +
                torch.sin(t * theta) / sin_theta * v1_flat
            )

            return result.reshape(v0.shape)

        merged = OrderedDict()

        for key in state_dict_1.keys():
            merged[key] = slerp(state_dict_1[key], state_dict_2[key], t)

        return merged

    @staticmethod
    def ties_merge(
        base_state_dict: OrderedDict,
        finetuned_state_dicts: List[OrderedDict],
        density: float = 0.5,
        majority_sign: bool = True
    ) -> OrderedDict:
        """
        TIES (Trim, Elect Sign, Merge) 合并

        1. Trim: 剪枝掉小的变化
        2. Elect Sign: 解决符号冲突
        3. Merge: 合并剩余参数

        论文: https://arxiv.org/abs/2306.01708
        """
        # 计算task vectors (微调后相对于base的变化)
        task_vectors = []
        for ft_sd in finetuned_state_dicts:
            tv = OrderedDict()
            for key in base_state_dict.keys():
                tv[key] = ft_sd[key] - base_state_dict[key]
            task_vectors.append(tv)

        merged_tv = OrderedDict()

        for key in base_state_dict.keys():
            # 收集所有task vector在这个key上的值
            vectors = torch.stack([tv[key] for tv in task_vectors])

            # Step 1: Trim - 保留top-k%的参数
            abs_vectors = vectors.abs()
            threshold = torch.quantile(abs_vectors, 1 - density)
            mask = abs_vectors >= threshold
            trimmed = vectors * mask

            # Step 2: Elect Sign - 多数投票决定符号
            if majority_sign:
                signs = torch.sign(trimmed)
                # 统计每个位置的符号
                sign_sum = signs.sum(dim=0)
                elected_sign = torch.sign(sign_sum)
                # 只保留符合多数符号的值
                sign_mask = (signs == elected_sign.unsqueeze(0)) | (signs == 0)
                trimmed = trimmed * sign_mask

            # Step 3: Merge - 平均非零值
            non_zero_count = (trimmed != 0).sum(dim=0).clamp(min=1)
            merged_tv[key] = trimmed.sum(dim=0) / non_zero_count

        # 应用merged task vector到base
        merged = OrderedDict()
        for key in base_state_dict.keys():
            merged[key] = base_state_dict[key] + merged_tv[key]

        return merged

    @staticmethod
    def dare_merge(
        base_state_dict: OrderedDict,
        finetuned_state_dicts: List[OrderedDict],
        drop_rate: float = 0.9,
        rescale: bool = True
    ) -> OrderedDict:
        """
        DARE (Drop And REscale) 合并

        随机丢弃大部分参数变化，然后重新缩放
        减少模型间的干扰

        论文: https://arxiv.org/abs/2311.03099
        """
        # 计算task vectors
        task_vectors = []
        for ft_sd in finetuned_state_dicts:
            tv = OrderedDict()
            for key in base_state_dict.keys():
                delta = ft_sd[key] - base_state_dict[key]

                # 随机丢弃
                mask = torch.rand_like(delta.float()) > drop_rate
                dropped = delta * mask

                # 重新缩放
                if rescale:
                    dropped = dropped / (1 - drop_rate)

                tv[key] = dropped
            task_vectors.append(tv)

        # 合并task vectors
        merged = OrderedDict()
        for key in base_state_dict.keys():
            merged_delta = sum(tv[key] for tv in task_vectors) / len(task_vectors)
            merged[key] = base_state_dict[key] + merged_delta

        return merged


class TaskVectorArithmetic:
    """任务向量算术"""

    @staticmethod
    def compute_task_vector(
        base_state_dict: OrderedDict,
        finetuned_state_dict: OrderedDict
    ) -> OrderedDict:
        """计算任务向量 = 微调后 - 基座"""
        tv = OrderedDict()
        for key in base_state_dict.keys():
            tv[key] = finetuned_state_dict[key] - base_state_dict[key]
        return tv

    @staticmethod
    def apply_task_vector(
        base_state_dict: OrderedDict,
        task_vector: OrderedDict,
        scaling: float = 1.0
    ) -> OrderedDict:
        """应用任务向量"""
        result = OrderedDict()
        for key in base_state_dict.keys():
            result[key] = base_state_dict[key] + scaling * task_vector[key]
        return result

    @staticmethod
    def add_task_vectors(
        task_vectors: List[OrderedDict],
        weights: List[float] = None
    ) -> OrderedDict:
        """
        加法：组合多个能力

        例如：数学能力 + 代码能力 = 数学+代码能力
        """
        if weights is None:
            weights = [1.0] * len(task_vectors)

        result = OrderedDict()
        for key in task_vectors[0].keys():
            result[key] = sum(
                w * tv[key] for w, tv in zip(weights, task_vectors)
            )
        return result

    @staticmethod
    def negate_task_vector(
        task_vector: OrderedDict,
        scaling: float = 1.0
    ) -> OrderedDict:
        """
        取反：移除某种能力/特性

        例如：-有毒内容向量 = 减少有毒输出
        """
        result = OrderedDict()
        for key in task_vector.keys():
            result[key] = -scaling * task_vector[key]
        return result
```

### 4.3 模型合并实践指南

```python
"""
模型合并实践示例
"""
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def merge_domain_experts(
    base_model_path: str,
    expert_model_paths: List[str],
    method: str = "ties",
    output_path: str = "./merged_model"
):
    """
    合并多个领域专家模型

    场景：有数学、代码、写作三个微调模型，想要一个全能模型
    """
    print("Loading models...")

    # 加载base模型
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16
    )
    base_sd = base_model.state_dict()

    # 加载专家模型
    expert_sds = []
    for path in expert_model_paths:
        expert = AutoModelForCausalLM.from_pretrained(
            path,
            torch_dtype=torch.float16
        )
        expert_sds.append(expert.state_dict())
        del expert  # 释放显存

    print(f"Merging with {method} method...")

    merger = ModelMerger()

    if method == "average":
        merged_sd = merger.average_merge(expert_sds)
    elif method == "slerp":
        # SLERP只支持两个模型
        merged_sd = merger.slerp_merge(expert_sds[0], expert_sds[1], t=0.5)
    elif method == "ties":
        merged_sd = merger.ties_merge(
            base_sd, expert_sds,
            density=0.5,
            majority_sign=True
        )
    elif method == "dare":
        merged_sd = merger.dare_merge(
            base_sd, expert_sds,
            drop_rate=0.9
        )

    # 保存合并后的模型
    base_model.load_state_dict(merged_sd)
    base_model.save_pretrained(output_path)

    # 保存tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    tokenizer.save_pretrained(output_path)

    print(f"Merged model saved to {output_path}")


def evaluate_merged_model(
    merged_model_path: str,
    eval_tasks: List[str]
) -> Dict:
    """
    评估合并后的模型

    确保各领域能力都保留
    """
    results = {}

    model = AutoModelForCausalLM.from_pretrained(merged_model_path)
    tokenizer = AutoTokenizer.from_pretrained(merged_model_path)

    for task in eval_tasks:
        if task == "math":
            score = evaluate_math(model, tokenizer)
        elif task == "code":
            score = evaluate_code(model, tokenizer)
        elif task == "writing":
            score = evaluate_writing(model, tokenizer)

        results[task] = score

    # 计算综合分数
    results["overall"] = sum(results.values()) / len(results)

    return results


# 使用mergekit进行合并（推荐）
MERGEKIT_CONFIG = """
# mergekit配置示例
models:
  - model: ./math_expert
    parameters:
      weight: 0.4
  - model: ./code_expert
    parameters:
      weight: 0.3
  - model: ./writing_expert
    parameters:
      weight: 0.3

merge_method: ties
base_model: ./base_model
parameters:
  density: 0.5
  majority_sign: true
dtype: float16
"""
```

### 4.4 模型合并注意事项

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     模型合并最佳实践                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ✅ 推荐做法                                                                │
│  ━━━━━━━━━━                                                                 │
│  • 使用相同base模型微调的模型进行合并                                       │
│  • 合并前确保各模型在各自任务上效果良好                                     │
│  • 从小规模实验开始，验证合并效果                                           │
│  • 使用TIES或DARE处理参数冲突                                               │
│  • 合并后进行全面评估                                                       │
│                                                                             │
│  ❌ 避免做法                                                                │
│  ━━━━━━━━━━                                                                 │
│  • 合并架构不同的模型                                                       │
│  • 合并差异极大的模型（如中文和英文专精）                                   │
│  • 不评估就上线                                                             │
│  • 忽略各领域的能力退化                                                     │
│                                                                             │
│  常见问题                                                                   │
│  ━━━━━━━━                                                                   │
│  Q: 合并后某领域能力下降严重？                                              │
│  A: 增加该领域模型的权重，或使用TIES减少冲突                                │
│                                                                             │
│  Q: 合并后模型输出质量整体下降？                                            │
│  A: 可能是模型差异太大，尝试只合并部分层                                    │
│                                                                             │
│  Q: 该用哪种合并方法？                                                      │
│  A: TIES通常最稳定，DARE适合合并多个模型，SLERP适合两个相近模型             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 五、本章小结

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        前沿技术总结                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  推理增强训练                                                               │
│  • RLVR使用可验证奖励训练推理能力                                           │
│  • DeepSeek-R1证明纯RL可以涌现推理                                          │
│  • 注意RLVR的Hacking风险，需要奖励塑造                                      │
│                                                                             │
│  Agent能力训练                                                              │
│  • 工具调用：SFT学格式 + RL优化效果                                         │
│  • ReAct：推理与行动交替训练                                                │
│  • 轨迹级RL：完整任务流程优化                                               │
│                                                                             │
│  长上下文扩展                                                               │
│  • YaRN/NTK进行位置编码扩展                                                 │
│  • 渐进式训练：短到长逐步扩展                                               │
│  • 注意Lost-in-the-Middle问题                                               │
│                                                                             │
│  模型合并                                                                   │
│  • TIES/DARE处理参数冲突                                                    │
│  • 任务向量支持能力组合/移除                                                │
│  • 零训练成本获得多领域能力                                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 参考资源

### 论文
- [DeepSeek-R1](https://arxiv.org/abs/2501.12948) - 纯RL推理训练
- [TIES-Merging](https://arxiv.org/abs/2306.01708) - 模型合并
- [DARE](https://arxiv.org/abs/2311.03099) - 模型合并
- [YaRN](https://arxiv.org/abs/2309.00071) - 长上下文扩展
- [ReAct](https://arxiv.org/abs/2210.03629) - Agent推理框架

### 工具
- [mergekit](https://github.com/cg123/mergekit) - 模型合并工具
- [LongRoPE](https://github.com/microsoft/LongRoPE) - 长上下文扩展

---

> **下一章**：返回 [00-概述与全景图.md](./00-概述与全景图.md) 复习完整技术栈
