# 大模型后训练完全指南：概述与全景图

> **目标读者**：希望系统掌握大模型后训练（Post-Training）技术的工程师和研究者
>
> **学习成果**：理解后训练的完整链路，能够独立完成从数据准备到模型部署的全流程
>
> **贯穿案例**：训练一个专业的销售对话LLM（SalesGPT）

---

## 一、什么是后训练（Post-Training）？

### 1.1 定义

**后训练**是指在预训练（Pre-Training）完成后，对基座模型进行的一系列优化和调整，使其从"通用文本预测器"转变为"可靠、对齐、专业的AI助手"。

```
┌─────────────────────────────────────────────────────────────────────┐
│                        大模型训练全流程                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   预训练 (Pre-Training)          后训练 (Post-Training)              │
│   ━━━━━━━━━━━━━━━━━━━           ━━━━━━━━━━━━━━━━━━━━━━              │
│                                                                     │
│   ┌─────────────┐               ┌─────────────────────────────────┐│
│   │             │               │  CPT → SFT → RM → DPO/RLHF      ││
│   │  海量文本   │  ──────────▶  │                                 ││
│   │  自监督学习 │               │  领域适配 → 指令遵循 → 偏好对齐  ││
│   │             │               │                                 ││
│   └─────────────┘               └─────────────────────────────────┘│
│                                                                     │
│   产出：Base Model               产出：Chat/Instruct Model          │
│   能力：文本续写                 能力：对话、推理、工具调用          │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.2 为什么后训练如此重要？

> "If pre-training built the engine, post-training is where we tune it for the track."
> — 业界共识

**OpenAI的数据**表明：
- 预训练占用了98%的计算资源
- 但**后训练才是解锁模型能力的关键**
- RLHF的效果 > 100倍参数量增长

**Thomas Scialom（Meta Llama 3 对齐负责人）**：
> "RLHF is so much more scalable. It costs less, it's easier, and leads to better performance. I would spend 100% of my alignment data budget on preferences rather than on instructions."

### 1.3 后训练的演进时间线

| 年份 | 里程碑 | 关键技术 |
|------|--------|----------|
| 2022 | InstructGPT | SFT + RLHF (PPO) |
| 2022 | Constitutional AI | RLAIF |
| 2023 | DPO | 无需RM的直接偏好优化 |
| 2024 | Llama 3 | 大规模多阶段后训练 |
| 2025 | DeepSeek-R1 | GRPO + 纯RL推理涌现 |
| 2025 | Qwen3 | GSPO（序列级优化）|

---

## 二、后训练技术全景图

### 2.1 完整技术栈

```
                          后训练技术全景图
                          ================

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌──────────┐ │
│  │ 继续预训练   │    │  监督微调   │    │  偏好对齐   │    │ 强化学习  │ │
│  │    (CPT)    │ ─▶ │   (SFT)    │ ─▶ │ (DPO/ORPO) │ ─▶ │(RLHF/GRPO)│ │
│  └─────────────┘    └─────────────┘    └─────────────┘    └──────────┘ │
│        │                  │                  │                  │      │
│        ▼                  ▼                  ▼                  ▼      │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌──────────┐ │
│  │ 领域知识注入 │    │ 指令遵循能力 │    │ 人类偏好对齐 │    │ 持续优化  │ │
│  │ 专业术语理解 │    │ 格式规范输出 │    │ 安全性提升   │    │ 推理增强  │ │
│  └─────────────┘    └─────────────┘    └─────────────┘    └──────────┘ │
│                                                                         │
│  ════════════════════════════════════════════════════════════════════  │
│                              数据工程基础                                │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │ 数据收集  │  │ 数据清洗  │  │ 数据标注  │  │ 数据增强  │  │ 质量评估  │  │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘  └──────────┘  │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.2 各阶段详解

#### 阶段一：继续预训练 (Continual Pre-Training, CPT)

**目的**：注入领域知识，适配特定语言/领域

**输入**：大规模无标注领域文本（数十GB~数TB）

**输出**：领域增强的Base Model

**适用场景**：
- 金融、医疗、法律等专业领域
- 小语种适配
- 企业私有知识库

**销售LLM示例**：
```
输入数据：
- 销售培训手册 (PDF → 文本)
- 历史成交对话记录
- 产品说明书
- 行业分析报告
- CRM系统中的客户互动记录
```

#### 阶段二：监督微调 (Supervised Fine-Tuning, SFT)

**目的**：教会模型遵循指令，掌握对话格式

**输入**：高质量的指令-回复对（数千~数万条）

**输出**：能够理解指令并生成格式化回复的模型

**核心能力培养**：
- 指令理解与遵循
- 多轮对话管理
- 格式规范（JSON/Markdown等）
- 角色扮演

**销售LLM示例**：
```json
{
  "instruction": "你是一个专业的销售顾问，客户询问产品价格时，你需要先了解客户需求，再给出合适的方案。",
  "input": "你们这个产品多少钱？",
  "output": "感谢您的咨询！为了给您推荐最适合的方案，我想先了解几个问题：\n\n1. 您主要想解决什么问题？\n2. 您的预算范围大概是多少？\n3. 您希望多快能够上线使用？\n\n了解这些信息后，我可以为您定制最优的解决方案。"
}
```

#### 阶段三：奖励模型训练 (Reward Modeling, RM)

**目的**：训练一个"评判官"，能够评估回复质量

**输入**：偏好数据（chosen vs rejected pairs）

**输出**：能够给回复打分的Reward Model

**关键作用**：为后续RLHF/GRPO提供奖励信号

**销售LLM示例**：
```
Prompt: "你们的产品和竞争对手比有什么优势？"

Chosen (高分回复):
"这是个很好的问题！让我从三个维度来分析：
1. 技术层面：我们采用了xxx专利技术，响应速度提升40%
2. 服务层面：提供7x24小时专属客服支持
3. 成本层面：按需付费，比传统方案节省约30%
您最关心哪个方面？我可以详细介绍。"

Rejected (低分回复):
"我们的产品是最好的，比其他家都强。你直接买就行了。"
```

#### 阶段四：偏好对齐 (Preference Alignment)

**方法A：DPO (Direct Preference Optimization)**
- 直接使用偏好数据优化模型
- 无需训练单独的RM
- 简单高效，适合快速迭代

**方法B：RLHF (Reinforcement Learning from Human Feedback)**
- 使用RM作为奖励信号
- PPO算法优化策略
- 效果上限更高，但复杂度高

**方法C：GRPO (Group Relative Policy Optimization)**
- DeepSeek提出的高效RL方法
- 无需Critic模型
- 组内相对比较计算奖励

#### 阶段五：强化推理优化 (RLVR) [可选]

**目的**：增强模型的推理和规划能力

**适用场景**：数学、代码、复杂逻辑任务

**方法**：
- 可验证奖励（数学正确性、代码执行结果）
- Chain-of-Thought训练
- 测试时计算扩展

---

## 三、领域后训练决策框架

> ⚠️ **关键决策**：在开始训练之前，必须明确回答以下问题。
> 错误的技术路径会导致数周的工作白费！

### 3.1 需求澄清模板（项目启动前必填）

```yaml
# 领域后训练需求澄清清单
# =============================

# 1. 领域边界
domain:
  name: "销售对话"
  scope: "B2B企业软件销售"
  out_of_scope: "C端零售、售后客服"

# 2. 知识需求
knowledge:
  required_facts:  # 必须答对的事实
    - "产品价格、功能、规格"
    - "竞品对比信息"
    - "公司政策（退换货、保修）"
  knowledge_source: "产品手册、CRM系统、培训材料"
  update_frequency: "每月更新"  # 知识更新频率

# 3. 行为规范
behavior:
  allowed_styles: ["专业友好", "顾问式销售"]
  prohibited_actions:
    - "贬低竞品"
    - "泄露客户隐私"
    - "承诺无法兑现的优惠"
    - "跳过需求了解直接报价"
  safety_requirements: "严格合规，不做虚假承诺"

# 4. 输出形态
deployment:
  type: "纯模型"  # 纯模型 / 模型+RAG / 模型+工具调用
  latency_requirement: "P99 < 3s"
  concurrency: "100 QPS"

# 5. 评估标准
evaluation:
  must_pass:
    - "产品信息准确率 > 98%"
    - "安全合规率 = 100%"
  nice_to_have:
    - "用户满意度 > 4.0/5.0"
    - "转化率提升 > 10%"
```

### 3.2 CPT vs RAG vs SFT 决策矩阵

> **核心问题**：领域知识应该「注入模型」还是「检索补充」？

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        知识处理方案选择决策树                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                      领域知识更新频率如何？                                  │
│                              │                                              │
│              ┌───────────────┼───────────────┐                              │
│              ▼               ▼               ▼                              │
│         高频更新         中频更新         低频/静态                          │
│        (天/周级)        (月/季度级)       (基本不变)                         │
│              │               │               │                              │
│              ▼               ▼               ▼                              │
│         ┌───────┐       ┌───────┐       ┌───────┐                          │
│         │  RAG  │       │RAG+SFT│       │CPT/SFT│                          │
│         └───────┘       └───────┘       └───────┘                          │
│                                                                             │
│  ═══════════════════════════════════════════════════════════════════════   │
│                                                                             │
│                      领域与通用语言差异多大？                                │
│                              │                                              │
│              ┌───────────────┼───────────────┐                              │
│              ▼               ▼               ▼                              │
│         差异巨大          中等差异          差异较小                         │
│   (专业术语/小语种)    (行业用语)     (日常对话风格)                        │
│              │               │               │                              │
│              ▼               ▼               ▼                              │
│         ┌───────┐       ┌───────┐       ┌───────┐                          │
│         │  CPT  │       │CPT可选│       │ 跳过CPT│                          │
│         │ 必需  │       │SFT为主│       │直接SFT│                          │
│         └───────┘       └───────┘       └───────┘                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**详细决策表**：

| 决策维度 | 选择RAG | 选择CPT | 选择纯SFT |
|----------|---------|---------|-----------|
| **知识更新** | 高频（天/周） | 低频（季度/年） | 静态或可放入prompt |
| **知识量级** | 大量（GB级文档） | 大量领域文本 | 知识量有限 |
| **推理需求** | 需要引用来源 | 内化知识推理 | 格式/风格为主 |
| **准确性要求** | 极高（可追溯） | 高（允许小概率错误） | 中等 |
| **成本考量** | 推理成本高 | 训练成本高 | 最低 |
| **典型场景** | 客服FAQ、法规查询 | 医疗、金融专业领域 | 对话风格、角色扮演 |

**销售LLM决策示例**：
```
知识更新频率: 月度（产品价格、促销活动）→ 中频
领域差异程度: 销售话术有行业特点，但非专业术语 → 中等
结论: RAG（产品库/价格库）+ SFT（销售技巧）组合方案

具体架构:
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  用户问题    │ ──▶ │  RAG检索    │ ──▶ │  SFT模型    │
│             │     │ 产品信息库   │     │ 销售对话    │
└─────────────┘     └─────────────┘     └─────────────┘
```

### 3.3 训练阶段Gate（进入/退出条件）

> 每个阶段都有明确的门槛，不满足则不应进入下一阶段

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           训练阶段Gate清单                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ CPT阶段                                                             │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │ 进入条件:                                                           │   │
│  │ □ 领域文本 > 5GB（理想 > 50GB）                                     │   │
│  │ □ 领域术语/表达与通用语料差异显著                                    │   │
│  │ □ 知识相对静态，不需要频繁更新                                       │   │
│  │                                                                     │   │
│  │ 退出条件:                                                           │   │
│  │ ✓ PPL在领域测试集上下降 > 20%                                       │   │
│  │ ✓ 领域术语填空测试准确率 > 80%                                      │   │
│  │ ✓ 通用能力（MMLU等）下降 < 5%                                       │   │
│  │                                                                     │   │
│  │ 回滚条件:                                                           │   │
│  │ ✗ 通用能力下降 > 10%                                                │   │
│  │ ✗ Loss不收敛或发散                                                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ SFT阶段                                                             │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │ 进入条件:                                                           │   │
│  │ □ 高质量SFT数据 > 1k条（理想 > 10k条）                              │   │
│  │ □ 数据格式验证通过（chat template、loss mask）                      │   │
│  │ □ 数据覆盖核心场景 > 80%                                            │   │
│  │                                                                     │   │
│  │ 退出条件:                                                           │   │
│  │ ✓ 指令遵循率 > 90%                                                  │   │
│  │ ✓ 格式正确率 > 95%                                                  │   │
│  │ ✓ 人工抽检满意度 > 3.5/5.0                                          │   │
│  │                                                                     │   │
│  │ 回滚条件:                                                           │   │
│  │ ✗ 灾难性遗忘（通用能力骤降）                                        │   │
│  │ ✗ 过拟合（train loss↓ 但 eval loss↑）                               │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ DPO/对齐阶段                                                        │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │ 进入条件:                                                           │   │
│  │ □ SFT模型已达到退出条件                                             │   │
│  │ □ 偏好数据 > 1k pairs，且chosen/rejected差异明显                    │   │
│  │ □ 偏好数据分布与SFT数据分布接近（in-distribution）                  │   │
│  │                                                                     │   │
│  │ 退出条件:                                                           │   │
│  │ ✓ Win rate vs SFT baseline > 55%                                    │   │
│  │ ✓ KL divergence < 0.5                                               │   │
│  │ ✓ 安全性评测通过率 > 98%                                            │   │
│  │                                                                     │   │
│  │ 回滚条件:                                                           │   │
│  │ ✗ KL divergence > 1.0（模型跑偏）                                   │   │
│  │ ✗ 多样性显著下降（distinct-2 < 0.3）                                │   │
│  │ ✗ Win rate反而下降                                                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ RLHF/GRPO阶段（可选）                                               │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │ 进入条件:                                                           │   │
│  │ □ DPO效果已饱和，需要进一步提升                                     │   │
│  │ □ 有高质量Reward Model或可验证奖励                                  │   │
│  │ □ 计算资源充足（RL需要4x以上资源）                                  │   │
│  │                                                                     │   │
│  │ 退出条件:                                                           │   │
│  │ ✓ Reward稳定上升且人工评估同步提升                                  │   │
│  │ ✓ 目标指标达成（如转化率提升）                                      │   │
│  │                                                                     │   │
│  │ 回滚条件:                                                           │   │
│  │ ✗ Reward hacking（reward↑ 但质量↓）                                 │   │
│  │ ✗ Entropy collapse（输出多样性消失）                                │   │
│  │ ✗ 训练不稳定（Loss震荡或NaN）                                       │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.4 资源与路径速查表

| 场景 | 数据量要求 | 推荐路径 | 预期效果 |
|------|-----------|----------|---------|
| 快速验证 | <1k条SFT数据 | Chat模型 + LoRA SFT | 基础可用 |
| 标准落地 | 5k-50k SFT + 偏好数据 | SFT → DPO | 生产级质量 |
| 深度定制 | 大量领域文本 + 标注数据 | CPT → SFT → DPO → GRPO | SOTA水平 |
| 推理增强 | 数学/代码验证数据 | SFT → RLVR | 推理能力显著提升 |

### 3.5 销售LLM完整训练路径

```
Phase 1: 数据准备 (2-4周)
├── 收集销售对话语料 (历史记录、培训材料)
├── 构建SFT数据集 (~10k高质量对话)
├── 标注偏好数据 (~5k pairs)
└── 数据清洗与格式化

Phase 2: 继续预训练 CPT (可选，1-2周)
├── 基座模型选择 (Qwen2.5-7B-Base)
├── 销售领域文本预训练
└── 验证领域知识习得

Phase 3: 监督微调 SFT (1周)
├── 多轮对话格式训练
├── 销售技巧注入
└── 格式规范化

Phase 4: 偏好对齐 DPO (3-5天)
├── 偏好数据训练
├── 安全性对齐
└── 风格一致性

Phase 5: 强化学习 GRPO (可选，1周)
├── 针对性问题优化
├── 成交率相关reward
└── 精细化调优

Phase 6: 评估与迭代 (持续)
├── A/B测试
├── 人工评估
└── 持续改进
```

---

## 四、关键概念速查

### 4.1 数据格式

| 格式名称 | 适用场景 | 结构特点 |
|---------|---------|---------|
| Alpaca | 单轮指令任务 | instruction + input + output |
| ShareGPT | 多轮对话 | conversations数组，支持多角色 |
| ChatML | 通用对话 | 角色标记（system/user/assistant）|
| Preference | DPO/RLHF | prompt + chosen + rejected |

### 4.2 参数高效微调方法

| 方法 | 显存占用 | 效果 | 适用场景 |
|------|---------|------|---------|
| Full Fine-tuning | 最高 | 最好 | 资源充足 |
| LoRA | 降低70% | 接近全参 | 主流选择 |
| QLoRA | 降低90% | 略有损失 | 资源受限 |
| Adapter | 适中 | 良好 | 多任务 |

### 4.3 超参数速查表

```
┌────────────────────────────────────────────────────────────┐
│                    推荐超参数设置                           │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  SFT阶段:                                                  │
│  ├── 学习率: 全参 2e-5, LoRA 5e-5, QLoRA 1e-4             │
│  ├── Epochs: 1-3                                          │
│  ├── Batch Size: 根据显存，通常4-32                        │
│  └── Warmup: 3-10% steps                                  │
│                                                            │
│  DPO阶段:                                                  │
│  ├── β (KL系数): 0.1 (常用)                               │
│  ├── 学习率: 比SFT更小，1e-6 ~ 5e-6                       │
│  └── Epochs: 1-2                                          │
│                                                            │
│  GRPO阶段:                                                 │
│  ├── KL系数: 0.001 ~ 0.02                                 │
│  ├── 学习率: 1e-6 ~ 5e-6                                  │
│  ├── 采样数: 8-16 per prompt                              │
│  └── Clip ratio: 10                                       │
│                                                            │
│  LoRA参数:                                                 │
│  ├── Rank (r): 8-256, 推荐从16开始                        │
│  ├── Alpha (α): 通常等于r或2r                             │
│  └── Target: 全部线性层 (attention + MLP)                 │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

---

## 五、手册结构导航

```
handbook/
├── 00-概述与全景图.md          ← 你在这里
├── 01-数据工程.md              # 数据收集、清洗、格式化
├── 02-继续预训练CPT.md         # 领域知识注入
├── 03-监督微调SFT.md           # 指令遵循训练
├── 04-奖励模型训练.md          # Reward Model + 偏好数据工程
├── 05-偏好对齐DPO.md           # Direct Preference Optimization
├── 06-强化学习RLHF-GRPO.md     # PPO/GRPO
├── 07-评估与迭代.md            # 评测体系 + Judge偏差控制
├── 08-工程实践.md              # 分布式训练、显存优化
├── 09-案例实战-销售LLM.md      # 完整案例
├── 10-故障排查Runbook.md       # 故障排查：现象→根因→排查→修复→验证（单表）
├── 10-故障排查与踩坑指南.md    # （补充）分阶段排查笔记、Tokenizer陷阱、工程黑魔法
└── 11-前沿技术.md              # 推理训练/Agent/长上下文/模型合并
```

### 5.1 专题文章（可选阅读，但更偏“工程SOP化”）

- [如何让Post-Train Solid——SFT](../如何让Post-Train Solid——SFT.md)：把 SFT 做成**可复现/可解释/可放行**的工程流程（模板一致、labels/masking Gate、数据契约、放行门槛）。

---

## 六、学习路径建议

### 入门级（1-2周）
1. 阅读本概述，建立全局认知
2. 学习01-数据工程，掌握数据处理
3. 实践03-SFT，完成第一次微调

### 进阶级（2-4周）
4. 深入02-CPT，理解领域适配
5. 掌握05-DPO，实现偏好对齐
6. 学习07-评估，建立评测体系

### 专家级（1-2月）
7. 精通04-RM和06-RLHF/GRPO
8. 实践08-工程实践，优化训练效率
9. 完成09-销售LLM完整案例

---

## 参考资源

### 核心论文
- [InstructGPT](https://arxiv.org/abs/2203.02155) - RLHF奠基之作
- [DeepSeek-R1](https://arxiv.org/abs/2501.12948) - GRPO与纯RL推理
- [DPO](https://arxiv.org/abs/2305.18290) - 直接偏好优化
- [Constitutional AI](https://arxiv.org/abs/2212.08073) - RLAIF

### 开源工具
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - 一站式微调框架
- [TRL](https://github.com/huggingface/trl) - HuggingFace官方RL库
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) - 分布式RLHF框架
- [Unsloth](https://github.com/unslothai/unsloth) - 高效微调

### 学习资源
- [RLHF Book](https://rlhfbook.com/) - Nathan Lambert
- [LLM Course](https://github.com/mlabonne/llm-course) - Maxime Labonne

---

> **下一章**：[01-数据工程.md](./01-数据工程.md) - 学习如何准备高质量训练数据
