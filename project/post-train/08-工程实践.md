# 08 - 工程实践

## 8.1 分布式训练

### 8.1.1 并行策略概览

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           分布式训练策略                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐ │
│  │ 数据并行 DP  │  │ 张量并行 TP  │  │ 流水线并行PP │  │ 序列并行 SP │ │
│  │              │  │              │  │              │  │             │ │
│  │ 每个GPU完整  │  │ 切分单层的   │  │ 切分不同层   │  │ 切分序列    │ │
│  │ 模型副本     │  │ 权重矩阵     │  │ 到不同GPU    │  │ 长度维度    │ │
│  │              │  │              │  │              │  │             │ │
│  │ 适用：小模型 │  │ 适用：大矩阵 │  │ 适用：深模型 │  │ 适用：长序列│ │
│  └──────────────┘  └──────────────┘  └──────────────┘  └─────────────┘ │
│                                                                         │
│  常见组合：                                                              │
│  • 7B模型：DP only 或 ZeRO-3                                            │
│  • 70B模型：TP8 + DP                                                    │
│  • 405B模型：TP8 + PP4 + DP                                             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 8.1.2 DeepSpeed ZeRO配置

```python
# deepspeed_config_zero2.json - 适用于7B-13B模型
{
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "none"
        },
        "contiguous_gradients": true,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5e8,
        "allgather_bucket_size": 5e8
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

```python
# deepspeed_config_zero3.json - 适用于30B+模型
{
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "gradient_accumulation_steps": 8,
    "gradient_clipping": 1.0,
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

```python
# 使用DeepSpeed训练
import deepspeed
from transformers import AutoModelForCausalLM, AutoTokenizer

def setup_deepspeed_training():
    """配置DeepSpeed训练"""

    # 模型和tokenizer
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B")

    # DeepSpeed初始化
    model_engine, optimizer, _, _ = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config="deepspeed_config_zero2.json"
    )

    return model_engine, tokenizer


# 启动命令
"""
# 单机8卡
deepspeed --num_gpus=8 train.py --deepspeed deepspeed_config_zero2.json

# 多机训练 (2台机器，每台8卡)
deepspeed --hostfile=hostfile.txt \
    --num_nodes=2 \
    --num_gpus=8 \
    train.py \
    --deepspeed deepspeed_config_zero3.json
"""
```

### 8.1.3 FSDP配置

```python
import torch
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    BackwardPrefetch,
    ShardingStrategy,
    CPUOffload,
)
from torch.distributed.fsdp.wrap import (
    size_based_auto_wrap_policy,
    transformer_auto_wrap_policy,
)
from transformers.models.llama.modeling_llama import LlamaDecoderLayer
import functools

def setup_fsdp_training(model, rank):
    """配置FSDP训练"""

    # 混合精度策略
    bf16_policy = MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        buffer_dtype=torch.bfloat16,
    )

    # 自动包装策略 - 按Transformer层包装
    auto_wrap_policy = functools.partial(
        transformer_auto_wrap_policy,
        transformer_layer_cls={LlamaDecoderLayer},
    )

    # FSDP配置
    fsdp_config = dict(
        auto_wrap_policy=auto_wrap_policy,
        sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO-3等价
        mixed_precision=bf16_policy,
        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
        device_id=torch.cuda.current_device(),
        limit_all_gathers=True,
        use_orig_params=True,  # 支持gradient checkpointing
    )

    # CPU Offload (可选，用于超大模型)
    if model.num_parameters() > 30e9:
        fsdp_config["cpu_offload"] = CPUOffload(offload_params=True)

    # 包装模型
    model = FSDP(model, **fsdp_config)

    return model


# accelerate配置文件 fsdp_config.yaml
"""
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
"""
```

### 8.1.4 显存计算与优化

```python
from dataclasses import dataclass
from typing import Optional
import math

@dataclass
class MemoryEstimator:
    """显存估算器"""

    model_params: int           # 模型参数量（如7e9）
    seq_length: int             # 序列长度
    batch_size: int             # 批次大小
    num_layers: int             # 层数
    hidden_size: int            # 隐藏层大小
    num_heads: int              # 注意力头数
    precision: str = "bf16"     # 精度：fp32, bf16, fp16

    def bytes_per_param(self) -> int:
        """每个参数占用字节数"""
        mapping = {"fp32": 4, "bf16": 2, "fp16": 2, "int8": 1, "int4": 0.5}
        return mapping.get(self.precision, 2)

    def estimate_model_memory(self) -> float:
        """估算模型权重显存 (GB)"""
        return self.model_params * self.bytes_per_param() / 1e9

    def estimate_optimizer_memory(self, optimizer: str = "adamw") -> float:
        """估算优化器状态显存 (GB)"""
        # AdamW需要存储: momentum + variance = 2x参数
        # 优化器状态通常用fp32
        if optimizer == "adamw":
            return self.model_params * 4 * 2 / 1e9
        elif optimizer == "sgd":
            return self.model_params * 4 / 1e9
        else:
            return self.model_params * 4 * 2 / 1e9

    def estimate_gradient_memory(self) -> float:
        """估算梯度显存 (GB)"""
        return self.model_params * self.bytes_per_param() / 1e9

    def estimate_activation_memory(self) -> float:
        """估算激活值显存 (GB)"""
        # 简化估算：每层激活 ≈ batch * seq * hidden * 2 (forward + backward)
        # 实际取决于是否使用gradient checkpointing

        bytes_per_element = self.bytes_per_param()

        # 注意力激活
        attn_activation = (
            self.batch_size * self.num_heads *
            self.seq_length * self.seq_length * bytes_per_element
        )

        # FFN激活
        ffn_activation = (
            self.batch_size * self.seq_length *
            self.hidden_size * 4 * bytes_per_element  # 4x for FFN intermediate
        )

        # 每层总激活
        per_layer = attn_activation + ffn_activation

        # 所有层（无gradient checkpointing）
        total = per_layer * self.num_layers

        return total / 1e9

    def estimate_activation_with_checkpointing(self) -> float:
        """使用gradient checkpointing时的激活显存 (GB)"""
        # 只保存每层输入，前向时重计算
        bytes_per_element = self.bytes_per_param()

        per_layer = (
            self.batch_size * self.seq_length *
            self.hidden_size * bytes_per_element
        )

        return per_layer * self.num_layers / 1e9

    def total_memory_training(
        self,
        use_checkpointing: bool = True,
        use_lora: bool = False,
        lora_params_ratio: float = 0.01
    ) -> dict:
        """训练总显存估算"""

        model_mem = self.estimate_model_memory()

        if use_lora:
            # LoRA只训练少量参数
            trainable_params = self.model_params * lora_params_ratio
            optimizer_mem = trainable_params * 4 * 2 / 1e9
            gradient_mem = trainable_params * self.bytes_per_param() / 1e9
        else:
            optimizer_mem = self.estimate_optimizer_memory()
            gradient_mem = self.estimate_gradient_memory()

        if use_checkpointing:
            activation_mem = self.estimate_activation_with_checkpointing()
        else:
            activation_mem = self.estimate_activation_memory()

        # KV Cache（推理时重要，训练时也占用）
        kv_cache_mem = self._estimate_kv_cache()

        total = model_mem + optimizer_mem + gradient_mem + activation_mem

        return {
            "model_weights": f"{model_mem:.2f} GB",
            "optimizer_states": f"{optimizer_mem:.2f} GB",
            "gradients": f"{gradient_mem:.2f} GB",
            "activations": f"{activation_mem:.2f} GB",
            "total_estimated": f"{total:.2f} GB",
            "recommendation": self._recommend_config(total)
        }

    def _estimate_kv_cache(self) -> float:
        """估算KV Cache大小"""
        # KV cache = 2 * num_layers * batch * seq * hidden * bytes
        bytes_per_element = self.bytes_per_param()
        kv_cache = (
            2 * self.num_layers * self.batch_size *
            self.seq_length * self.hidden_size * bytes_per_element
        )
        return kv_cache / 1e9

    def _recommend_config(self, total_mem: float) -> str:
        """根据显存需求推荐配置"""

        if total_mem < 20:
            return "单张24GB GPU (RTX 4090/A5000)"
        elif total_mem < 40:
            return "单张48GB GPU (A6000/RTX 6000) 或 2张24GB (DP/FSDP)"
        elif total_mem < 80:
            return "单张80GB GPU (A100/H100) 或 4张24GB (FSDP)"
        elif total_mem < 160:
            return "2张80GB GPU (A100/H100) 或 8张24GB (FSDP)"
        else:
            return "多节点分布式训练 (DeepSpeed ZeRO-3 / FSDP)"


# 使用示例
estimator = MemoryEstimator(
    model_params=7e9,        # 7B模型
    seq_length=4096,
    batch_size=4,
    num_layers=32,
    hidden_size=4096,
    num_heads=32,
    precision="bf16"
)

# 全参数训练
print("=== 全参数微调 ===")
result = estimator.total_memory_training(use_checkpointing=True, use_lora=False)
for k, v in result.items():
    print(f"{k}: {v}")

# LoRA训练
print("\n=== LoRA微调 ===")
result = estimator.total_memory_training(use_checkpointing=True, use_lora=True)
for k, v in result.items():
    print(f"{k}: {v}")
```

---

## 8.2 显存优化技术

### 8.2.1 Gradient Checkpointing

```python
from transformers import AutoModelForCausalLM
import torch

def enable_gradient_checkpointing(model):
    """启用Gradient Checkpointing"""

    # Transformers原生支持
    model.gradient_checkpointing_enable()

    # 对于某些模型需要特殊处理
    if hasattr(model, 'model'):
        # Llama风格模型
        model.model.gradient_checkpointing_enable()

    return model


# 自定义checkpointing策略
from torch.utils.checkpoint import checkpoint

class CustomCheckpointedBlock(torch.nn.Module):
    """自定义checkpoint块"""

    def __init__(self, block, use_checkpoint=True):
        super().__init__()
        self.block = block
        self.use_checkpoint = use_checkpoint

    def forward(self, hidden_states, **kwargs):
        if self.use_checkpoint and self.training:
            # 使用checkpoint，不保存中间激活
            return checkpoint(
                self.block,
                hidden_states,
                use_reentrant=False,
                **kwargs
            )
        else:
            return self.block(hidden_states, **kwargs)


def apply_selective_checkpointing(model, checkpoint_every_n=2):
    """选择性checkpointing - 每n层checkpoint一次"""

    for i, layer in enumerate(model.model.layers):
        if i % checkpoint_every_n == 0:
            model.model.layers[i] = CustomCheckpointedBlock(layer, use_checkpoint=True)
        else:
            model.model.layers[i] = CustomCheckpointedBlock(layer, use_checkpoint=False)

    return model
```

### 8.2.2 混合精度训练

```python
import torch
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTrainer:
    """混合精度训练器"""

    def __init__(
        self,
        model,
        optimizer,
        precision: str = "bf16",
        loss_scale: str = "dynamic"
    ):
        self.model = model
        self.optimizer = optimizer
        self.precision = precision

        # BF16不需要loss scaling
        if precision == "fp16":
            self.scaler = GradScaler(
                init_scale=65536.0,
                growth_factor=2.0,
                backoff_factor=0.5,
                growth_interval=2000,
            )
        else:
            self.scaler = None

        self.dtype = torch.bfloat16 if precision == "bf16" else torch.float16

    def training_step(self, batch):
        """单步训练"""

        self.optimizer.zero_grad()

        # 自动混合精度前向
        with autocast(dtype=self.dtype):
            outputs = self.model(**batch)
            loss = outputs.loss

        # 反向传播
        if self.scaler is not None:
            # FP16需要loss scaling
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            # BF16直接反向
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()

        return loss.item()


# 推荐配置
"""
GPU类型            推荐精度        原因
A100/H100         BF16           原生支持，无需loss scaling
V100              FP16           不支持BF16
RTX 3090/4090     BF16           Ampere/Ada架构支持
T4                FP16           不支持BF16
"""
```

### 8.2.3 FlashAttention

```python
from flash_attn import flash_attn_func, flash_attn_qkvpacked_func
import torch

def use_flash_attention(model):
    """为模型启用FlashAttention"""

    # 方法1：通过transformers参数
    from transformers import AutoModelForCausalLM

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-7B",
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",  # 关键参数
    )

    return model


# 方法2：手动实现FlashAttention模块
class FlashAttentionModule(torch.nn.Module):
    """FlashAttention实现"""

    def __init__(self, hidden_size, num_heads, dropout=0.0):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.dropout = dropout

        self.q_proj = torch.nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = torch.nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = torch.nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = torch.nn.Linear(hidden_size, hidden_size, bias=False)

    def forward(self, hidden_states, attention_mask=None):
        batch_size, seq_len, _ = hidden_states.shape

        # 投影
        q = self.q_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # FlashAttention
        # 输入shape: (batch, seqlen, nheads, headdim)
        output = flash_attn_func(
            q, k, v,
            dropout_p=self.dropout if self.training else 0.0,
            causal=True,  # 因果注意力
        )

        # 输出投影
        output = output.view(batch_size, seq_len, self.hidden_size)
        output = self.o_proj(output)

        return output


# FlashAttention显存节省估算
def flash_attention_memory_saving(seq_length, num_heads, batch_size):
    """估算FlashAttention节省的显存"""

    # 标准注意力: O(batch * heads * seq^2)
    standard_mem = batch_size * num_heads * seq_length * seq_length * 2  # fp16

    # FlashAttention: O(batch * heads * seq) - 不存储完整注意力矩阵
    flash_mem = batch_size * num_heads * seq_length * 2

    saving_ratio = (standard_mem - flash_mem) / standard_mem * 100

    print(f"序列长度: {seq_length}")
    print(f"标准注意力显存: {standard_mem / 1e9:.2f} GB")
    print(f"FlashAttention显存: {flash_mem / 1e6:.2f} MB")
    print(f"节省比例: {saving_ratio:.1f}%")

    return saving_ratio

# 示例：4096长度序列
flash_attention_memory_saving(4096, 32, 4)
```

### 8.2.4 高效LoRA实现

```python
import torch
import torch.nn as nn
from typing import Optional, List, Dict
import math

class LoRALayer(nn.Module):
    """高效LoRA层实现"""

    def __init__(
        self,
        in_features: int,
        out_features: int,
        rank: int = 8,
        alpha: float = 16,
        dropout: float = 0.0,
    ):
        super().__init__()

        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        # LoRA矩阵
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

        # 初始化
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

    def forward(self, x: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
        # 原始前向
        result = nn.functional.linear(x, weight)

        # LoRA增量
        lora_output = self.dropout(x) @ self.lora_A.T @ self.lora_B.T
        result = result + lora_output * self.scaling

        return result


class LoRALinear(nn.Module):
    """带LoRA的线性层"""

    def __init__(
        self,
        original_layer: nn.Linear,
        rank: int = 8,
        alpha: float = 16,
        dropout: float = 0.0,
    ):
        super().__init__()

        self.original_layer = original_layer
        self.lora = LoRALayer(
            in_features=original_layer.in_features,
            out_features=original_layer.out_features,
            rank=rank,
            alpha=alpha,
            dropout=dropout,
        )

        # 冻结原始层
        for param in self.original_layer.parameters():
            param.requires_grad = False

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.lora(x, self.original_layer.weight)

    def merge_weights(self):
        """合并LoRA权重到原始层（推理优化）"""
        with torch.no_grad():
            merged_weight = (
                self.original_layer.weight +
                self.lora.lora_B @ self.lora.lora_A * self.lora.scaling
            )
            self.original_layer.weight.copy_(merged_weight)

        return self.original_layer


def apply_lora_to_model(
    model,
    rank: int = 8,
    alpha: float = 16,
    target_modules: List[str] = None,
    dropout: float = 0.0,
):
    """为模型应用LoRA"""

    if target_modules is None:
        # 默认目标模块
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

    lora_modules = {}

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # 检查是否是目标模块
            if any(target in name for target in target_modules):
                lora_module = LoRALinear(
                    module, rank=rank, alpha=alpha, dropout=dropout
                )
                lora_modules[name] = lora_module

    # 替换模块
    for name, lora_module in lora_modules.items():
        parent_name = ".".join(name.split(".")[:-1])
        child_name = name.split(".")[-1]
        parent = model.get_submodule(parent_name) if parent_name else model
        setattr(parent, child_name, lora_module)

    # 统计参数量
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"总参数量: {total_params:,}")
    print(f"可训练参数量: {trainable_params:,}")
    print(f"可训练比例: {trainable_params / total_params * 100:.2f}%")

    return model


# 使用PEFT库（推荐）
from peft import LoraConfig, get_peft_model, TaskType

def apply_peft_lora(model, rank=8, alpha=16):
    """使用PEFT库应用LoRA"""

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=rank,
        lora_alpha=alpha,
        lora_dropout=0.05,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        bias="none",
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    return model
```

---

## 8.3 训练稳定性

### 8.3.1 Loss异常检测与处理

```python
import torch
import numpy as np
from collections import deque
from typing import Optional, Callable
import logging

logger = logging.getLogger(__name__)

class LossMonitor:
    """Loss监控器"""

    def __init__(
        self,
        window_size: int = 100,
        spike_threshold: float = 5.0,
        nan_patience: int = 3,
        checkpoint_callback: Optional[Callable] = None
    ):
        self.window_size = window_size
        self.spike_threshold = spike_threshold
        self.nan_patience = nan_patience
        self.checkpoint_callback = checkpoint_callback

        self.loss_history = deque(maxlen=window_size)
        self.nan_count = 0
        self.spike_count = 0
        self.last_good_step = 0

    def check_loss(self, loss: float, step: int) -> dict:
        """检查Loss是否异常"""

        result = {
            "is_valid": True,
            "action": None,
            "reason": None
        }

        # 检查NaN/Inf
        if not np.isfinite(loss):
            self.nan_count += 1
            result["is_valid"] = False
            result["reason"] = f"NaN/Inf loss (count: {self.nan_count})"

            if self.nan_count >= self.nan_patience:
                result["action"] = "rollback"
                logger.error(f"Step {step}: 连续{self.nan_count}次NaN，需要回滚")
            else:
                result["action"] = "skip"
                logger.warning(f"Step {step}: Loss为NaN，跳过此step")

            return result

        # 重置NaN计数
        self.nan_count = 0

        # 检查Loss突增
        if len(self.loss_history) >= 10:
            recent_mean = np.mean(list(self.loss_history)[-10:])
            recent_std = np.std(list(self.loss_history)[-10:])

            if recent_std > 0 and (loss - recent_mean) > self.spike_threshold * recent_std:
                self.spike_count += 1
                result["reason"] = f"Loss spike: {loss:.4f} (mean: {recent_mean:.4f})"
                logger.warning(f"Step {step}: {result['reason']}")

                if self.spike_count >= 3:
                    result["action"] = "reduce_lr"
                    result["is_valid"] = False
                    logger.warning("连续spike，建议降低学习率")
            else:
                self.spike_count = 0
                self.last_good_step = step

        self.loss_history.append(loss)

        return result

    def get_statistics(self) -> dict:
        """获取Loss统计信息"""

        if len(self.loss_history) == 0:
            return {}

        losses = list(self.loss_history)

        return {
            "current": losses[-1],
            "mean": np.mean(losses),
            "std": np.std(losses),
            "min": np.min(losses),
            "max": np.max(losses),
            "trend": "decreasing" if len(losses) > 10 and np.mean(losses[-10:]) < np.mean(losses[:10]) else "stable"
        }


class TrainingStabilizer:
    """训练稳定器"""

    def __init__(
        self,
        model,
        optimizer,
        initial_lr: float,
        min_lr: float = 1e-6,
    ):
        self.model = model
        self.optimizer = optimizer
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.min_lr = min_lr

        self.checkpoints = {}
        self.loss_monitor = LossMonitor()

    def save_checkpoint(self, step: int):
        """保存检查点"""

        self.checkpoints[step] = {
            "model_state": {k: v.clone() for k, v in self.model.state_dict().items()},
            "optimizer_state": {k: v.clone() if isinstance(v, torch.Tensor) else v
                               for k, v in self.optimizer.state_dict().items()},
            "lr": self.current_lr
        }

        # 只保留最近3个检查点
        if len(self.checkpoints) > 3:
            oldest = min(self.checkpoints.keys())
            del self.checkpoints[oldest]

    def rollback_to_checkpoint(self, step: Optional[int] = None):
        """回滚到检查点"""

        if not self.checkpoints:
            logger.error("没有可用的检查点")
            return False

        if step is None:
            step = max(self.checkpoints.keys())

        if step not in self.checkpoints:
            step = max(k for k in self.checkpoints.keys() if k <= step)

        checkpoint = self.checkpoints[step]
        self.model.load_state_dict(checkpoint["model_state"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state"])
        self.current_lr = checkpoint["lr"]

        logger.info(f"已回滚到step {step}")
        return True

    def reduce_learning_rate(self, factor: float = 0.5):
        """降低学习率"""

        new_lr = max(self.current_lr * factor, self.min_lr)

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr

        logger.info(f"学习率从 {self.current_lr} 降低到 {new_lr}")
        self.current_lr = new_lr

    def handle_training_step(self, loss: float, step: int) -> bool:
        """处理训练步骤"""

        check_result = self.loss_monitor.check_loss(loss, step)

        if check_result["action"] == "rollback":
            self.rollback_to_checkpoint()
            self.reduce_learning_rate(0.5)
            return False

        elif check_result["action"] == "reduce_lr":
            self.reduce_learning_rate(0.8)

        elif check_result["action"] == "skip":
            return False

        # 定期保存检查点
        if step % 100 == 0:
            self.save_checkpoint(step)

        return True
```

### 8.3.2 梯度处理

```python
import torch
from typing import Optional, Dict
import logging

logger = logging.getLogger(__name__)

class GradientHandler:
    """梯度处理器"""

    def __init__(
        self,
        max_grad_norm: float = 1.0,
        detect_anomaly: bool = False
    ):
        self.max_grad_norm = max_grad_norm
        self.detect_anomaly = detect_anomaly

        if detect_anomaly:
            torch.autograd.set_detect_anomaly(True)

    def clip_gradients(self, model) -> float:
        """梯度裁剪"""

        total_norm = torch.nn.utils.clip_grad_norm_(
            model.parameters(),
            self.max_grad_norm
        )

        return total_norm.item()

    def check_gradients(self, model) -> Dict:
        """检查梯度健康度"""

        result = {
            "has_nan": False,
            "has_inf": False,
            "max_grad": 0.0,
            "min_grad": float('inf'),
            "zero_grad_layers": []
        }

        for name, param in model.named_parameters():
            if param.grad is not None:
                grad = param.grad

                # 检查NaN
                if torch.isnan(grad).any():
                    result["has_nan"] = True
                    logger.warning(f"梯度NaN: {name}")

                # 检查Inf
                if torch.isinf(grad).any():
                    result["has_inf"] = True
                    logger.warning(f"梯度Inf: {name}")

                # 统计梯度范围
                grad_abs = grad.abs()
                grad_max = grad_abs.max().item()
                grad_min = grad_abs.min().item()

                result["max_grad"] = max(result["max_grad"], grad_max)
                result["min_grad"] = min(result["min_grad"], grad_min)

                # 检查零梯度
                if grad_abs.max().item() < 1e-8:
                    result["zero_grad_layers"].append(name)

        return result

    def apply_gradient_scaling(
        self,
        model,
        layer_wise_lr_decay: float = 0.9
    ):
        """层级学习率衰减"""

        # 从输出层到输入层递减学习率
        num_layers = len(list(model.named_parameters()))

        for i, (name, param) in enumerate(model.named_parameters()):
            if param.grad is not None:
                # 越靠近输入层，学习率越小
                decay = layer_wise_lr_decay ** (num_layers - i - 1)
                param.grad = param.grad * decay


def setup_gradient_hooks(model):
    """设置梯度钩子用于调试"""

    gradient_stats = {}

    def hook_fn(name):
        def hook(grad):
            gradient_stats[name] = {
                "mean": grad.mean().item(),
                "std": grad.std().item(),
                "max": grad.max().item(),
                "min": grad.min().item()
            }
            return grad
        return hook

    hooks = []
    for name, param in model.named_parameters():
        if param.requires_grad:
            hook = param.register_hook(hook_fn(name))
            hooks.append(hook)

    return gradient_stats, hooks
```

---

## 8.4 模型压缩与量化

### 8.4.1 训练后量化 (PTQ)

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from typing import Optional

def load_quantized_model_4bit(model_name: str):
    """加载4bit量化模型"""

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",           # 使用NormalFloat4
        bnb_4bit_compute_dtype=torch.bfloat16,  # 计算使用bf16
        bnb_4bit_use_double_quant=True,       # 双重量化进一步压缩
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    return model


def load_quantized_model_8bit(model_name: str):
    """加载8bit量化模型"""

    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_threshold=6.0,              # 离群值阈值
        llm_int8_has_fp16_weight=False,
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto"
    )

    return model


# AWQ量化
def quantize_with_awq(model_path: str, output_path: str):
    """使用AWQ量化"""

    from awq import AutoAWQForCausalLM
    from transformers import AutoTokenizer

    model = AutoAWQForCausalLM.from_pretrained(
        model_path,
        safetensors=True,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # 量化配置
    quant_config = {
        "zero_point": True,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "GEMM"  # 或 "GEMV"
    }

    # 执行量化（需要校准数据）
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        calib_data="wikitext",  # 校准数据集
        split="train",
        text_column="text",
        duo_scaling=True,
        export_compatible=True
    )

    # 保存量化模型
    model.save_quantized(output_path)
    tokenizer.save_pretrained(output_path)


# GPTQ量化
def quantize_with_gptq(model_path: str, output_path: str):
    """使用GPTQ量化"""

    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
    from transformers import AutoTokenizer
    from datasets import load_dataset

    # 加载模型
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoGPTQForCausalLM.from_pretrained(
        model_path,
        quantize_config=BaseQuantizeConfig(
            bits=4,
            group_size=128,
            damp_percent=0.1,
            desc_act=True,
        )
    )

    # 准备校准数据
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
    calib_data = [
        tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        for text in dataset["text"][:128]
    ]

    # 量化
    model.quantize(calib_data)

    # 保存
    model.save_quantized(output_path)
    tokenizer.save_pretrained(output_path)
```

### 8.4.2 量化感知训练 (QAT)

```python
import torch
import torch.nn as nn
from typing import Optional

class QuantizedLinear(nn.Module):
    """量化感知线性层"""

    def __init__(
        self,
        original_layer: nn.Linear,
        bits: int = 4,
        group_size: int = 128
    ):
        super().__init__()

        self.in_features = original_layer.in_features
        self.out_features = original_layer.out_features
        self.bits = bits
        self.group_size = group_size

        # 原始权重
        self.weight = nn.Parameter(original_layer.weight.data)
        if original_layer.bias is not None:
            self.bias = nn.Parameter(original_layer.bias.data)
        else:
            self.bias = None

        # 量化参数（可学习）
        num_groups = (self.in_features + group_size - 1) // group_size
        self.scale = nn.Parameter(torch.ones(self.out_features, num_groups))
        self.zero_point = nn.Parameter(torch.zeros(self.out_features, num_groups))

    def quantize_weight(self, weight: torch.Tensor) -> torch.Tensor:
        """量化权重"""

        qmin = 0
        qmax = 2 ** self.bits - 1

        # 分组量化
        weight_reshaped = weight.view(self.out_features, -1, self.group_size)

        # 计算scale和zero_point
        w_min = weight_reshaped.min(dim=-1, keepdim=True)[0]
        w_max = weight_reshaped.max(dim=-1, keepdim=True)[0]

        scale = (w_max - w_min) / (qmax - qmin)
        zero_point = qmin - w_min / scale

        # 量化
        weight_q = torch.clamp(
            torch.round(weight_reshaped / scale + zero_point),
            qmin, qmax
        )

        # 反量化（训练时使用STE）
        weight_dq = (weight_q - zero_point) * scale

        return weight_dq.view(self.out_features, self.in_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.training:
            # 训练时使用Straight-Through Estimator (STE)
            weight_q = self.quantize_weight(self.weight)
            # STE: 前向使用量化权重，反向传播原始梯度
            weight = self.weight + (weight_q - self.weight).detach()
        else:
            # 推理时直接使用量化权重
            weight = self.quantize_weight(self.weight)

        return nn.functional.linear(x, weight, self.bias)


def apply_qat(model, bits=4):
    """应用量化感知训练"""

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # 获取父模块
            parent_name = ".".join(name.split(".")[:-1])
            child_name = name.split(".")[-1]

            if parent_name:
                parent = model.get_submodule(parent_name)
            else:
                parent = model

            # 替换为量化层
            quantized_layer = QuantizedLinear(module, bits=bits)
            setattr(parent, child_name, quantized_layer)

    return model
```

### 8.4.3 知识蒸馏

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict

class DistillationLoss(nn.Module):
    """知识蒸馏损失"""

    def __init__(
        self,
        temperature: float = 2.0,
        alpha: float = 0.5,  # hard label权重
    ):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            student_logits: 学生模型输出 (batch, seq, vocab)
            teacher_logits: 教师模型输出 (batch, seq, vocab)
            labels: 真实标签 (batch, seq)
        """
        # Soft labels loss (KL散度)
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(teacher_logits / self.temperature, dim=-1),
            reduction='batchmean'
        ) * (self.temperature ** 2)

        # Hard labels loss (交叉熵)
        hard_loss = F.cross_entropy(
            student_logits.view(-1, student_logits.size(-1)),
            labels.view(-1),
            ignore_index=-100
        )

        # 组合损失
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss

        return total_loss


class DistillationTrainer:
    """知识蒸馏训练器"""

    def __init__(
        self,
        teacher_model,
        student_model,
        tokenizer,
        temperature: float = 2.0,
        alpha: float = 0.5
    ):
        self.teacher = teacher_model
        self.student = student_model
        self.tokenizer = tokenizer

        # 冻结教师模型
        for param in self.teacher.parameters():
            param.requires_grad = False
        self.teacher.eval()

        self.loss_fn = DistillationLoss(temperature, alpha)

    def train_step(self, batch: Dict) -> torch.Tensor:
        """单步训练"""

        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        # 教师模型前向（不需要梯度）
        with torch.no_grad():
            teacher_outputs = self.teacher(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            teacher_logits = teacher_outputs.logits

        # 学生模型前向
        student_outputs = self.student(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        student_logits = student_outputs.logits

        # 计算蒸馏损失
        loss = self.loss_fn(student_logits, teacher_logits, labels)

        return loss


# 层级蒸馏
class LayerWiseDistillation(nn.Module):
    """层级知识蒸馏"""

    def __init__(
        self,
        student_hidden_size: int,
        teacher_hidden_size: int,
        num_layers: int
    ):
        super().__init__()

        # 如果学生和教师隐藏层大小不同，需要投影
        if student_hidden_size != teacher_hidden_size:
            self.projectors = nn.ModuleList([
                nn.Linear(student_hidden_size, teacher_hidden_size)
                for _ in range(num_layers)
            ])
        else:
            self.projectors = None

    def forward(
        self,
        student_hidden_states: list,
        teacher_hidden_states: list
    ) -> torch.Tensor:
        """
        Args:
            student_hidden_states: 学生模型各层隐藏状态
            teacher_hidden_states: 教师模型各层隐藏状态
        """
        total_loss = 0.0

        for i, (s_hidden, t_hidden) in enumerate(
            zip(student_hidden_states, teacher_hidden_states)
        ):
            # 投影（如需要）
            if self.projectors is not None:
                s_hidden = self.projectors[i](s_hidden)

            # MSE损失
            layer_loss = F.mse_loss(s_hidden, t_hidden)
            total_loss += layer_loss

        return total_loss / len(student_hidden_states)
```

---

## 8.5 推理部署

### 8.5.1 vLLM部署

```python
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest
from typing import List, Dict, Optional

class VLLMInferenceEngine:
    """vLLM推理引擎"""

    def __init__(
        self,
        model_path: str,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.85,  # ⚠️ 生产环境建议0.8-0.85
        max_model_len: int = 4096,
        enable_lora: bool = False,
        max_loras: int = 4,
    ):
        """
        ⚠️ gpu_memory_utilization 参数说明：
        - 0.9: 开发/测试环境可用，但高并发下可能OOM
        - 0.85: 生产环境推荐，平衡吞吐与稳定性
        - 0.8: 保守设置，适合内存紧张或并发极高的场景

        原因：KV Cache会产生碎片，预留空间防止OOM
        """
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=max_model_len,
            enable_lora=enable_lora,
            max_loras=max_loras,
            trust_remote_code=True,
        )

        self.lora_adapters = {}

    def load_lora_adapter(self, name: str, path: str):
        """加载LoRA适配器"""
        self.lora_adapters[name] = path

    def generate(
        self,
        prompts: List[str],
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        lora_name: Optional[str] = None,
    ) -> List[str]:
        """批量生成"""

        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )

        # 使用LoRA适配器
        lora_request = None
        if lora_name and lora_name in self.lora_adapters:
            lora_request = LoRARequest(
                lora_name=lora_name,
                lora_int_id=hash(lora_name) % 1000,
                lora_path=self.lora_adapters[lora_name]
            )

        outputs = self.llm.generate(
            prompts,
            sampling_params,
            lora_request=lora_request
        )

        return [output.outputs[0].text for output in outputs]

    def stream_generate(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
    ):
        """流式生成"""

        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
        )

        for output in self.llm.generate([prompt], sampling_params, stream=True):
            yield output[0].outputs[0].text


# vLLM服务端部署
"""
# 启动OpenAI兼容API服务
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/model \
    --tensor-parallel-size 2 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.9 \
    --host 0.0.0.0 \
    --port 8000

# 启动带LoRA的服务
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/base_model \
    --enable-lora \
    --lora-modules sales=/path/to/sales_lora,support=/path/to/support_lora \
    --max-loras 4 \
    --max-lora-rank 64
"""
```

### 8.5.2 TGI部署

```yaml
# docker-compose.yml for Text Generation Inference
version: '3.8'
services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:latest
    ports:
      - "8080:80"
    volumes:
      - ./models:/data
    environment:
      - MODEL_ID=/data/sales-llm
      - NUM_SHARD=2
      - MAX_INPUT_LENGTH=2048
      - MAX_TOTAL_TOKENS=4096
      - QUANTIZE=bitsandbytes-nf4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
```

```python
# TGI客户端
from huggingface_hub import InferenceClient

class TGIClient:
    """TGI推理客户端"""

    def __init__(self, endpoint: str):
        self.client = InferenceClient(endpoint)

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        stop_sequences: List[str] = None,
    ) -> str:
        """生成回复"""

        response = self.client.text_generation(
            prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            stop_sequences=stop_sequences,
            return_full_text=False,
        )

        return response

    def stream_generate(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
    ):
        """流式生成"""

        for token in self.client.text_generation(
            prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            stream=True,
        ):
            yield token
```

### 8.5.3 FastAPI服务

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
import asyncio
import uvicorn

app = FastAPI(title="Sales LLM API")

# 全局模型实例
model_engine = None

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    max_tokens: int = 512
    temperature: float = 0.7
    stream: bool = False

class ChatResponse(BaseModel):
    id: str
    choices: List[dict]
    usage: dict

@app.on_event("startup")
async def startup():
    """启动时加载模型"""
    global model_engine
    from vllm import LLM

    model_engine = LLM(
        model="./sales-llm",
        tensor_parallel_size=1,
        gpu_memory_utilization=0.85  # 生产环境建议0.8-0.85
    )

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """Chat Completions API（OpenAI兼容）"""

    if model_engine is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    # 构建prompt
    prompt = format_chat_messages(request.messages)

    if request.stream:
        return StreamingResponse(
            stream_response(prompt, request),
            media_type="text/event-stream"
        )

    # 非流式响应
    from vllm import SamplingParams

    sampling_params = SamplingParams(
        max_tokens=request.max_tokens,
        temperature=request.temperature,
    )

    outputs = model_engine.generate([prompt], sampling_params)
    response_text = outputs[0].outputs[0].text

    return ChatResponse(
        id="chatcmpl-" + str(hash(prompt))[:8],
        choices=[{
            "index": 0,
            "message": {"role": "assistant", "content": response_text},
            "finish_reason": "stop"
        }],
        usage={
            "prompt_tokens": len(prompt.split()),
            "completion_tokens": len(response_text.split()),
            "total_tokens": len(prompt.split()) + len(response_text.split())
        }
    )

def format_chat_messages(messages: List[ChatMessage]) -> str:
    """格式化聊天消息"""
    formatted = ""
    for msg in messages:
        if msg.role == "system":
            formatted += f"<|im_start|>system\n{msg.content}<|im_end|>\n"
        elif msg.role == "user":
            formatted += f"<|im_start|>user\n{msg.content}<|im_end|>\n"
        elif msg.role == "assistant":
            formatted += f"<|im_start|>assistant\n{msg.content}<|im_end|>\n"

    formatted += "<|im_start|>assistant\n"
    return formatted

async def stream_response(prompt: str, request: ChatRequest):
    """流式响应生成"""
    from vllm import SamplingParams

    sampling_params = SamplingParams(
        max_tokens=request.max_tokens,
        temperature=request.temperature,
    )

    # vLLM流式生成
    for output in model_engine.generate([prompt], sampling_params, stream=True):
        token = output[0].outputs[0].text
        data = {
            "id": "chatcmpl-stream",
            "choices": [{
                "index": 0,
                "delta": {"content": token},
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(data)}\n\n"
        await asyncio.sleep(0)

    yield "data: [DONE]\n\n"


# 健康检查
@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": model_engine is not None}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 8.6 监控与运维

### 8.6.1 训练监控

```python
import wandb
from typing import Dict, Optional
import torch
from datetime import datetime

class TrainingMonitor:
    """训练监控器"""

    def __init__(
        self,
        project: str,
        name: str,
        config: Dict
    ):
        # 初始化W&B
        wandb.init(
            project=project,
            name=name,
            config=config
        )

        self.step = 0
        self.epoch = 0

    def log_step(
        self,
        loss: float,
        learning_rate: float,
        grad_norm: Optional[float] = None,
        **kwargs
    ):
        """记录训练步骤"""

        metrics = {
            "train/loss": loss,
            "train/learning_rate": learning_rate,
            "train/step": self.step,
        }

        if grad_norm is not None:
            metrics["train/grad_norm"] = grad_norm

        # 额外指标
        metrics.update(kwargs)

        wandb.log(metrics, step=self.step)
        self.step += 1

    def log_eval(self, metrics: Dict):
        """记录评估结果"""

        eval_metrics = {f"eval/{k}": v for k, v in metrics.items()}
        wandb.log(eval_metrics, step=self.step)

    def log_model_stats(self, model):
        """记录模型统计"""

        # 参数统计
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        wandb.log({
            "model/total_params": total_params,
            "model/trainable_params": trainable_params,
            "model/trainable_ratio": trainable_params / total_params
        }, step=self.step)

        # 权重分布
        for name, param in model.named_parameters():
            if param.requires_grad:
                wandb.log({
                    f"weights/{name}_mean": param.data.mean().item(),
                    f"weights/{name}_std": param.data.std().item(),
                }, step=self.step)

    def log_gpu_stats(self):
        """记录GPU统计"""

        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                memory_allocated = torch.cuda.memory_allocated(i) / 1e9
                memory_reserved = torch.cuda.memory_reserved(i) / 1e9

                wandb.log({
                    f"gpu/{i}/memory_allocated_gb": memory_allocated,
                    f"gpu/{i}/memory_reserved_gb": memory_reserved,
                }, step=self.step)

    def save_checkpoint(self, model, optimizer, path: str):
        """保存检查点"""

        checkpoint = {
            "step": self.step,
            "epoch": self.epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        }

        torch.save(checkpoint, path)

        # 上传到W&B
        artifact = wandb.Artifact(
            name=f"checkpoint-{self.step}",
            type="model"
        )
        artifact.add_file(path)
        wandb.log_artifact(artifact)

    def finish(self):
        """结束监控"""
        wandb.finish()
```

### 8.6.2 推理服务监控

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
from functools import wraps
from typing import Callable

# Prometheus指标定义
REQUEST_COUNT = Counter(
    'llm_requests_total',
    'Total number of LLM requests',
    ['endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'llm_request_latency_seconds',
    'Request latency in seconds',
    ['endpoint'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

TOKENS_GENERATED = Counter(
    'llm_tokens_generated_total',
    'Total tokens generated'
)

TOKENS_PER_SECOND = Gauge(
    'llm_tokens_per_second',
    'Token generation speed'
)

GPU_MEMORY_USAGE = Gauge(
    'llm_gpu_memory_usage_bytes',
    'GPU memory usage',
    ['gpu_id']
)

ACTIVE_REQUESTS = Gauge(
    'llm_active_requests',
    'Number of active requests'
)

def monitor_request(endpoint: str):
    """请求监控装饰器"""

    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            ACTIVE_REQUESTS.inc()
            start_time = time.time()

            try:
                result = await func(*args, **kwargs)
                REQUEST_COUNT.labels(endpoint=endpoint, status='success').inc()
                return result

            except Exception as e:
                REQUEST_COUNT.labels(endpoint=endpoint, status='error').inc()
                raise

            finally:
                latency = time.time() - start_time
                REQUEST_LATENCY.labels(endpoint=endpoint).observe(latency)
                ACTIVE_REQUESTS.dec()

        return wrapper
    return decorator


class InferenceMetricsCollector:
    """推理指标收集器"""

    def __init__(self, port: int = 9090):
        # 启动Prometheus HTTP服务器
        start_http_server(port)

    def record_generation(
        self,
        num_tokens: int,
        latency: float
    ):
        """记录生成指标"""

        TOKENS_GENERATED.inc(num_tokens)

        if latency > 0:
            tokens_per_sec = num_tokens / latency
            TOKENS_PER_SECOND.set(tokens_per_sec)

    def update_gpu_metrics(self):
        """更新GPU指标"""

        import torch
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                memory_used = torch.cuda.memory_allocated(i)
                GPU_MEMORY_USAGE.labels(gpu_id=str(i)).set(memory_used)


# Grafana Dashboard配置示例
GRAFANA_DASHBOARD = """
{
  "dashboard": {
    "title": "Sales LLM Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {"expr": "rate(llm_requests_total[5m])"}
        ]
      },
      {
        "title": "Latency P95",
        "type": "graph",
        "targets": [
          {"expr": "histogram_quantile(0.95, rate(llm_request_latency_seconds_bucket[5m]))"}
        ]
      },
      {
        "title": "Tokens per Second",
        "type": "gauge",
        "targets": [
          {"expr": "llm_tokens_per_second"}
        ]
      },
      {
        "title": "GPU Memory Usage",
        "type": "graph",
        "targets": [
          {"expr": "llm_gpu_memory_usage_bytes"}
        ]
      }
    ]
  }
}
"""
```

---

## 8.7 本章小结

工程实践是将模型从实验室带到生产环境的关键：

1. **分布式训练**：根据模型规模选择合适的并行策略（ZeRO/FSDP/TP/PP）
2. **显存优化**：Gradient Checkpointing、混合精度、FlashAttention
3. **训练稳定性**：Loss监控、梯度处理、自动回滚机制
4. **模型压缩**：PTQ/QAT量化、知识蒸馏，在效果和效率间平衡
5. **推理部署**：vLLM/TGI高性能部署，API服务设计
6. **监控运维**：W&B训练监控，Prometheus+Grafana生产监控

**工程师的核心能力不是让模型跑起来，而是让它稳定、高效、可控地运行**。
