"""
Phase 2 åŠŸèƒ½æµ‹è¯•

æµ‹è¯• Recipe System, Config Diff, Data Versioning, Experience Reuse
"""

import sys
import os
import json
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from training_platform.core.recipes import (
    RecipeRegistry,
    TaskType,
    apply_recipe_to_job_config,
    validate_recipe_config,
)
from training_platform.core.config_diff import (
    compare_configs,
    format_diff_report,
    compare_recipes,
)
from training_platform.core.dataset_version import (
    calculate_file_hash,
    create_dataset_snapshot,
    compare_dataset_versions,
)


def test_recipe_system():
    """æµ‹è¯•é…æ–¹ç³»ç»Ÿ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 1: Recipe System (é…æ–¹ç³»ç»Ÿ)")
    print("=" * 80)

    # æµ‹è¯• 1.1: åˆ—å‡ºæ‰€æœ‰é…æ–¹
    print("\n1.1 åˆ—å‡ºæ‰€æœ‰é…æ–¹:")
    all_recipes = RecipeRegistry.list_all()
    print(f"âœ“ æ‰¾åˆ° {len(all_recipes)} ä¸ªé…æ–¹")
    for recipe_info in all_recipes[:3]:
        print(f"  - {recipe_info['name']}: {recipe_info['description'][:50]}...")

    # æµ‹è¯• 1.2: è·å–ç‰¹å®šé…æ–¹
    print("\n1.2 è·å– GRPO é…æ–¹:")
    grpo_recipe = RecipeRegistry.get("grpo_large_scale")
    if grpo_recipe:
        print(f"âœ“ é…æ–¹åç§°: {grpo_recipe.name}")
        print(f"âœ“ ç®—æ³•: {grpo_recipe.recommended_algorithm}")
        print(f"âœ“ æ¨è GPU æ•°: {grpo_recipe.recommended_gpus}")
    else:
        print("âœ— æœªæ‰¾åˆ°é…æ–¹")
        return False

    # æµ‹è¯• 1.3: æŒ‰ä»»åŠ¡ç±»å‹ç­›é€‰
    print("\n1.3 ç­›é€‰ RLHF ç±»å‹é…æ–¹:")
    rlhf_recipes = RecipeRegistry.list_by_task_type(TaskType.RLHF)
    print(f"âœ“ æ‰¾åˆ° {len(rlhf_recipes)} ä¸ª RLHF é…æ–¹")

    # æµ‹è¯• 1.3b: æŒ‰æ ‡ç­¾ç­›é€‰
    print("\n1.3b æŒ‰æ ‡ç­¾ç­›é€‰ GRPO é…æ–¹:")
    grpo_recipes = RecipeRegistry.list_by_tag("grpo")
    print(f"âœ“ æ‰¾åˆ° {len(grpo_recipes)} ä¸ªå¸¦ 'grpo' æ ‡ç­¾çš„é…æ–¹")

    # æµ‹è¯• 1.4: è‡ªé€‚åº”é…ç½®
    print("\n1.4 æµ‹è¯•è‡ªé€‚åº”é…ç½®:")
    config_7b = grpo_recipe.get_config(model_size="7B", num_gpus=8)
    config_70b = grpo_recipe.get_config(model_size="70B", num_gpus=32)
    print(f"âœ“ 7B æ¨¡å‹ batch_size: {config_7b.get('batch_size')}")
    print(f"âœ“ 70B æ¨¡å‹ batch_size: {config_70b.get('batch_size')}")

    # æµ‹è¯• 1.5: éªŒè¯é…ç½®
    print("\n1.5 æµ‹è¯•é…ç½®éªŒè¯:")
    warnings = validate_recipe_config(grpo_recipe, config_7b)
    print(f"âœ“ é…ç½®éªŒè¯å®Œæˆï¼Œ{len(warnings)} ä¸ªè­¦å‘Š")

    print("\nâœ… Recipe System æµ‹è¯•é€šè¿‡")
    return True


def test_config_diff():
    """æµ‹è¯•é…ç½®å¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: Config Diff (é…ç½®å¯¹æ¯”)")
    print("=" * 80)

    # æµ‹è¯• 2.1: åŸºç¡€å¯¹æ¯”
    print("\n2.1 åŸºç¡€é…ç½®å¯¹æ¯”:")
    config_a = {
        "learning_rate": 1e-6,
        "batch_size": 256,
        "kl_coef": 0.02,
        "optimizer": {
            "type": "adam",
            "weight_decay": 0.01
        }
    }
    config_b = {
        "learning_rate": 5e-7,
        "batch_size": 512,
        "kl_coef": 0.02,
        "optimizer": {
            "type": "adam",
            "weight_decay": 0.01
        },
        "warmup_steps": 100
    }

    result = compare_configs(config_a, config_b, "Config A", "Config B")
    print(f"âœ“ å¯¹æ¯”å®Œæˆ:")
    print(f"  - æ–°å¢: {result.added_count}")
    print(f"  - åˆ é™¤: {result.removed_count}")
    print(f"  - ä¿®æ”¹: {result.modified_count}")
    print(f"  - å…³é”®å‚æ•°å˜åŒ–: {result.has_critical_changes}")

    # æµ‹è¯• 2.2: ç”ŸæˆæŠ¥å‘Š
    print("\n2.2 ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š:")
    report = format_diff_report(result)
    print(report[:500])  # æ‰“å°å‰ 500 å­—ç¬¦

    # æµ‹è¯• 2.3: å¯¹æ¯”é…æ–¹
    print("\n2.3 å¯¹æ¯”ä¸¤ä¸ªé…æ–¹:")
    result_recipes = compare_recipes("grpo_basic", "grpo_large_scale")
    if result_recipes:
        print(f"âœ“ é…æ–¹å¯¹æ¯”å®Œæˆ:")
        print(f"  - ä¿®æ”¹å‚æ•°: {result_recipes.modified_count}")
        print(f"  - æ‘˜è¦: {result_recipes.summary}")
    else:
        print("âœ— é…æ–¹å¯¹æ¯”å¤±è´¥")
        return False

    print("\nâœ… Config Diff æµ‹è¯•é€šè¿‡")
    return True


def test_dataset_versioning():
    """æµ‹è¯•æ•°æ®ç‰ˆæœ¬åŒ–"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: Data Versioning (æ•°æ®ç‰ˆæœ¬åŒ–)")
    print("=" * 80)

    # åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶
    test_file = "/tmp/test_dataset.jsonl"
    test_data = [
        {"prompt": "What is 2+2?", "response": "4"},
        {"prompt": "What is 3+3?", "response": "6"},
    ]

    print(f"\n3.1 åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶: {test_file}")
    with open(test_file, "w") as f:
        for item in test_data:
            f.write(json.dumps(item) + "\n")
    print(f"âœ“ æµ‹è¯•æ–‡ä»¶åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯• 3.2: è®¡ç®— hash
    print("\n3.2 è®¡ç®—æ–‡ä»¶ hash:")
    file_hash = calculate_file_hash(test_file)
    print(f"âœ“ SHA256: {file_hash[:16]}...")

    # æµ‹è¯• 3.3: åˆ›å»ºå¿«ç…§
    print("\n3.3 åˆ›å»ºæ•°æ®é›†å¿«ç…§:")
    snapshot = create_dataset_snapshot(
        file_path=test_file,
        dataset_name="test_dataset",
        description="Test dataset for Phase 2",
        tags=["test", "math"]
    )
    print(f"âœ“ å¿«ç…§åˆ›å»ºæˆåŠŸ:")
    print(f"  - æ•°æ®é›†: {snapshot['dataset_name']}")
    print(f"  - Hash: {snapshot['file_hash'][:16]}...")
    print(f"  - æ ¼å¼: {snapshot['format']}")
    print(f"  - æ ·æœ¬æ•°: {snapshot['num_samples']}")

    # æµ‹è¯• 3.4: ä¿®æ”¹æ–‡ä»¶å¹¶é‡æ–°å¿«ç…§
    print("\n3.4 ä¿®æ”¹æ–‡ä»¶å¹¶åˆ›å»ºæ–°å¿«ç…§:")
    test_data.append({"prompt": "What is 5+5?", "response": "10"})
    with open(test_file, "w") as f:
        for item in test_data:
            f.write(json.dumps(item) + "\n")

    snapshot2 = create_dataset_snapshot(
        file_path=test_file,
        dataset_name="test_dataset",
        description="Modified test dataset",
        tags=["test", "math"]
    )
    print(f"âœ“ æ–°å¿«ç…§åˆ›å»ºæˆåŠŸ:")
    print(f"  - Hash: {snapshot2['file_hash'][:16]}...")
    print(f"  - æ ·æœ¬æ•°: {snapshot2['num_samples']}")

    # æµ‹è¯• 3.5: å¯¹æ¯”ç‰ˆæœ¬
    print("\n3.5 å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬:")
    diff = compare_dataset_versions(snapshot, snapshot2)
    print(f"âœ“ ç‰ˆæœ¬å¯¹æ¯”å®Œæˆ:")
    print(f"  - å†…å®¹ç›¸åŒ: {diff['identical']}")
    print(f"  - Hash å˜åŒ–: {diff['hash_changed']}")
    print(f"  - æ ·æœ¬æ•°å˜åŒ–: {diff['samples_diff']}")

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    os.remove(test_file)

    print("\nâœ… Data Versioning æµ‹è¯•é€šè¿‡")
    return True


def test_experience_reuse():
    """æµ‹è¯•ç»éªŒå¤ç”¨"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 4: Experience Reuse (ç»éªŒå¤ç”¨)")
    print("=" * 80)

    # è¿™ä¸ªæµ‹è¯•éœ€è¦æ•°æ®åº“ï¼Œæˆ‘ä»¬åšä¸€äº›åŸºç¡€çš„é€»è¾‘æµ‹è¯•
    from training_platform.core.experience_reuse import (
        suggest_config_adjustments,
    )

    print("\n4.1 æµ‹è¯•é…ç½®è°ƒæ•´å»ºè®®:")
    current_config = {
        "learning_rate": 1e-5,  # åé«˜
        "batch_size": 128,       # åå°
        "kl_coef": 0.01
    }

    best_practices = [
        {
            "learning_rate": 5e-7,
            "batch_size": 512,
            "kl_coef": 0.02,
            "metric_value": 0.85
        },
        {
            "learning_rate": 8e-7,
            "batch_size": 512,
            "kl_coef": 0.02,
            "metric_value": 0.82
        }
    ]

    suggestions = suggest_config_adjustments(current_config, best_practices)
    print(f"âœ“ ç”Ÿæˆ {len(suggestions)} æ¡å»ºè®®:")
    for sug in suggestions:
        print(f"  - {sug['parameter']}: {sug['current_value']} â†’ {sug['suggested_value']}")
        print(f"    åŸå› : {sug['reason']}")

    print("\nâœ… Experience Reuse æµ‹è¯•é€šè¿‡")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("Phase 2 åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)

    results = []

    # è¿è¡Œæµ‹è¯•
    results.append(("Recipe System", test_recipe_system()))
    results.append(("Config Diff", test_config_diff()))
    results.append(("Data Versioning", test_dataset_versioning()))
    results.append(("Experience Reuse", test_experience_reuse()))

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name}: {status}")

    print(f"\næ€»è®¡: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
