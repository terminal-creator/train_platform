#!/usr/bin/env python
"""
Demoæ¨¡å¼æµ‹è¯•è„šæœ¬

æµ‹è¯•Demoæ¨¡å¼çš„å„é¡¹åŠŸèƒ½
"""
import json
import sys


def test_demo_imports():
    """æµ‹è¯•Demoæ¨¡å—å¯¼å…¥"""
    print("=" * 60)
    print("æµ‹è¯• 1: Demoæ¨¡å—å¯¼å…¥")
    print("=" * 60)

    try:
        from training_platform.demo import demo_settings, is_demo_mode
        from training_platform.demo.config import set_demo_mode
        from training_platform.demo.mock_data import (
            get_all_demo_jobs,
            get_demo_metrics,
            get_gradient_heatmap,
            get_demo_evaluation_results,
            get_all_demo_pipelines,
            get_demo_compute_result,
        )
        print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_demo_jobs():
    """æµ‹è¯•Demoä»»åŠ¡æ•°æ®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: Demoä»»åŠ¡æ•°æ®")
    print("=" * 60)

    from training_platform.demo.mock_data import get_all_demo_jobs, get_demo_job

    jobs = get_all_demo_jobs()
    print(f"âœ“ è·å–åˆ° {len(jobs)} ä¸ªDemoä»»åŠ¡")

    for job in jobs:
        status_icon = {"running": "ğŸ”„", "completed": "âœ…", "pending": "â³"}.get(
            job["status"], "â“"
        )
        print(f"  {status_icon} {job['name']} ({job['algorithm'].upper()}) - {job['status']}")
        if job["status"] == "running":
            print(f"     è¿›åº¦: {job['current_step']}/{job['total_steps']} ({job['progress']}%)")

    # æµ‹è¯•å•ä¸ªä»»åŠ¡è·å–
    job = get_demo_job("demo-grpo-qwen7b-math-002")
    if job:
        print(f"âœ“ æˆåŠŸè·å–GRPOä»»åŠ¡è¯¦æƒ…")
        print(f"  æœ€æ–°æŒ‡æ ‡: reward={job['latest_metrics'].get('reward_mean', 'N/A')}")
    else:
        print("âœ— è·å–GRPOä»»åŠ¡å¤±è´¥")

    return len(jobs) > 0


def test_demo_metrics():
    """æµ‹è¯•DemoæŒ‡æ ‡æ•°æ®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: Demoè®­ç»ƒæŒ‡æ ‡")
    print("=" * 60)

    from training_platform.demo.mock_data import get_demo_metrics, get_metrics_summary

    job_id = "demo-grpo-qwen7b-math-002"
    metrics = get_demo_metrics(job_id)
    print(f"âœ“ è·å–åˆ° {len(metrics)} æ¡æŒ‡æ ‡è®°å½•")

    if metrics:
        first = metrics[0]
        last = metrics[-1]
        print(f"  èµ·å§‹ (step {first['step']}): reward={first.get('reward_mean', 'N/A')}")
        print(f"  æœ€æ–° (step {last['step']}): reward={last.get('reward_mean', 'N/A')}")

    summary = get_metrics_summary(job_id)
    if summary:
        print(f"âœ“ æŒ‡æ ‡æ±‡æ€»: {summary}")

    return len(metrics) > 0


def test_gradient_heatmap():
    """æµ‹è¯•æ¢¯åº¦çƒ­åŠ›å›¾"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: æ¢¯åº¦çƒ­åŠ›å›¾")
    print("=" * 60)

    from training_platform.demo.mock_data import get_gradient_heatmap, get_gradient_health_report

    job_id = "demo-grpo-qwen7b-math-002"
    heatmap = get_gradient_heatmap(job_id)

    print(f"âœ“ çƒ­åŠ›å›¾å±‚æ•°: {len(heatmap['layers'])}")
    print(f"âœ“ çƒ­åŠ›å›¾æ­¥æ•°: {len(heatmap['steps'])}")
    print(f"âœ“ æ•°å€¼èŒƒå›´: {heatmap['value_range']}")

    health = get_gradient_health_report(job_id)
    print(f"âœ“ æ¢¯åº¦å¥åº·çŠ¶æ€: {health['status']}")
    if health['recommendations']:
        print(f"  å»ºè®®: {health['recommendations'][0]}")

    return len(heatmap['layers']) > 0


def test_evaluation_comparison():
    """æµ‹è¯•è¯„ä¼°å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: è¯„ä¼°å¯¹æ¯”")
    print("=" * 60)

    from training_platform.demo.mock_data import get_evaluation_comparison

    comparison = get_evaluation_comparison()

    print(f"âœ“ å¯¹æ¯”æ¨¡å‹æ•°: {len(comparison['models'])}")
    print(f"âœ“ Benchmarkæ•°: {len(comparison['benchmarks'])}")

    print("\n  æ¨¡å‹èƒ½åŠ›å¯¹æ¯”:")
    benchmarks = comparison['benchmarks']
    for bm_name, scores in benchmarks.items():
        print(f"  {bm_name}:")
        for model_id, score in scores.items():
            if model_id == "baseline":
                print(f"    åŸºåº§æ¨¡å‹: {score*100:.1f}%")
            elif "grpo" in model_id:
                print(f"    GRPOæ¨¡å‹: {score*100:.1f}%")

    return len(comparison['models']) > 0


def test_compute_calculator():
    """æµ‹è¯•è®¡ç®—é…ç½®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: æ™ºèƒ½è®¡ç®—é…ç½®")
    print("=" * 60)

    from training_platform.demo.mock_data import get_demo_compute_result

    result = get_demo_compute_result(
        model_size="7B",
        gpu_type="A100-80G",
        gpu_count=8,
        training_type="grpo",
    )

    summary = result["summary"]
    memory = result["memory_estimate"]

    print(f"âœ“ æ¨¡å‹: {summary['model_size']}")
    print(f"âœ“ GPU: {summary['gpu_count']}x {summary['gpu_type']}")
    print(f"âœ“ æ‰¹é‡å¤§å°: micro={summary['micro_batch_size']}, global={summary['global_batch_size']}")
    print(f"âœ“ ZeRO Stage: {summary['zero_stage']}")
    print(f"âœ“ å†…å­˜ä¼°ç®—: {memory['per_gpu_gb']:.1f}GB / {memory['available_gpu_memory_gb']}GB")

    if summary['recommendations']:
        print(f"\n  ä¼˜åŒ–å»ºè®®:")
        for rec in summary['recommendations'][:3]:
            print(f"    - {rec}")

    return "config" in result


def test_pipelines():
    """æµ‹è¯•æµæ°´çº¿æ•°æ®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 7: è®­ç»ƒæµæ°´çº¿")
    print("=" * 60)

    from training_platform.demo.mock_data import get_all_demo_pipelines, get_pipeline_templates

    pipelines = get_all_demo_pipelines()
    print(f"âœ“ æµæ°´çº¿æ•°é‡: {len(pipelines)}")

    for p in pipelines:
        status_icon = {"running": "ğŸ”„", "completed": "âœ…"}.get(p["status"], "â“")
        print(f"  {status_icon} {p['name']}")
        print(f"     çŠ¶æ€: {p['status']}, è¿›åº¦: {p['progress']}%")
        print(f"     é˜¶æ®µ: {p['current_stage']}/{p['total_stages']}")

    templates = get_pipeline_templates()
    print(f"\nâœ“ æµæ°´çº¿æ¨¡æ¿: {len(templates)} ä¸ª")
    for t in templates:
        print(f"  - {t['name']}")

    return len(pipelines) > 0


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("      Demoæ¨¡å¼åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    tests = [
        ("æ¨¡å—å¯¼å…¥", test_demo_imports),
        ("ä»»åŠ¡æ•°æ®", test_demo_jobs),
        ("è®­ç»ƒæŒ‡æ ‡", test_demo_metrics),
        ("æ¢¯åº¦çƒ­åŠ›å›¾", test_gradient_heatmap),
        ("è¯„ä¼°å¯¹æ¯”", test_evaluation_comparison),
        ("è®¡ç®—é…ç½®", test_compute_calculator),
        ("è®­ç»ƒæµæ°´çº¿", test_pipelines),
    ]

    results = []
    for name, test_func in tests:
        try:
            success = test_func()
            results.append((name, success))
        except Exception as e:
            print(f"\nâœ— æµ‹è¯• '{name}' å¼‚å¸¸: {e}")
            results.append((name, False))

    # æ±‡æ€»
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 60)

    passed = sum(1 for _, s in results if s)
    total = len(results)

    for name, success in results:
        icon = "âœ“" if success else "âœ—"
        print(f"  {icon} {name}")

    print(f"\nç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")

    return 0 if passed == total else 1


if __name__ == "__main__":
    sys.exit(main())
