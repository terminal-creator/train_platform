#!/usr/bin/env python3
"""
å…¨åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•æ‰€æœ‰å…³é”®åŠŸèƒ½ï¼š
1. æ•°æ®åº“è¿æ¥å’Œåˆå§‹åŒ–
2. æ•°æ®é›†ç®¡ç†
3. Job åˆ›å»º
4. Pipeline æ‰§è¡Œ
5. Metrics åŒæ­¥
6. API ç«¯ç‚¹
"""

import sys
import os
import time
import requests
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from training_platform.core.database import (
    engine,
    Session,
    init_db,
    JobRepository,
    PipelineRepository,
    TrainingJob,
    Pipeline,
    PipelineStage,
    JobStatus,
    TrainingAlgorithm,
    PipelineStatus,
    PipelineStageStatus,
)
from training_platform.core.pipeline_executor import PipelineExecutor
from sqlmodel import select

# é¢œè‰²è¾“å‡º
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    END = '\033[0m'

def print_success(msg):
    print(f"{Colors.GREEN}âœ“ {msg}{Colors.END}")

def print_error(msg):
    print(f"{Colors.RED}âœ— {msg}{Colors.END}")

def print_info(msg):
    print(f"{Colors.BLUE}â„¹ {msg}{Colors.END}")

def print_warning(msg):
    print(f"{Colors.YELLOW}âš  {msg}{Colors.END}")

def print_section(title):
    print(f"\n{Colors.BLUE}{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}{Colors.END}\n")


# ============== æµ‹è¯• 1: æ•°æ®åº“è¿æ¥ ==============

def test_database_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    print_section("æµ‹è¯• 1: æ•°æ®åº“è¿æ¥")

    try:
        # åˆ›å»ºè¡¨
        init_db()
        print_success("æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•è¿æ¥
        with Session(engine) as session:
            # ç®€å•æŸ¥è¯¢
            result = session.exec(select(TrainingJob).limit(1)).first()
            print_success(f"æ•°æ®åº“è¿æ¥æˆåŠŸ (ç°æœ‰ jobs: {result.name if result else 'æ— '})")

        return True
    except Exception as e:
        print_error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False


# ============== æµ‹è¯• 2: æ•°æ®é›†æ–‡ä»¶ ==============

def test_datasets():
    """æµ‹è¯•æ•°æ®é›†æ–‡ä»¶"""
    print_section("æµ‹è¯• 2: æ•°æ®é›†æ–‡ä»¶")

    datasets_dir = Path("./datasets")

    if not datasets_dir.exists():
        print_error(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {datasets_dir}")
        return False

    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    expected_files = [
        "sft_math.json",
        "ppo_general.json",
        "grpo_math.json",
        "dpo_preference.json",
        "sales_sft.jsonl",
        "sales_grpo.jsonl",
        "sales_dpo.jsonl",
    ]

    found_files = []
    missing_files = []

    for file in expected_files:
        file_path = datasets_dir / file
        if file_path.exists():
            size = file_path.stat().st_size
            print_success(f"{file} ({size:,} bytes)")
            found_files.append(file)
        else:
            print_warning(f"{file} ä¸å­˜åœ¨")
            missing_files.append(file)

    if found_files:
        print_info(f"æ‰¾åˆ° {len(found_files)}/{len(expected_files)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
        return True
    else:
        print_error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®é›†æ–‡ä»¶")
        return False


# ============== æµ‹è¯• 3: Job åˆ›å»º ==============

def test_job_creation():
    """æµ‹è¯• Job åˆ›å»º"""
    print_section("æµ‹è¯• 3: Job åˆ›å»º")

    try:
        with Session(engine) as session:
            repo = JobRepository(session)

            # åˆ›å»ºæµ‹è¯• job
            job = TrainingJob(
                uuid="test-job-" + str(int(time.time())),
                name="Test Training Job",
                description="æµ‹è¯•è®­ç»ƒä»»åŠ¡",
                status=JobStatus.PENDING,
                algorithm=TrainingAlgorithm.SFT,
                model_path="Qwen/Qwen2.5-0.5B",
                train_data_path="./datasets/sales_sft.jsonl",
                num_gpus=1,
                learning_rate=1e-5,
                batch_size=4,
                num_epochs=1,
                context_length=512,
            )

            created_job = repo.create(job)
            print_success(f"Job åˆ›å»ºæˆåŠŸ: {created_job.uuid}")
            print_info(f"  - åç§°: {created_job.name}")
            print_info(f"  - ç®—æ³•: {created_job.algorithm}")
            print_info(f"  - çŠ¶æ€: {created_job.status}")

            return created_job.uuid
    except Exception as e:
        print_error(f"Job åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============== æµ‹è¯• 4: Pipeline åˆ›å»º ==============

def test_pipeline_creation():
    """æµ‹è¯• Pipeline åˆ›å»º"""
    print_section("æµ‹è¯• 4: Pipeline åˆ›å»º")

    try:
        pipeline_uuid = "test-pipeline-" + str(int(time.time()))

        with Session(engine) as session:
            repo = PipelineRepository(session)

            # åˆ›å»º pipeline
            pipeline = Pipeline(
                uuid=pipeline_uuid,
                name="Test Pipeline",
                description="æµ‹è¯• pipeline",
                status=PipelineStatus.PENDING,
            )
            created_pipeline = repo.create(pipeline)

            # åˆ›å»º stages
            stages_config = [
                {
                    "name": "stage_A",
                    "task_name": "preprocess_dataset",
                    "task_params": {"dataset_uuid": "test-dataset", "preprocessing_config": {}},
                    "depends_on": [],
                    "stage_order": 0,
                },
                {
                    "name": "stage_B",
                    "task_name": "train_model",
                    "task_params": {"job_uuid": "test-job", "config": {}},
                    "depends_on": ["stage_A"],
                    "stage_order": 1,
                },
            ]

            for stage_config in stages_config:
                stage = PipelineStage(
                    pipeline_uuid=pipeline_uuid,
                    stage_name=stage_config["name"],
                    task_name=stage_config["task_name"],
                    task_params=stage_config["task_params"],
                    depends_on=stage_config["depends_on"],
                    stage_order=stage_config["stage_order"],
                    status=PipelineStageStatus.PENDING,
                )
                repo.create_stage(stage)

            print_success(f"Pipeline åˆ›å»ºæˆåŠŸ: {pipeline_uuid}")
            print_info(f"  - åç§°: {created_pipeline.name}")
            print_info(f"  - Stages: {len(stages_config)}")

            return pipeline_uuid
    except Exception as e:
        print_error(f"Pipeline åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============== æµ‹è¯• 5: DAG è§£æ ==============

def test_dag_resolution():
    """æµ‹è¯• DAG ä¾èµ–è§£æ"""
    print_section("æµ‹è¯• 5: DAG ä¾èµ–è§£æ")

    try:
        from training_platform.core.pipeline_executor import DagResolver

        # æµ‹è¯•ç®€å•çº¿æ€§ DAG
        stages = [
            {"name": "A", "task": "preprocess_dataset", "params": {}, "depends_on": []},
            {"name": "B", "task": "train_model", "params": {}, "depends_on": ["A"]},
            {"name": "C", "task": "run_evaluation", "params": {}, "depends_on": ["B"]},
        ]

        resolver = DagResolver(stages)
        resolver.validate()
        layers = resolver.get_execution_layers()

        print_success("çº¿æ€§ DAG è§£ææˆåŠŸ")
        print_info(f"  æ‰§è¡Œå±‚çº§: {layers}")

        # æµ‹è¯•å¹¶è¡Œ DAG
        parallel_stages = [
            {"name": "A", "task": "preprocess_dataset", "params": {}, "depends_on": []},
            {"name": "B", "task": "train_model", "params": {}, "depends_on": ["A"]},
            {"name": "C", "task": "train_model", "params": {}, "depends_on": ["A"]},
            {"name": "D", "task": "run_evaluation", "params": {}, "depends_on": ["B", "C"]},
        ]

        resolver2 = DagResolver(parallel_stages)
        resolver2.validate()
        layers2 = resolver2.get_execution_layers()

        print_success("å¹¶è¡Œ DAG è§£ææˆåŠŸ")
        print_info(f"  æ‰§è¡Œå±‚çº§: {layers2}")

        return True
    except Exception as e:
        print_error(f"DAG è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============== æµ‹è¯• 6: Celery è¿æ¥ ==============

def test_celery_connection():
    """æµ‹è¯• Celery è¿æ¥"""
    print_section("æµ‹è¯• 6: Celery/Redis è¿æ¥")

    try:
        from training_platform.core.celery_config import app

        # æ£€æŸ¥ Redis è¿æ¥
        inspect = app.control.inspect()
        active_workers = inspect.active()

        if active_workers:
            print_success(f"å‘ç° {len(active_workers)} ä¸ªæ´»è·ƒ worker")
            for worker_name, tasks in active_workers.items():
                print_info(f"  - {worker_name}: {len(tasks)} ä¸ªæ´»è·ƒä»»åŠ¡")
        else:
            print_warning("æ²¡æœ‰å‘ç°æ´»è·ƒçš„ Celery workers")
            print_info("  æç¤º: éœ€è¦å¯åŠ¨ Celery workers æ‰èƒ½æ‰§è¡Œ pipeline")

        return True
    except Exception as e:
        print_error(f"Celery è¿æ¥å¤±è´¥: {e}")
        return False


# ============== æµ‹è¯• 7: API ç«¯ç‚¹ ==============

def test_api_endpoints():
    """æµ‹è¯• API ç«¯ç‚¹"""
    print_section("æµ‹è¯• 7: API ç«¯ç‚¹ (éœ€è¦ FastAPI è¿è¡Œ)")

    base_url = "http://localhost:8000"

    try:
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        response = requests.get(f"{base_url}/health", timeout=2)
        if response.status_code == 200:
            print_success("API å¥åº·æ£€æŸ¥é€šè¿‡")

            # æµ‹è¯•è·å– jobs åˆ—è¡¨
            response = requests.get(f"{base_url}/api/jobs", timeout=2)
            if response.status_code == 200:
                jobs = response.json()
                print_success(f"è·å– jobs åˆ—è¡¨æˆåŠŸ ({jobs.get('total', 0)} ä¸ª)")
            else:
                print_warning(f"è·å– jobs åˆ—è¡¨å¤±è´¥: {response.status_code}")

            return True
        else:
            print_warning(f"API å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print_warning("æ— æ³•è¿æ¥åˆ° API æœåŠ¡å™¨ (æœªå¯åŠ¨)")
        print_info("  æç¤º: è¿è¡Œ 'uvicorn training_platform.api.main:app' å¯åŠ¨ API")
        return False
    except Exception as e:
        print_error(f"API æµ‹è¯•å¤±è´¥: {e}")
        return False


# ============== æµ‹è¯• 8: Metrics è·¯å¾„ ==============

def test_metrics_paths():
    """æµ‹è¯• Metrics è·¯å¾„é…ç½®"""
    print_section("æµ‹è¯• 8: Metrics è·¯å¾„é…ç½®")

    import os

    # æ£€æŸ¥ metrics ç›®å½•
    metrics_dir = Path(os.getenv("PLATFORM_METRICS_DIR", "./platform_metrics"))

    if not metrics_dir.exists():
        print_warning(f"Metrics ç›®å½•ä¸å­˜åœ¨: {metrics_dir}")
        print_info("  åˆ›å»ºç›®å½•...")
        metrics_dir.mkdir(parents=True, exist_ok=True)
        print_success(f"Metrics ç›®å½•å·²åˆ›å»º: {metrics_dir}")
    else:
        print_success(f"Metrics ç›®å½•å­˜åœ¨: {metrics_dir}")

        # åˆ—å‡ºç°æœ‰æ–‡ä»¶
        files = list(metrics_dir.glob("*_metrics.jsonl"))
        if files:
            print_info(f"  æ‰¾åˆ° {len(files)} ä¸ª metrics æ–‡ä»¶")
            for f in files[:3]:  # åªæ˜¾ç¤ºå‰ 3 ä¸ª
                print_info(f"    - {f.name}")
        else:
            print_info("  ç›®å½•ä¸ºç©ºï¼ˆè®­ç»ƒåä¼šç”Ÿæˆæ–‡ä»¶ï¼‰")

    return True


# ============== æµ‹è¯• 9: SSH é…ç½® ==============

def test_ssh_config():
    """æµ‹è¯• SSH é…ç½®"""
    print_section("æµ‹è¯• 9: SSH é…ç½®")

    # è¿™é‡Œåªæµ‹è¯•é…ç½®æ ¼å¼ï¼Œä¸å®é™…è¿æ¥
    print_info("SSH é…ç½®æµ‹è¯•ï¼ˆä»…æ£€æŸ¥æ ¼å¼ï¼‰")

    ssh_config_example = {
        "ssh_host": "remote.server.com",
        "ssh_port": 22,
        "ssh_username": "user",
        "ssh_password": "password",  # æˆ–ä½¿ç”¨ ssh_key_path
        "ssh_working_dir": "~/verl_jobs",
    }

    print_success("SSH é…ç½®æ ¼å¼æ­£ç¡®")
    print_info("  ç¤ºä¾‹é…ç½®:")
    for key, value in ssh_config_example.items():
        print_info(f"    {key}: {value}")

    print_info("\n  æç¤º: å®é™…è¿æ¥æµ‹è¯•éœ€è¦åœ¨ Job åˆ›å»ºæ—¶æŒ‡å®š SSH é…ç½®")

    return True


# ============== ä¸»æµ‹è¯•å‡½æ•° ==============

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print(f"\n{Colors.BLUE}")
    print("="*60)
    print("  Training Platform - å…¨åŠŸèƒ½æµ‹è¯•")
    print("="*60)
    print(f"{Colors.END}\n")

    results = {}

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results["database"] = test_database_connection()
    results["datasets"] = test_datasets()
    results["job_creation"] = test_job_creation()
    results["pipeline_creation"] = test_pipeline_creation()
    results["dag_resolution"] = test_dag_resolution()
    results["celery"] = test_celery_connection()
    results["api"] = test_api_endpoints()
    results["metrics_paths"] = test_metrics_paths()
    results["ssh_config"] = test_ssh_config()

    # æ±‡æ€»ç»“æœ
    print_section("æµ‹è¯•ç»“æœæ±‡æ€»")

    passed = sum(1 for v in results.values() if v)
    total = len(results)

    for test_name, result in results.items():
        status = "âœ“ PASS" if result else "âœ— FAIL"
        color = Colors.GREEN if result else Colors.RED
        print(f"{color}{status:10}{Colors.END} {test_name}")

    print(f"\n{Colors.BLUE}æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡{Colors.END}\n")

    # åç»­æ­¥éª¤å»ºè®®
    if passed == total:
        print_success("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå°±ç»ª ğŸ‰")
        print_info("\nä¸‹ä¸€æ­¥:")
        print_info("  1. å¯åŠ¨ Celery workers: ./scripts/start_workers.sh")
        print_info("  2. å¯åŠ¨ API æœåŠ¡å™¨: uvicorn training_platform.api.main:app")
        print_info("  3. åˆ›å»ºè®­ç»ƒä»»åŠ¡å¹¶è¿è¡Œ pipeline")
    else:
        print_warning("éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„ç»„ä»¶")

        if not results["celery"]:
            print_info("\n  å¯åŠ¨ Celery workers:")
            print_info("    ./scripts/start_workers.sh")

        if not results["api"]:
            print_info("\n  å¯åŠ¨ API æœåŠ¡å™¨:")
            print_info("    uvicorn training_platform.api.main:app --reload")

    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
