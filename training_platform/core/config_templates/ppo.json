{
  "name": "PPO - Proximal Policy Optimization",
  "description": "近端策略优化训练配置模板。经典RLHF算法，需要reward model和critic model。",
  "category": "reinforcement_learning",
  "algorithm": "ppo",
  "verl_module": "verl.trainer.main_ppo",
  "verl_adv_estimator": "gae",
  "required_fields": ["model_path", "train_data_path"],
  "data_format": {
    "type": "prompt-only",
    "required_columns": ["prompt"],
    "optional_columns": ["data_source"],
    "description": "每条数据包含prompt字段，reward由reward model提供"
  },
  "defaults": {
    "num_epochs": 3,
    "learning_rate": 1e-6,
    "batch_size": 256,
    "micro_batch_size": 4,
    "max_prompt_length": 512,
    "max_response_length": 1024,
    "rollout_n": 1,
    "kl_coef": 0.001,
    "entropy_coef": 0.01,
    "clip_ratio": 0.2,
    "use_kl_loss": true,
    "reward_model_path": null,
    "reward_model_enable_gc": true,
    "reward_model_offload": false,
    "gpu_memory_utilization": 0.5,
    "lora_enabled": false
  },
  "parameter_ranges": {
    "learning_rate": {"min": 1e-8, "max": 1e-4, "recommended": 1e-6},
    "batch_size": {"min": 16, "max": 1024, "recommended": 256},
    "clip_ratio": {"min": 0.1, "max": 0.5, "recommended": 0.2},
    "kl_coef": {"min": 0.0, "max": 1.0, "recommended": 0.001}
  },
  "tips": [
    "PPO需要reward model来评估生成质量",
    "可以使用reward script替代reward model (rule-based reward)",
    "critic model通常使用与actor相同的模型初始化",
    "gpu_memory_utilization需要更低因为需要同时加载多个模型"
  ]
}
