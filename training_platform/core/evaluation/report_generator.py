"""
Evaluation Report Generator

Generates evaluation reports in Markdown and HTML formats.
Supports radar charts, comparison tables, and historical trends.
"""

import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


def generate_report(
    results: List[Dict[str, Any]],
    model_name: str = "Unknown Model",
    output_format: str = "markdown",
    comparison: Optional[List[Dict[str, Any]]] = None,
) -> str:
    """
    Generate evaluation report.

    Args:
        results: List of benchmark results
        model_name: Name of the evaluated model
        output_format: 'markdown' or 'html'
        comparison: Optional previous results for comparison

    Returns:
        Report content as string
    """
    if output_format == "markdown":
        return _generate_markdown(results, model_name, comparison)
    elif output_format == "html":
        return _generate_html(results, model_name, comparison)
    else:
        raise ValueError(f"Unsupported format: {output_format}")


def _generate_markdown(
    results: List[Dict[str, Any]],
    model_name: str,
    comparison: Optional[List[Dict[str, Any]]] = None,
) -> str:
    """Generate Markdown report."""
    lines = []
    lines.append(f"# Evaluation Report: {model_name}")
    lines.append(f"\n**Generated**: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\n")

    # Summary table
    lines.append("## Summary\n")
    lines.append("| Benchmark | Accuracy | Correct | Total |")
    lines.append("|-----------|----------|---------|-------|")

    for r in results:
        acc = r.get("accuracy", 0)
        total = r.get("total", 0)
        correct = r.get("correct", 0)
        benchmark = r.get("benchmark", "Unknown")
        lines.append(f"| {benchmark} | {acc*100:.1f}% | {correct} | {total} |")

    # Overall average
    if results:
        avg_acc = sum(r.get("accuracy", 0) for r in results) / len(results)
        lines.append(f"| **Average** | **{avg_acc*100:.1f}%** | | |")

    # Comparison with previous results
    if comparison:
        lines.append("\n## Comparison with Previous\n")
        lines.append("| Benchmark | Current | Previous | Change |")
        lines.append("|-----------|---------|----------|--------|")

        prev_map = {r.get("benchmark"): r for r in comparison}
        for r in results:
            bm = r.get("benchmark", "")
            curr_acc = r.get("accuracy", 0)
            prev = prev_map.get(bm, {})
            prev_acc = prev.get("accuracy", 0)
            change = curr_acc - prev_acc
            change_str = f"+{change*100:.1f}%" if change >= 0 else f"{change*100:.1f}%"
            emoji = "+" if change > 0 else ("-" if change < 0 else "=")
            lines.append(f"| {bm} | {curr_acc*100:.1f}% | {prev_acc*100:.1f}% | {emoji}{abs(change)*100:.1f}% |")

    # Detailed results per benchmark
    for r in results:
        bm = r.get("benchmark", "Unknown")
        metadata = r.get("metadata", {})

        if metadata:
            lines.append(f"\n## {bm} Details\n")
            for key, value in metadata.items():
                lines.append(f"- **{key}**: {value}")

        # Per-subject breakdown for MMLU
        by_subject = r.get("by_subject", {})
        if by_subject:
            lines.append(f"\n### {bm} - Subject Breakdown\n")
            lines.append("| Subject | Accuracy | Correct | Total |")
            lines.append("|---------|----------|---------|-------|")
            for subject, stats in sorted(by_subject.items()):
                lines.append(
                    f"| {subject} | {stats['accuracy']*100:.1f}% | {stats['correct']} | {stats['total']} |"
                )

    lines.append(f"\n---\n*Report generated by LLM Training Platform*")

    return "\n".join(lines)


def _generate_html(
    results: List[Dict[str, Any]],
    model_name: str,
    comparison: Optional[List[Dict[str, Any]]] = None,
) -> str:
    """Generate HTML report with embedded charts."""
    # Prepare chart data
    benchmarks = [r.get("benchmark", "") for r in results]
    accuracies = [r.get("accuracy", 0) * 100 for r in results]

    comparison_data = ""
    if comparison:
        prev_accs = []
        for r in results:
            bm = r.get("benchmark", "")
            prev = next((c for c in comparison if c.get("benchmark") == bm), {})
            prev_accs.append(prev.get("accuracy", 0) * 100)
        comparison_data = f"""
            {{
                label: 'Previous',
                data: {json.dumps(prev_accs)},
                backgroundColor: 'rgba(255, 99, 132, 0.2)',
                borderColor: 'rgba(255, 99, 132, 1)',
                borderWidth: 1
            }},"""

    html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Evaluation Report: {model_name}</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 40px; }}
        h1 {{ color: #333; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #4CAF50; color: white; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        .chart-container {{ width: 600px; margin: 20px auto; }}
        .summary {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; }}
    </style>
</head>
<body>
    <h1>Evaluation Report: {model_name}</h1>
    <p>Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC</p>

    <div class="summary">
        <h2>Summary</h2>
        <table>
            <tr><th>Benchmark</th><th>Accuracy</th><th>Correct</th><th>Total</th></tr>
            {''.join(f'<tr><td>{r.get("benchmark","")}</td><td>{r.get("accuracy",0)*100:.1f}%</td><td>{r.get("correct",0)}</td><td>{r.get("total",0)}</td></tr>' for r in results)}
        </table>
    </div>

    <div class="chart-container">
        <canvas id="radarChart"></canvas>
    </div>

    <div class="chart-container">
        <canvas id="barChart"></canvas>
    </div>

    <script>
    // Radar Chart
    new Chart(document.getElementById('radarChart'), {{
        type: 'radar',
        data: {{
            labels: {json.dumps(benchmarks)},
            datasets: [{{
                label: 'Current',
                data: {json.dumps(accuracies)},
                backgroundColor: 'rgba(54, 162, 235, 0.2)',
                borderColor: 'rgba(54, 162, 235, 1)',
                borderWidth: 2
            }},{comparison_data}]
        }},
        options: {{
            scales: {{ r: {{ beginAtZero: true, max: 100 }} }},
            plugins: {{ title: {{ display: true, text: 'Benchmark Comparison' }} }}
        }}
    }});

    // Bar Chart
    new Chart(document.getElementById('barChart'), {{
        type: 'bar',
        data: {{
            labels: {json.dumps(benchmarks)},
            datasets: [{{
                label: 'Accuracy (%)',
                data: {json.dumps(accuracies)},
                backgroundColor: 'rgba(75, 192, 192, 0.6)',
                borderColor: 'rgba(75, 192, 192, 1)',
                borderWidth: 1
            }}]
        }},
        options: {{
            scales: {{ y: {{ beginAtZero: true, max: 100 }} }},
            plugins: {{ title: {{ display: true, text: 'Benchmark Scores' }} }}
        }}
    }});
    </script>
</body>
</html>"""
    return html


def save_report(
    results: List[Dict[str, Any]],
    output_path: str,
    model_name: str = "Unknown Model",
    comparison: Optional[List[Dict[str, Any]]] = None,
):
    """Generate and save report to file."""
    path = Path(output_path)
    fmt = "html" if path.suffix == ".html" else "markdown"
    content = generate_report(results, model_name, fmt, comparison)
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")
    logger.info(f"Report saved to {output_path}")
    return str(path)
